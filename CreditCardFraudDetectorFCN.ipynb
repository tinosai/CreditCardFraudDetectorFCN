{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "CreditCardFraudDetectorFCN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpCf7v37Btn1",
        "colab_type": "text"
      },
      "source": [
        "# Credit Card Fraud Detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo2hmo5TBtn6",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, we will develop a credit card fraud detector using a fully-connected neural network (FCN) in PyTorch. <br />\n",
        "\n",
        "## 1. Dataset\n",
        "The necessary dataset can be found at the following [link](https://www.kaggle.com/mlg-ulb/creditcardfraud). <br />\n",
        "All you need to do is to download the dataset into your working folder and unzip the *creditcardfraud.zip* file. The final data file will be a *creditcardfraud.csv* file which we will use for training, validating and test the neural network architecture. If you decide to download and unzip the files through means other than the command line, feel free to skip **1.1**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s_GTFXTBtn9",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Download via API ( for Google Colab Training )\n",
        "You may also download the necessary file through the kaggle command if that is installed on your environment. The necessary code is reported in this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTi1H_MwBtn_",
        "colab_type": "text"
      },
      "source": [
        "Make sure you replace the following lines with your personal kaggle username and API key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ5Ai0ODBtoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!echo '{\"username\":\"KAGGLE_USERNAME\",\"key\":\"KAGGLE_API_KEY\"}' > kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBIdLa1gBtoI",
        "colab_type": "text"
      },
      "source": [
        "Move the kaggle.json file into the appropriate folder and download the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mMPcY1Af2YG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /root/.kaggle/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX7Bk9s0q5K8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv kaggle.json /root/.kaggle/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZSdxdVABtoL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "3b93bb74-c3e9-40ea-9848-daf1a292510b"
      },
      "source": [
        "!kaggle datasets download -d mlg-ulb/creditcardfraud\n",
        "!unzip creditcardfraud.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading creditcardfraud.zip to /content\n",
            " 74% 49.0M/66.0M [00:00<00:00, 65.9MB/s]\n",
            "100% 66.0M/66.0M [00:00<00:00, 122MB/s] \n",
            "Archive:  creditcardfraud.zip\n",
            "  inflating: creditcard.csv          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LRzFyWFBtoR",
        "colab_type": "text"
      },
      "source": [
        "Now that the necessary file is ready, we can move to the next section: Data Preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kkWsyKXBtoT",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai71yCu5BtoU",
        "colab_type": "text"
      },
      "source": [
        "Let's import the necessary packages first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNqSopiKBtoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djo3OV4aBtoc",
        "colab_type": "text"
      },
      "source": [
        "Read in the csv file and visualize some of the data included"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH23M8dqBtod",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "72104d49-fc20-4ec8-fa55-8ae635f2f970"
      },
      "source": [
        "df=pd.read_csv('creditcard.csv')\n",
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
              "0   0.0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0\n",
              "1   0.0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69      0\n",
              "2   1.0 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66      0\n",
              "3   1.0 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50      0\n",
              "4   2.0 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99      0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmIH4H1eBtoh",
        "colab_type": "text"
      },
      "source": [
        "The dataset is provided as the result of a PCA, which means we do not really how the features relate to physical features (i.e. location, time of day etc.). Let's now check the size of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLCEopUbBtoj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "54ffbbe1-8b36-4271-e461-fc8da0b17f8d"
      },
      "source": [
        "print(\"The dataset contains {} columns\".format(len(df.columns)))\n",
        "print(\"The dataset contains {} rows\".format(len(df)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The dataset contains 31 columns\n",
            "The dataset contains 284807 rows\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqAGk9NJBtoo",
        "colab_type": "text"
      },
      "source": [
        "In this case, we are deciding not to pay attention to the *Time* feature, which we will drop here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvuV8hChBtoq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "9a3338be-816d-48c6-c135-e3f732ece5b9"
      },
      "source": [
        "df=df.drop([\"Time\"],axis=1)\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         V1        V2        V3        V4  ...       V27       V28  Amount  Class\n",
              "0 -1.359807 -0.072781  2.536347  1.378155  ...  0.133558 -0.021053  149.62      0\n",
              "1  1.191857  0.266151  0.166480  0.448154  ... -0.008983  0.014724    2.69      0\n",
              "2 -1.358354 -1.340163  1.773209  0.379780  ... -0.055353 -0.059752  378.66      0\n",
              "3 -0.966272 -0.185226  1.792993 -0.863291  ...  0.062723  0.061458  123.50      0\n",
              "4 -1.158233  0.877737  1.548718  0.403034  ...  0.219422  0.215153   69.99      0\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ir7JVCbBtov",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "bacf8222-067f-4ba2-8891-040b5cb40a8e"
      },
      "source": [
        "print(\"The dataset contains {} columns\".format(len(df.columns)))\n",
        "print(\"The dataset contains {} rows\".format(len(df)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The dataset contains 30 columns\n",
            "The dataset contains 284807 rows\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzB-CQpVBto0",
        "colab_type": "text"
      },
      "source": [
        "In a problem like fraud detection, most of the examples are negative (which means, non-fraudulent transactions) which is by definition a problem normally referred to as *Anomaly Detection*. <br />\n",
        "Below, we will show the first 10 fraudulent transactions in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1qDJXJMBto2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "11cfdc50-f5c6-4ab6-a566-2db61a4008cc"
      },
      "source": [
        "df[df[\"Class\"]==1].head(10)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>541</th>\n",
              "      <td>-2.312227</td>\n",
              "      <td>1.951992</td>\n",
              "      <td>-1.609851</td>\n",
              "      <td>3.997906</td>\n",
              "      <td>-0.522188</td>\n",
              "      <td>-1.426545</td>\n",
              "      <td>-2.537387</td>\n",
              "      <td>1.391657</td>\n",
              "      <td>-2.770089</td>\n",
              "      <td>-2.772272</td>\n",
              "      <td>3.202033</td>\n",
              "      <td>-2.899907</td>\n",
              "      <td>-0.595222</td>\n",
              "      <td>-4.289254</td>\n",
              "      <td>0.389724</td>\n",
              "      <td>-1.140747</td>\n",
              "      <td>-2.830056</td>\n",
              "      <td>-0.016822</td>\n",
              "      <td>0.416956</td>\n",
              "      <td>0.126911</td>\n",
              "      <td>0.517232</td>\n",
              "      <td>-0.035049</td>\n",
              "      <td>-0.465211</td>\n",
              "      <td>0.320198</td>\n",
              "      <td>0.044519</td>\n",
              "      <td>0.177840</td>\n",
              "      <td>0.261145</td>\n",
              "      <td>-0.143276</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>623</th>\n",
              "      <td>-3.043541</td>\n",
              "      <td>-3.157307</td>\n",
              "      <td>1.088463</td>\n",
              "      <td>2.288644</td>\n",
              "      <td>1.359805</td>\n",
              "      <td>-1.064823</td>\n",
              "      <td>0.325574</td>\n",
              "      <td>-0.067794</td>\n",
              "      <td>-0.270953</td>\n",
              "      <td>-0.838587</td>\n",
              "      <td>-0.414575</td>\n",
              "      <td>-0.503141</td>\n",
              "      <td>0.676502</td>\n",
              "      <td>-1.692029</td>\n",
              "      <td>2.000635</td>\n",
              "      <td>0.666780</td>\n",
              "      <td>0.599717</td>\n",
              "      <td>1.725321</td>\n",
              "      <td>0.283345</td>\n",
              "      <td>2.102339</td>\n",
              "      <td>0.661696</td>\n",
              "      <td>0.435477</td>\n",
              "      <td>1.375966</td>\n",
              "      <td>-0.293803</td>\n",
              "      <td>0.279798</td>\n",
              "      <td>-0.145362</td>\n",
              "      <td>-0.252773</td>\n",
              "      <td>0.035764</td>\n",
              "      <td>529.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4920</th>\n",
              "      <td>-2.303350</td>\n",
              "      <td>1.759247</td>\n",
              "      <td>-0.359745</td>\n",
              "      <td>2.330243</td>\n",
              "      <td>-0.821628</td>\n",
              "      <td>-0.075788</td>\n",
              "      <td>0.562320</td>\n",
              "      <td>-0.399147</td>\n",
              "      <td>-0.238253</td>\n",
              "      <td>-1.525412</td>\n",
              "      <td>2.032912</td>\n",
              "      <td>-6.560124</td>\n",
              "      <td>0.022937</td>\n",
              "      <td>-1.470102</td>\n",
              "      <td>-0.698826</td>\n",
              "      <td>-2.282194</td>\n",
              "      <td>-4.781831</td>\n",
              "      <td>-2.615665</td>\n",
              "      <td>-1.334441</td>\n",
              "      <td>-0.430022</td>\n",
              "      <td>-0.294166</td>\n",
              "      <td>-0.932391</td>\n",
              "      <td>0.172726</td>\n",
              "      <td>-0.087330</td>\n",
              "      <td>-0.156114</td>\n",
              "      <td>-0.542628</td>\n",
              "      <td>0.039566</td>\n",
              "      <td>-0.153029</td>\n",
              "      <td>239.93</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6108</th>\n",
              "      <td>-4.397974</td>\n",
              "      <td>1.358367</td>\n",
              "      <td>-2.592844</td>\n",
              "      <td>2.679787</td>\n",
              "      <td>-1.128131</td>\n",
              "      <td>-1.706536</td>\n",
              "      <td>-3.496197</td>\n",
              "      <td>-0.248778</td>\n",
              "      <td>-0.247768</td>\n",
              "      <td>-4.801637</td>\n",
              "      <td>4.895844</td>\n",
              "      <td>-10.912819</td>\n",
              "      <td>0.184372</td>\n",
              "      <td>-6.771097</td>\n",
              "      <td>-0.007326</td>\n",
              "      <td>-7.358083</td>\n",
              "      <td>-12.598419</td>\n",
              "      <td>-5.131549</td>\n",
              "      <td>0.308334</td>\n",
              "      <td>-0.171608</td>\n",
              "      <td>0.573574</td>\n",
              "      <td>0.176968</td>\n",
              "      <td>-0.436207</td>\n",
              "      <td>-0.053502</td>\n",
              "      <td>0.252405</td>\n",
              "      <td>-0.657488</td>\n",
              "      <td>-0.827136</td>\n",
              "      <td>0.849573</td>\n",
              "      <td>59.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6329</th>\n",
              "      <td>1.234235</td>\n",
              "      <td>3.019740</td>\n",
              "      <td>-4.304597</td>\n",
              "      <td>4.732795</td>\n",
              "      <td>3.624201</td>\n",
              "      <td>-1.357746</td>\n",
              "      <td>1.713445</td>\n",
              "      <td>-0.496358</td>\n",
              "      <td>-1.282858</td>\n",
              "      <td>-2.447469</td>\n",
              "      <td>2.101344</td>\n",
              "      <td>-4.609628</td>\n",
              "      <td>1.464378</td>\n",
              "      <td>-6.079337</td>\n",
              "      <td>-0.339237</td>\n",
              "      <td>2.581851</td>\n",
              "      <td>6.739384</td>\n",
              "      <td>3.042493</td>\n",
              "      <td>-2.721853</td>\n",
              "      <td>0.009061</td>\n",
              "      <td>-0.379068</td>\n",
              "      <td>-0.704181</td>\n",
              "      <td>-0.656805</td>\n",
              "      <td>-1.632653</td>\n",
              "      <td>1.488901</td>\n",
              "      <td>0.566797</td>\n",
              "      <td>-0.010016</td>\n",
              "      <td>0.146793</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6331</th>\n",
              "      <td>0.008430</td>\n",
              "      <td>4.137837</td>\n",
              "      <td>-6.240697</td>\n",
              "      <td>6.675732</td>\n",
              "      <td>0.768307</td>\n",
              "      <td>-3.353060</td>\n",
              "      <td>-1.631735</td>\n",
              "      <td>0.154612</td>\n",
              "      <td>-2.795892</td>\n",
              "      <td>-6.187891</td>\n",
              "      <td>5.664395</td>\n",
              "      <td>-9.854485</td>\n",
              "      <td>-0.306167</td>\n",
              "      <td>-10.691196</td>\n",
              "      <td>-0.638498</td>\n",
              "      <td>-2.041974</td>\n",
              "      <td>-1.129056</td>\n",
              "      <td>0.116453</td>\n",
              "      <td>-1.934666</td>\n",
              "      <td>0.488378</td>\n",
              "      <td>0.364514</td>\n",
              "      <td>-0.608057</td>\n",
              "      <td>-0.539528</td>\n",
              "      <td>0.128940</td>\n",
              "      <td>1.488481</td>\n",
              "      <td>0.507963</td>\n",
              "      <td>0.735822</td>\n",
              "      <td>0.513574</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6334</th>\n",
              "      <td>0.026779</td>\n",
              "      <td>4.132464</td>\n",
              "      <td>-6.560600</td>\n",
              "      <td>6.348557</td>\n",
              "      <td>1.329666</td>\n",
              "      <td>-2.513479</td>\n",
              "      <td>-1.689102</td>\n",
              "      <td>0.303253</td>\n",
              "      <td>-3.139409</td>\n",
              "      <td>-6.045468</td>\n",
              "      <td>6.754625</td>\n",
              "      <td>-8.948179</td>\n",
              "      <td>0.702725</td>\n",
              "      <td>-10.733854</td>\n",
              "      <td>-1.379520</td>\n",
              "      <td>-1.638960</td>\n",
              "      <td>-1.746350</td>\n",
              "      <td>0.776744</td>\n",
              "      <td>-1.327357</td>\n",
              "      <td>0.587743</td>\n",
              "      <td>0.370509</td>\n",
              "      <td>-0.576752</td>\n",
              "      <td>-0.669605</td>\n",
              "      <td>-0.759908</td>\n",
              "      <td>1.605056</td>\n",
              "      <td>0.540675</td>\n",
              "      <td>0.737040</td>\n",
              "      <td>0.496699</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6336</th>\n",
              "      <td>0.329594</td>\n",
              "      <td>3.712889</td>\n",
              "      <td>-5.775935</td>\n",
              "      <td>6.078266</td>\n",
              "      <td>1.667359</td>\n",
              "      <td>-2.420168</td>\n",
              "      <td>-0.812891</td>\n",
              "      <td>0.133080</td>\n",
              "      <td>-2.214311</td>\n",
              "      <td>-5.134454</td>\n",
              "      <td>4.560720</td>\n",
              "      <td>-8.873748</td>\n",
              "      <td>-0.797484</td>\n",
              "      <td>-9.177166</td>\n",
              "      <td>-0.257025</td>\n",
              "      <td>-0.871688</td>\n",
              "      <td>1.313014</td>\n",
              "      <td>0.773914</td>\n",
              "      <td>-2.370599</td>\n",
              "      <td>0.269773</td>\n",
              "      <td>0.156617</td>\n",
              "      <td>-0.652450</td>\n",
              "      <td>-0.551572</td>\n",
              "      <td>-0.716522</td>\n",
              "      <td>1.415717</td>\n",
              "      <td>0.555265</td>\n",
              "      <td>0.530507</td>\n",
              "      <td>0.404474</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6338</th>\n",
              "      <td>0.316459</td>\n",
              "      <td>3.809076</td>\n",
              "      <td>-5.615159</td>\n",
              "      <td>6.047445</td>\n",
              "      <td>1.554026</td>\n",
              "      <td>-2.651353</td>\n",
              "      <td>-0.746579</td>\n",
              "      <td>0.055586</td>\n",
              "      <td>-2.678679</td>\n",
              "      <td>-4.959493</td>\n",
              "      <td>6.439053</td>\n",
              "      <td>-7.520117</td>\n",
              "      <td>0.386352</td>\n",
              "      <td>-9.252307</td>\n",
              "      <td>-1.365188</td>\n",
              "      <td>-0.502362</td>\n",
              "      <td>0.784427</td>\n",
              "      <td>1.494305</td>\n",
              "      <td>-1.808012</td>\n",
              "      <td>0.388307</td>\n",
              "      <td>0.208828</td>\n",
              "      <td>-0.511747</td>\n",
              "      <td>-0.583813</td>\n",
              "      <td>-0.219845</td>\n",
              "      <td>1.474753</td>\n",
              "      <td>0.491192</td>\n",
              "      <td>0.518868</td>\n",
              "      <td>0.402528</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6427</th>\n",
              "      <td>0.725646</td>\n",
              "      <td>2.300894</td>\n",
              "      <td>-5.329976</td>\n",
              "      <td>4.007683</td>\n",
              "      <td>-1.730411</td>\n",
              "      <td>-1.732193</td>\n",
              "      <td>-3.968593</td>\n",
              "      <td>1.063728</td>\n",
              "      <td>-0.486097</td>\n",
              "      <td>-4.624985</td>\n",
              "      <td>5.588724</td>\n",
              "      <td>-7.148243</td>\n",
              "      <td>1.680451</td>\n",
              "      <td>-6.210258</td>\n",
              "      <td>0.495282</td>\n",
              "      <td>-3.599540</td>\n",
              "      <td>-4.830324</td>\n",
              "      <td>-0.649090</td>\n",
              "      <td>2.250123</td>\n",
              "      <td>0.504646</td>\n",
              "      <td>0.589669</td>\n",
              "      <td>0.109541</td>\n",
              "      <td>0.601045</td>\n",
              "      <td>-0.364700</td>\n",
              "      <td>-1.843078</td>\n",
              "      <td>0.351909</td>\n",
              "      <td>0.594550</td>\n",
              "      <td>0.099372</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            V1        V2        V3        V4  ...       V27       V28  Amount  Class\n",
              "541  -2.312227  1.951992 -1.609851  3.997906  ...  0.261145 -0.143276    0.00      1\n",
              "623  -3.043541 -3.157307  1.088463  2.288644  ... -0.252773  0.035764  529.00      1\n",
              "4920 -2.303350  1.759247 -0.359745  2.330243  ...  0.039566 -0.153029  239.93      1\n",
              "6108 -4.397974  1.358367 -2.592844  2.679787  ... -0.827136  0.849573   59.00      1\n",
              "6329  1.234235  3.019740 -4.304597  4.732795  ... -0.010016  0.146793    1.00      1\n",
              "6331  0.008430  4.137837 -6.240697  6.675732  ...  0.735822  0.513574    1.00      1\n",
              "6334  0.026779  4.132464 -6.560600  6.348557  ...  0.737040  0.496699    1.00      1\n",
              "6336  0.329594  3.712889 -5.775935  6.078266  ...  0.530507  0.404474    1.00      1\n",
              "6338  0.316459  3.809076 -5.615159  6.047445  ...  0.518868  0.402528    1.00      1\n",
              "6427  0.725646  2.300894 -5.329976  4.007683  ...  0.594550  0.099372    1.00      1\n",
              "\n",
              "[10 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POYaJUxXBto6",
        "colab_type": "text"
      },
      "source": [
        "How many fraudulent transactions are there in the dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmbfxHJBBto8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "67bb9beb-5266-44d0-c407-bcc0e3734a8d"
      },
      "source": [
        "len(df[df[\"Class\"]==1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "492"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOU9vuDhBto_",
        "colab_type": "text"
      },
      "source": [
        "There are only 492 fraudulent transactions recorder, which means that they are:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o37-ZvIVBtpB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "daf45d11-3b74-4cd4-90f5-60971adbed6e"
      },
      "source": [
        "print(\"Fraudulent transactions represent {}% of the entire dataset\".format(len(df[df[\"Class\"]==1])/len(df)*100))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fraudulent transactions represent 0.1727485630620034% of the entire dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PajaOcosBtpF",
        "colab_type": "text"
      },
      "source": [
        "With just 0.17% of fraudulent transactions, a trivial model always predicting 0 (non-fraudulent) would achieve 99.8% of accuracy! As a result, accuracy cannot be considered a proper metric for the present problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Q8ijpXBtpH",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 Training, Validation, Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fMAt2AvBtpH",
        "colab_type": "text"
      },
      "source": [
        "We first reshuffle the dataset, set a parameter indicating the size of the training set and split the remainder of the dataset into validation and test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l54BWboRBtpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#let's reshuffle the dataset\n",
        "df=df.sample(frac=1)\n",
        "\n",
        "#lets create train, validation, test sets\n",
        "p=0.8\n",
        "kiri=int(p*df.shape[0])\n",
        "train_X,val_X,test_X=np.array(df.iloc[:kiri,:-1]),np.array(df.iloc[kiri::2,:-1]),np.array(df.iloc[kiri+1::2,:-1])\n",
        "train_Y,val_Y,test_Y=np.array(df.iloc[:kiri,-1]),np.array(df.iloc[kiri::2,-1]),np.array(df.iloc[kiri+1::2,-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xovuNFszBtpO",
        "colab_type": "text"
      },
      "source": [
        "Count Positive and Negative Examples in each Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52oEEcQ7BtpP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "6710f866-70c3-4d3a-900e-4f99c08c7410"
      },
      "source": [
        "from collections import Counter\n",
        "train_Y_count=Counter(train_Y)\n",
        "val_Y_count=Counter(val_Y)\n",
        "test_Y_count=Counter(test_Y)\n",
        "\n",
        "#Print Statistics\n",
        "print(\"In train set: {} negative and {} positive\".format(train_Y_count[0],train_Y_count[1]))\n",
        "print(\"In val set: {} negative and {} positive\".format(val_Y_count[0],val_Y_count[1]))\n",
        "print(\"In test set: {} negative and {} positive\".format(test_Y_count[0],test_Y_count[1]))\n",
        "print(\"Train Set, Positive/Negative: {} %\".format(train_Y_count[1]/train_Y_count[0]*100))\n",
        "print(\"Val Set, Positive/Negative: {} %\".format(val_Y_count[1]/val_Y_count[0]*100))\n",
        "print(\"Test Set, Positive/Negative: {} %\".format(test_Y_count[1]/test_Y_count[0]*100))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In train set: 227453 negative and 392 positive\n",
            "In val set: 28433 negative and 48 positive\n",
            "In test set: 28429 negative and 52 positive\n",
            "Train Set, Positive/Negative: 0.17234329729658435 %\n",
            "Val Set, Positive/Negative: 0.1688179228361411 %\n",
            "Test Set, Positive/Negative: 0.18291181539976784 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyUSc1JdBtpS",
        "colab_type": "text"
      },
      "source": [
        "Inputs need to be normalized in order to allow a faster training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QFn5wAgBtpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#mean vector\n",
        "mean_vector=train_X.mean(axis=0)\n",
        "#std vector\n",
        "std_vector=train_X.std(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCDSJpt6BtpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalized_train_X,normalized_val_X,normalized_test_X=(train_X-mean_vector)/std_vector,(val_X-mean_vector)/std_vector,(test_X-mean_vector)/std_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBOQJ9dlBtpa",
        "colab_type": "text"
      },
      "source": [
        "This completes the data preprocessing for the present neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D_iPhMrBtpc",
        "colab_type": "text"
      },
      "source": [
        "## 2. Loaders and Architecture\n",
        "In this section, we will create the loaders and build the architecture for the neural network. <br />\n",
        "First of all, we load the proper packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvG64LzXBtpd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJSnK09vBtph",
        "colab_type": "text"
      },
      "source": [
        "Then we check whether a GPU is available for faster training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0catf0UBtpi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "22da3ffa-717e-465d-bfbb-24ab5d2bfab0"
      },
      "source": [
        "on_cuda=torch.cuda.is_available()\n",
        "print(\"GPU is {} available\".format(\"\" if on_cuda else \"not\"))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is  available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn3Z9go7Btpl",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwkcn1ScBtpn",
        "colab_type": "text"
      },
      "source": [
        "on_cuda will from this point on the variable we will use to decide whether to send the variables to GPU. <br />\n",
        "We will then create the loaders making use of TensorDataset/DataLoader utilities. TensorDataset takes as input a torch.tensor representing a list of examples with the respective features and the respective labels; DataLoader will take the entity generated by TensorDataset and make an iterator out of it (in accordance to the set batch_size)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKz2fGLWBtpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TensorDataset creation\n",
        "train_data=TensorDataset(torch.from_numpy(normalized_train_X),torch.from_numpy(train_Y))\n",
        "val_data=TensorDataset(torch.from_numpy(normalized_val_X),torch.from_numpy(val_Y))\n",
        "test_data=TensorDataset(torch.from_numpy(normalized_test_X),torch.from_numpy(test_Y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhXarJY1Btpr",
        "colab_type": "text"
      },
      "source": [
        "Before proceeding, we have to remember that a large imbalance exist between positive and negative examples. As a result, we need to offset this imbalance by sampling from the DataLoader in accordance to a weighted frequency, this can be done using an entity called sampler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m6meiOtBtpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#find out the number positive and negative examples in the training set\n",
        "num_classes=[train_Y_count[0],train_Y_count[1]]\n",
        "#find the respective weights (mind that weights need to be a torch tensor)\n",
        "weights=1./torch.tensor(num_classes,dtype=float)\n",
        "#create a torch tensor associating the train_Y and the weights. Sample Weights is a torch tensor\n",
        "sample_weights=weights[train_Y]\n",
        "sampler=torch.utils.data.WeightedRandomSampler(weights=sample_weights,num_samples=len(sample_weights),replacement=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vrg2Y3QBtpv",
        "colab_type": "text"
      },
      "source": [
        "We finally create the loaders with the DataLoader utility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1aaozg2Btpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=50001 #was 100\n",
        "train_loader=DataLoader(train_data,sampler=sampler,batch_size=batch_size)\n",
        "val_loader=DataLoader(val_data,shuffle=True,batch_size=batch_size)\n",
        "test_loader=DataLoader(test_data,shuffle=False,batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9tD082vBtpy",
        "colab_type": "text"
      },
      "source": [
        "As a sanity check, let's print out the output of the validation loader, as an example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34BtfBEjBtp0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "bf138527-f7d9-4a23-e6cb-376a08538318"
      },
      "source": [
        "train_iter=iter(val_loader)\n",
        "inputs,outputs=train_iter.next()\n",
        "print(inputs)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.1311, -0.2871, -1.8085,  ..., -0.3400, -0.2974, -0.2276],\n",
            "        [ 0.1439, -1.8976, -1.3114,  ..., -0.3476,  0.3676,  3.2563],\n",
            "        [ 1.0957, -0.0452, -1.6993,  ..., -0.2607, -0.2908, -0.3431],\n",
            "        ...,\n",
            "        [ 0.0568,  0.4326, -0.1185,  ...,  0.5834,  0.2555, -0.3229],\n",
            "        [-0.1541,  0.4983, -0.1982,  ...,  0.6136,  0.2790, -0.3471],\n",
            "        [ 0.6192, -0.1519,  0.0944,  ...,  0.0105,  0.0567, -0.2274]],\n",
            "       dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwlEtrAABtp5",
        "colab_type": "text"
      },
      "source": [
        "And check the dimensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8rsekx9Btp6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "09934d4b-e469-4083-9975-d657798f9e9b"
      },
      "source": [
        "print(inputs.size())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([28481, 29])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4bypog7Btp-",
        "colab_type": "text"
      },
      "source": [
        "This is compatible with the validation set size. <br />\n",
        "Finally, let's wrap the loaders into a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdJlpGCSBtp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loader wrapping\n",
        "loaders={'train':train_loader, 'valid':val_loader}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwNl-ICgBtqA",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmVeQtsPBtqB",
        "colab_type": "text"
      },
      "source": [
        "We now build the neural network. I am opting for a 6 layer architecture. Mind that this is not necessarily the optimal solution, but it worked quite well in this case. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au8RCDLtBtqC",
        "colab_type": "text"
      },
      "source": [
        "The architecture comprises 6 fully-connected layers. \n",
        "The layers follow the sizes below: <br />\n",
        "1. 29,100\n",
        "2. 100,50\n",
        "3. 50,25\n",
        "4. 25,8\n",
        "5. 8,3\n",
        "6. 3,1 <br/>\n",
        "\n",
        "Layer 2,3,4,5 use batch normalization. As a result, they do not need the calculation of the bias, as this would cancel out owing to the normalization. Also, all layers require ReLu activation function, except the last, of which we will just take the logits (i.e. no activation function required)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x3sLvUkBtqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kvS0qbRBtqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net,self).__init__()\n",
        "        self.fc1=nn.Linear(29,100)\n",
        "        self.fc2=nn.Linear(100,50,bias=False)\n",
        "        self.norm2=nn.BatchNorm1d(50)\n",
        "        self.fc3=nn.Linear(50,25,bias=False)\n",
        "        self.norm3=nn.BatchNorm1d(25)\n",
        "        self.fc4=nn.Linear(25,8,bias=False)\n",
        "        self.norm4=nn.BatchNorm1d(8)\n",
        "        self.fc5=nn.Linear(8,3,bias=False)\n",
        "        self.norm5=nn.BatchNorm1d(3)\n",
        "        self.fc6=nn.Linear(3,1)        \n",
        "        \n",
        "    def forward(self,x):\n",
        "        x=F.selu(self.fc1(x))\n",
        "        x=F.selu(self.norm2(self.fc2(x)))\n",
        "        x=F.selu(self.norm3(self.fc3(x)))\n",
        "        x=F.selu(self.norm4(self.fc4(x)))\n",
        "        x=F.selu(self.norm5(self.fc5(x)))\n",
        "        x=self.fc6(x)        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5qBGg3TBtqH",
        "colab_type": "text"
      },
      "source": [
        "We now create the neural network and send it to the GPU if that is available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGEW9kmtBtqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=Net()\n",
        "if on_cuda:\n",
        "    model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQOxLnxjBtqL",
        "colab_type": "text"
      },
      "source": [
        "Following, we set up the loss function. In this case, we opt for the BCEWithLogitsLoss, which is slightly more numerically stable than the BCELoss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__AQaV4OBtqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion=nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKcWqn_JBtqN",
        "colab_type": "text"
      },
      "source": [
        "Then we set up Adam as an optimizer, with a learning rate of 0.01 (we can afford a high learning rate as the batch size is relatively large) and a weight_decay of 1e-4. Weight Decay in pytorch establishes the weight for L2 normalization on the Loss Function. Also, a scheduler is attached so as to decrease the learning rate when the loss reaches a plateau (the tracked loss will be the validation loss, which is calculated inside the training routine)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04HP8lpUBtqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer=torch.optim.Adam(model.parameters(),lr=0.01,weight_decay=1e-4)\n",
        "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode=\"min\",factor=0.75,patience=50,min_lr=1e-5) #1e-6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03TIUHj_BtqR",
        "colab_type": "text"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv4gWNqdBtqS",
        "colab_type": "text"
      },
      "source": [
        "We now need to write down the training routine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvqrObI8BtqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(n_epochs,loaders,model,optimizer,criterion,scheduler):\n",
        "    #Trains the neural network passed in \"model\", for a number of\n",
        "    #epochs equal to n_epochs, on the loaders contained in the \n",
        "    #dictionary \"loaders\", using the optimizer in \"optimizer\", \n",
        "    #with the loss function defined in \"criterion\" and, if necessary\n",
        "    #decreases the learning rate in accordance to the rules contained \n",
        "    #in \"scheduler\"\n",
        "    valid_loss_min=np.Inf\n",
        "    list_train_loss=[]\n",
        "    list_valid_loss=[]\n",
        "    for epoch in range(1,n_epochs+1):\n",
        "        train_loss=0.0\n",
        "        valid_loss=0.0\n",
        "        model.train()\n",
        "        for batch_idx,(data,target) in enumerate(loaders[\"train\"]):\n",
        "            optimizer.zero_grad()\n",
        "            if on_cuda:\n",
        "                data,target=data.cuda(),target.cuda()\n",
        "            output=model(data.float())\n",
        "            loss=criterion(output.squeeze(),target.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss=train_loss+1/(batch_idx+1)*(loss.item()-train_loss)\n",
        "        list_train_loss.append(train_loss)\n",
        "        print(\"At {} epoch, Training Loss: {} \".format(epoch,train_loss))\n",
        "        model.eval()\n",
        "        for batch_idx,(data,target) in enumerate(loaders[\"valid\"]):\n",
        "            if on_cuda:\n",
        "                data,target=data.cuda(),target.cuda()\n",
        "            output=model(data.float())\n",
        "            loss=criterion(output.squeeze(),target.float())\n",
        "            valid_loss=valid_loss+1/(batch_idx+1)*(loss.item()-valid_loss)\n",
        "        scheduler.step(valid_loss)\n",
        "        list_valid_loss.append(valid_loss)\n",
        "        print(\"At {} epoch, Validation Loss: {} \".format(epoch,valid_loss))\n",
        "    \n",
        "        #Save the model\n",
        "        if valid_loss < valid_loss_min:\n",
        "            torch.save(model.state_dict(),'ccFraud.pt')\n",
        "            print(\"Minimum validation loss detected, saving model......................................................................................\")\n",
        "            valid_loss_min=valid_loss\n",
        "    \n",
        "    return model, list_train_loss, list_valid_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La9xxZeTBtqU",
        "colab_type": "text"
      },
      "source": [
        "And train the model for a specified number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spR_-wFJBtqW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "25232372-b585-4bf2-d6c4-212b0a650175"
      },
      "source": [
        "n_epoch=1500\n",
        "model,train_loss,valid_loss=train(n_epoch,loaders,model,optimizer,criterion,scheduler) "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "At 1 epoch, Training Loss: 0.49385441541671754 \n",
            "At 1 epoch, Validation Loss: 0.38573145866394043 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 2 epoch, Training Loss: 0.380945748090744 \n",
            "At 2 epoch, Validation Loss: 0.3524896502494812 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 3 epoch, Training Loss: 0.32456547021865845 \n",
            "At 3 epoch, Validation Loss: 0.3218713104724884 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 4 epoch, Training Loss: 0.27823286056518554 \n",
            "At 4 epoch, Validation Loss: 0.28621751070022583 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 5 epoch, Training Loss: 0.23964687287807465 \n",
            "At 5 epoch, Validation Loss: 0.25682392716407776 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 6 epoch, Training Loss: 0.2079464912414551 \n",
            "At 6 epoch, Validation Loss: 0.2206549495458603 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 7 epoch, Training Loss: 0.180183282494545 \n",
            "At 7 epoch, Validation Loss: 0.1935986578464508 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 8 epoch, Training Loss: 0.1561034381389618 \n",
            "At 8 epoch, Validation Loss: 0.18363647162914276 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 9 epoch, Training Loss: 0.16257118284702302 \n",
            "At 9 epoch, Validation Loss: 0.10668523609638214 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 10 epoch, Training Loss: 0.14611706137657166 \n",
            "At 10 epoch, Validation Loss: 0.15829534828662872 \n",
            "At 11 epoch, Training Loss: 0.13295100629329681 \n",
            "At 11 epoch, Validation Loss: 0.15697036683559418 \n",
            "At 12 epoch, Training Loss: 0.11900917291641236 \n",
            "At 12 epoch, Validation Loss: 0.13204121589660645 \n",
            "At 13 epoch, Training Loss: 0.10560490936040878 \n",
            "At 13 epoch, Validation Loss: 0.13598337769508362 \n",
            "At 14 epoch, Training Loss: 0.09564561247825623 \n",
            "At 14 epoch, Validation Loss: 0.13712535798549652 \n",
            "At 15 epoch, Training Loss: 0.08492953330278397 \n",
            "At 15 epoch, Validation Loss: 0.10760791599750519 \n",
            "At 16 epoch, Training Loss: 0.07638600021600724 \n",
            "At 16 epoch, Validation Loss: 0.07747223973274231 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 17 epoch, Training Loss: 0.06904533207416534 \n",
            "At 17 epoch, Validation Loss: 0.06052865833044052 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 18 epoch, Training Loss: 0.06375736221671105 \n",
            "At 18 epoch, Validation Loss: 0.07577452063560486 \n",
            "At 19 epoch, Training Loss: 0.06128691732883453 \n",
            "At 19 epoch, Validation Loss: 0.06061141565442085 \n",
            "At 20 epoch, Training Loss: 0.05275376886129379 \n",
            "At 20 epoch, Validation Loss: 0.06585875153541565 \n",
            "At 21 epoch, Training Loss: 0.04414069950580597 \n",
            "At 21 epoch, Validation Loss: 0.049343325197696686 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 22 epoch, Training Loss: 0.03777753934264183 \n",
            "At 22 epoch, Validation Loss: 0.051957935094833374 \n",
            "At 23 epoch, Training Loss: 0.03577431887388229 \n",
            "At 23 epoch, Validation Loss: 0.033931486308574677 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 24 epoch, Training Loss: 0.035345178470015524 \n",
            "At 24 epoch, Validation Loss: 0.029190832749009132 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 25 epoch, Training Loss: 0.033393848687410355 \n",
            "At 25 epoch, Validation Loss: 0.042353712022304535 \n",
            "At 26 epoch, Training Loss: 0.027150562033057213 \n",
            "At 26 epoch, Validation Loss: 0.0257897786796093 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 27 epoch, Training Loss: 0.022949591279029846 \n",
            "At 27 epoch, Validation Loss: 0.03439803421497345 \n",
            "At 28 epoch, Training Loss: 0.020728829503059387 \n",
            "At 28 epoch, Validation Loss: 0.02282763086259365 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 29 epoch, Training Loss: 0.01704886816442013 \n",
            "At 29 epoch, Validation Loss: 0.027391204610466957 \n",
            "At 30 epoch, Training Loss: 0.026580093055963518 \n",
            "At 30 epoch, Validation Loss: 0.01880018599331379 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 31 epoch, Training Loss: 0.02912355326116085 \n",
            "At 31 epoch, Validation Loss: 0.032665662467479706 \n",
            "At 32 epoch, Training Loss: 0.022365430369973183 \n",
            "At 32 epoch, Validation Loss: 0.042132358998060226 \n",
            "At 33 epoch, Training Loss: 0.019595085084438323 \n",
            "At 33 epoch, Validation Loss: 0.032490383833646774 \n",
            "At 34 epoch, Training Loss: 0.016204469092190265 \n",
            "At 34 epoch, Validation Loss: 0.019902916625142097 \n",
            "At 35 epoch, Training Loss: 0.01691090501844883 \n",
            "At 35 epoch, Validation Loss: 0.015186143107712269 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 36 epoch, Training Loss: 0.01390795037150383 \n",
            "At 36 epoch, Validation Loss: 0.018589410930871964 \n",
            "At 37 epoch, Training Loss: 0.011522763222455979 \n",
            "At 37 epoch, Validation Loss: 0.01748366840183735 \n",
            "At 38 epoch, Training Loss: 0.009795298241078854 \n",
            "At 38 epoch, Validation Loss: 0.01690049096941948 \n",
            "At 39 epoch, Training Loss: 0.009758018329739571 \n",
            "At 39 epoch, Validation Loss: 0.00950930267572403 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 40 epoch, Training Loss: 0.03049870077520609 \n",
            "At 40 epoch, Validation Loss: 0.026847777888178825 \n",
            "At 41 epoch, Training Loss: 0.02712896429002285 \n",
            "At 41 epoch, Validation Loss: 0.03721396252512932 \n",
            "At 42 epoch, Training Loss: 0.019696725904941557 \n",
            "At 42 epoch, Validation Loss: 0.03142061084508896 \n",
            "At 43 epoch, Training Loss: 0.015775607898831367 \n",
            "At 43 epoch, Validation Loss: 0.028592415153980255 \n",
            "At 44 epoch, Training Loss: 0.012444815039634705 \n",
            "At 44 epoch, Validation Loss: 0.021970022469758987 \n",
            "At 45 epoch, Training Loss: 0.01089908294379711 \n",
            "At 45 epoch, Validation Loss: 0.014682927168905735 \n",
            "At 46 epoch, Training Loss: 0.009288582392036915 \n",
            "At 46 epoch, Validation Loss: 0.010195307433605194 \n",
            "At 47 epoch, Training Loss: 0.00862430427223444 \n",
            "At 47 epoch, Validation Loss: 0.008697816170752048 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 48 epoch, Training Loss: 0.007390239275991917 \n",
            "At 48 epoch, Validation Loss: 0.00904612336307764 \n",
            "At 49 epoch, Training Loss: 0.0068516327068209645 \n",
            "At 49 epoch, Validation Loss: 0.008234196342527866 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 50 epoch, Training Loss: 0.005876798834651709 \n",
            "At 50 epoch, Validation Loss: 0.007015482522547245 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 51 epoch, Training Loss: 0.0059261775575578214 \n",
            "At 51 epoch, Validation Loss: 0.006345654837787151 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 52 epoch, Training Loss: 0.005591433309018612 \n",
            "At 52 epoch, Validation Loss: 0.005638398230075836 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 53 epoch, Training Loss: 0.005219300743192434 \n",
            "At 53 epoch, Validation Loss: 0.005662508774548769 \n",
            "At 54 epoch, Training Loss: 0.004952842462807893 \n",
            "At 54 epoch, Validation Loss: 0.005782701075077057 \n",
            "At 55 epoch, Training Loss: 0.004829129856079817 \n",
            "At 55 epoch, Validation Loss: 0.006366482935845852 \n",
            "At 56 epoch, Training Loss: 0.0044880244880914685 \n",
            "At 56 epoch, Validation Loss: 0.005687304772436619 \n",
            "At 57 epoch, Training Loss: 0.004308845708146691 \n",
            "At 57 epoch, Validation Loss: 0.005602906458079815 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 58 epoch, Training Loss: 0.004060036549344659 \n",
            "At 58 epoch, Validation Loss: 0.005860802251845598 \n",
            "At 59 epoch, Training Loss: 0.003970739711076021 \n",
            "At 59 epoch, Validation Loss: 0.005927476566284895 \n",
            "At 60 epoch, Training Loss: 0.004098595399409532 \n",
            "At 60 epoch, Validation Loss: 0.00851376261562109 \n",
            "At 61 epoch, Training Loss: 0.0045033726841211316 \n",
            "At 61 epoch, Validation Loss: 0.0076340618543326855 \n",
            "At 62 epoch, Training Loss: 0.36170459985733033 \n",
            "At 62 epoch, Validation Loss: 0.8377472162246704 \n",
            "At 63 epoch, Training Loss: 0.23388477563858032 \n",
            "At 63 epoch, Validation Loss: 0.3967445492744446 \n",
            "At 64 epoch, Training Loss: 0.1569476008415222 \n",
            "At 64 epoch, Validation Loss: 0.4893004298210144 \n",
            "At 65 epoch, Training Loss: 0.13163301348686218 \n",
            "At 65 epoch, Validation Loss: 0.25390899181365967 \n",
            "At 66 epoch, Training Loss: 0.11750158518552781 \n",
            "At 66 epoch, Validation Loss: 0.23237372934818268 \n",
            "At 67 epoch, Training Loss: 0.10818500816822052 \n",
            "At 67 epoch, Validation Loss: 0.15486444532871246 \n",
            "At 68 epoch, Training Loss: 0.10273583829402924 \n",
            "At 68 epoch, Validation Loss: 0.10924335569143295 \n",
            "At 69 epoch, Training Loss: 0.09422861784696579 \n",
            "At 69 epoch, Validation Loss: 0.08078613877296448 \n",
            "At 70 epoch, Training Loss: 0.08852714151144028 \n",
            "At 70 epoch, Validation Loss: 0.06938059628009796 \n",
            "At 71 epoch, Training Loss: 0.08044221401214599 \n",
            "At 71 epoch, Validation Loss: 0.04989669844508171 \n",
            "At 72 epoch, Training Loss: 0.07126055359840393 \n",
            "At 72 epoch, Validation Loss: 0.03630072623491287 \n",
            "At 73 epoch, Training Loss: 0.063606958091259 \n",
            "At 73 epoch, Validation Loss: 0.0381084568798542 \n",
            "At 74 epoch, Training Loss: 0.05712230131030083 \n",
            "At 74 epoch, Validation Loss: 0.027094051241874695 \n",
            "At 75 epoch, Training Loss: 0.05185647234320641 \n",
            "At 75 epoch, Validation Loss: 0.05881677567958832 \n",
            "At 76 epoch, Training Loss: 0.04780581519007683 \n",
            "At 76 epoch, Validation Loss: 0.061125967651605606 \n",
            "At 77 epoch, Training Loss: 0.038440392911434175 \n",
            "At 77 epoch, Validation Loss: 0.055956412106752396 \n",
            "At 78 epoch, Training Loss: 0.032712575793266294 \n",
            "At 78 epoch, Validation Loss: 0.03001958690583706 \n",
            "At 79 epoch, Training Loss: 0.025063694268465043 \n",
            "At 79 epoch, Validation Loss: 0.019666185602545738 \n",
            "At 80 epoch, Training Loss: 0.02025250978767872 \n",
            "At 80 epoch, Validation Loss: 0.01587565243244171 \n",
            "At 81 epoch, Training Loss: 0.025045227259397507 \n",
            "At 81 epoch, Validation Loss: 0.02486739307641983 \n",
            "At 82 epoch, Training Loss: 0.024566983059048652 \n",
            "At 82 epoch, Validation Loss: 0.02309306152164936 \n",
            "At 83 epoch, Training Loss: 0.019452685862779616 \n",
            "At 83 epoch, Validation Loss: 0.014461774379014969 \n",
            "At 84 epoch, Training Loss: 0.0160896398127079 \n",
            "At 84 epoch, Validation Loss: 0.013518447987735271 \n",
            "At 85 epoch, Training Loss: 0.013513448089361191 \n",
            "At 85 epoch, Validation Loss: 0.012538878247141838 \n",
            "At 86 epoch, Training Loss: 0.011819163151085377 \n",
            "At 86 epoch, Validation Loss: 0.011588823981583118 \n",
            "At 87 epoch, Training Loss: 0.010742929577827454 \n",
            "At 87 epoch, Validation Loss: 0.01161922886967659 \n",
            "At 88 epoch, Training Loss: 0.009683016501367093 \n",
            "At 88 epoch, Validation Loss: 0.008442841470241547 \n",
            "At 89 epoch, Training Loss: 0.00912193451076746 \n",
            "At 89 epoch, Validation Loss: 0.0063112275674939156 \n",
            "At 90 epoch, Training Loss: 0.008349863160401583 \n",
            "At 90 epoch, Validation Loss: 0.006127630360424519 \n",
            "At 91 epoch, Training Loss: 0.05638899803161621 \n",
            "At 91 epoch, Validation Loss: 1.5681521892547607 \n",
            "At 92 epoch, Training Loss: 0.05058344751596451 \n",
            "At 92 epoch, Validation Loss: 1.2081184387207031 \n",
            "At 93 epoch, Training Loss: 0.07226301506161689 \n",
            "At 93 epoch, Validation Loss: 0.07684142142534256 \n",
            "At 94 epoch, Training Loss: 0.08566747456789017 \n",
            "At 94 epoch, Validation Loss: 0.035247739404439926 \n",
            "At 95 epoch, Training Loss: 0.06593966335058213 \n",
            "At 95 epoch, Validation Loss: 0.011169358156621456 \n",
            "At 96 epoch, Training Loss: 0.04782522693276405 \n",
            "At 96 epoch, Validation Loss: 0.0068170479498803616 \n",
            "At 97 epoch, Training Loss: 0.032895172759890554 \n",
            "At 97 epoch, Validation Loss: 0.004824054427444935 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 98 epoch, Training Loss: 0.023418600484728814 \n",
            "At 98 epoch, Validation Loss: 0.004875713028013706 \n",
            "At 99 epoch, Training Loss: 0.0193670067936182 \n",
            "At 99 epoch, Validation Loss: 0.005113421007990837 \n",
            "At 100 epoch, Training Loss: 0.014558394998311996 \n",
            "At 100 epoch, Validation Loss: 0.005555414129048586 \n",
            "At 101 epoch, Training Loss: 0.012633275240659714 \n",
            "At 101 epoch, Validation Loss: 0.006170944310724735 \n",
            "At 102 epoch, Training Loss: 0.011713871359825134 \n",
            "At 102 epoch, Validation Loss: 0.006173971574753523 \n",
            "At 103 epoch, Training Loss: 0.009648696146905422 \n",
            "At 103 epoch, Validation Loss: 0.006408886518329382 \n",
            "At 104 epoch, Training Loss: 0.009102557599544526 \n",
            "At 104 epoch, Validation Loss: 0.00682090362533927 \n",
            "At 105 epoch, Training Loss: 0.008139545749872922 \n",
            "At 105 epoch, Validation Loss: 0.007451817858964205 \n",
            "At 106 epoch, Training Loss: 0.007329877465963364 \n",
            "At 106 epoch, Validation Loss: 0.007943699136376381 \n",
            "At 107 epoch, Training Loss: 0.006814681645482779 \n",
            "At 107 epoch, Validation Loss: 0.007944608107209206 \n",
            "At 108 epoch, Training Loss: 0.006071174051612616 \n",
            "At 108 epoch, Validation Loss: 0.008275865577161312 \n",
            "At 109 epoch, Training Loss: 0.005979134421795607 \n",
            "At 109 epoch, Validation Loss: 0.008276963606476784 \n",
            "At 110 epoch, Training Loss: 0.005998170375823975 \n",
            "At 110 epoch, Validation Loss: 0.007811151910573244 \n",
            "At 111 epoch, Training Loss: 0.005909286253154278 \n",
            "At 111 epoch, Validation Loss: 0.008556213229894638 \n",
            "At 112 epoch, Training Loss: 0.0052100070752203465 \n",
            "At 112 epoch, Validation Loss: 0.0073954202234745026 \n",
            "At 113 epoch, Training Loss: 0.0050176002085208895 \n",
            "At 113 epoch, Validation Loss: 0.007336207665503025 \n",
            "At 114 epoch, Training Loss: 0.005078056827187538 \n",
            "At 114 epoch, Validation Loss: 0.007376697845757008 \n",
            "At 115 epoch, Training Loss: 0.004708908405154944 \n",
            "At 115 epoch, Validation Loss: 0.006201860029250383 \n",
            "At 116 epoch, Training Loss: 0.004929462634027004 \n",
            "At 116 epoch, Validation Loss: 0.0066431122832000256 \n",
            "At 117 epoch, Training Loss: 0.004574196133762598 \n",
            "At 117 epoch, Validation Loss: 0.00668081222102046 \n",
            "At 118 epoch, Training Loss: 0.004472566768527031 \n",
            "At 118 epoch, Validation Loss: 0.007401322014629841 \n",
            "At 119 epoch, Training Loss: 0.004148058267310262 \n",
            "At 119 epoch, Validation Loss: 0.006465505342930555 \n",
            "At 120 epoch, Training Loss: 0.004580773087218404 \n",
            "At 120 epoch, Validation Loss: 0.006878013256937265 \n",
            "At 121 epoch, Training Loss: 0.0041600150056183335 \n",
            "At 121 epoch, Validation Loss: 0.006225514691323042 \n",
            "At 122 epoch, Training Loss: 0.0038082774728536604 \n",
            "At 122 epoch, Validation Loss: 0.0061589814722537994 \n",
            "At 123 epoch, Training Loss: 0.003606080776080489 \n",
            "At 123 epoch, Validation Loss: 0.006471862550824881 \n",
            "At 124 epoch, Training Loss: 0.0038649818394333123 \n",
            "At 124 epoch, Validation Loss: 0.006161661818623543 \n",
            "At 125 epoch, Training Loss: 0.003476448869332671 \n",
            "At 125 epoch, Validation Loss: 0.006723619066178799 \n",
            "At 126 epoch, Training Loss: 0.003992816433310509 \n",
            "At 126 epoch, Validation Loss: 0.00678744912147522 \n",
            "At 127 epoch, Training Loss: 0.003586559044197202 \n",
            "At 127 epoch, Validation Loss: 0.005488493945449591 \n",
            "At 128 epoch, Training Loss: 0.0034447495359927415 \n",
            "At 128 epoch, Validation Loss: 0.0061126104556024075 \n",
            "At 129 epoch, Training Loss: 0.0030100478790700434 \n",
            "At 129 epoch, Validation Loss: 0.006462009157985449 \n",
            "At 130 epoch, Training Loss: 0.003430379135534167 \n",
            "At 130 epoch, Validation Loss: 0.006223544478416443 \n",
            "At 131 epoch, Training Loss: 0.0031781940255314113 \n",
            "At 131 epoch, Validation Loss: 0.0055135455913841724 \n",
            "At 132 epoch, Training Loss: 0.0038214656990021465 \n",
            "At 132 epoch, Validation Loss: 0.005832039751112461 \n",
            "At 133 epoch, Training Loss: 0.00372270280495286 \n",
            "At 133 epoch, Validation Loss: 0.005464034620672464 \n",
            "At 134 epoch, Training Loss: 0.004085003212094307 \n",
            "At 134 epoch, Validation Loss: 0.004753542598336935 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 135 epoch, Training Loss: 0.004713599849492311 \n",
            "At 135 epoch, Validation Loss: 0.00532083585858345 \n",
            "At 136 epoch, Training Loss: 0.005762727884575725 \n",
            "At 136 epoch, Validation Loss: 0.005928779020905495 \n",
            "At 137 epoch, Training Loss: 0.005133429449051619 \n",
            "At 137 epoch, Validation Loss: 0.007198556791990995 \n",
            "At 138 epoch, Training Loss: 0.004365735407918692 \n",
            "At 138 epoch, Validation Loss: 0.007224578410387039 \n",
            "At 139 epoch, Training Loss: 0.0039314155001193285 \n",
            "At 139 epoch, Validation Loss: 0.006564824841916561 \n",
            "At 140 epoch, Training Loss: 0.004341702070087194 \n",
            "At 140 epoch, Validation Loss: 0.005668338388204575 \n",
            "At 141 epoch, Training Loss: 0.003925990127027035 \n",
            "At 141 epoch, Validation Loss: 0.007678304333239794 \n",
            "At 142 epoch, Training Loss: 0.005856192577630282 \n",
            "At 142 epoch, Validation Loss: 0.011741762049496174 \n",
            "At 143 epoch, Training Loss: 0.007293108198791742 \n",
            "At 143 epoch, Validation Loss: 0.02337711676955223 \n",
            "At 144 epoch, Training Loss: 0.01269778711721301 \n",
            "At 144 epoch, Validation Loss: 0.01636441797018051 \n",
            "At 145 epoch, Training Loss: 0.01095187859609723 \n",
            "At 145 epoch, Validation Loss: 0.024827778339385986 \n",
            "At 146 epoch, Training Loss: 0.010002530366182327 \n",
            "At 146 epoch, Validation Loss: 0.015760574489831924 \n",
            "At 147 epoch, Training Loss: 0.008497482631355524 \n",
            "At 147 epoch, Validation Loss: 0.021797452121973038 \n",
            "At 148 epoch, Training Loss: 0.006313312519341707 \n",
            "At 148 epoch, Validation Loss: 0.014768906868994236 \n",
            "At 149 epoch, Training Loss: 0.005911485012620688 \n",
            "At 149 epoch, Validation Loss: 0.010453816503286362 \n",
            "At 150 epoch, Training Loss: 0.004463283624500036 \n",
            "At 150 epoch, Validation Loss: 0.00874154269695282 \n",
            "At 151 epoch, Training Loss: 0.004243370611220598 \n",
            "At 151 epoch, Validation Loss: 0.007062392774969339 \n",
            "At 152 epoch, Training Loss: 0.0034136119298636912 \n",
            "At 152 epoch, Validation Loss: 0.0058320751413702965 \n",
            "At 153 epoch, Training Loss: 0.003369826404377818 \n",
            "At 153 epoch, Validation Loss: 0.006154151633381844 \n",
            "At 154 epoch, Training Loss: 0.0033801390323787927 \n",
            "At 154 epoch, Validation Loss: 0.005779625847935677 \n",
            "At 155 epoch, Training Loss: 0.00299277207814157 \n",
            "At 155 epoch, Validation Loss: 0.00503561832010746 \n",
            "At 156 epoch, Training Loss: 0.0030760268215090036 \n",
            "At 156 epoch, Validation Loss: 0.004904539790004492 \n",
            "At 157 epoch, Training Loss: 0.002688723849132657 \n",
            "At 157 epoch, Validation Loss: 0.004724555648863316 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 158 epoch, Training Loss: 0.0026984917465597393 \n",
            "At 158 epoch, Validation Loss: 0.00489797443151474 \n",
            "At 159 epoch, Training Loss: 0.0025387205881997944 \n",
            "At 159 epoch, Validation Loss: 0.005421387031674385 \n",
            "At 160 epoch, Training Loss: 0.0027290900237858296 \n",
            "At 160 epoch, Validation Loss: 0.004664651583880186 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 161 epoch, Training Loss: 0.0025783882476389406 \n",
            "At 161 epoch, Validation Loss: 0.006060310639441013 \n",
            "At 162 epoch, Training Loss: 0.0028502169530838727 \n",
            "At 162 epoch, Validation Loss: 0.004767811857163906 \n",
            "At 163 epoch, Training Loss: 0.0026631344109773634 \n",
            "At 163 epoch, Validation Loss: 0.004975170828402042 \n",
            "At 164 epoch, Training Loss: 0.002386053930968046 \n",
            "At 164 epoch, Validation Loss: 0.004820303060114384 \n",
            "At 165 epoch, Training Loss: 0.0023314997786656024 \n",
            "At 165 epoch, Validation Loss: 0.004558454733341932 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 166 epoch, Training Loss: 0.002621730393730104 \n",
            "At 166 epoch, Validation Loss: 0.005078040063381195 \n",
            "At 167 epoch, Training Loss: 0.002236217702738941 \n",
            "At 167 epoch, Validation Loss: 0.00499568460509181 \n",
            "At 168 epoch, Training Loss: 0.0021786640863865614 \n",
            "At 168 epoch, Validation Loss: 0.004505013581365347 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 169 epoch, Training Loss: 0.0022789291106164455 \n",
            "At 169 epoch, Validation Loss: 0.00498699676245451 \n",
            "At 170 epoch, Training Loss: 0.0022851759102195503 \n",
            "At 170 epoch, Validation Loss: 0.004513151478022337 \n",
            "At 171 epoch, Training Loss: 0.002258524065837264 \n",
            "At 171 epoch, Validation Loss: 0.004742606543004513 \n",
            "At 172 epoch, Training Loss: 0.002247129217721522 \n",
            "At 172 epoch, Validation Loss: 0.005533856805413961 \n",
            "At 173 epoch, Training Loss: 0.0019699102034792305 \n",
            "At 173 epoch, Validation Loss: 0.004697741474956274 \n",
            "At 174 epoch, Training Loss: 0.0021671014139428733 \n",
            "At 174 epoch, Validation Loss: 0.004728586412966251 \n",
            "At 175 epoch, Training Loss: 0.0018982384819537402 \n",
            "At 175 epoch, Validation Loss: 0.004486054182052612 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 176 epoch, Training Loss: 0.0020075897220522164 \n",
            "At 176 epoch, Validation Loss: 0.004638056270778179 \n",
            "At 177 epoch, Training Loss: 0.0018680153414607048 \n",
            "At 177 epoch, Validation Loss: 0.004428316373378038 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 178 epoch, Training Loss: 0.0019346005748957396 \n",
            "At 178 epoch, Validation Loss: 0.005223831161856651 \n",
            "At 179 epoch, Training Loss: 0.0019480883842334152 \n",
            "At 179 epoch, Validation Loss: 0.0045969849452376366 \n",
            "At 180 epoch, Training Loss: 0.0019637659890577195 \n",
            "At 180 epoch, Validation Loss: 0.004588997922837734 \n",
            "At 181 epoch, Training Loss: 0.0016297909896820784 \n",
            "At 181 epoch, Validation Loss: 0.005302014760673046 \n",
            "At 182 epoch, Training Loss: 0.0016132978722453117 \n",
            "At 182 epoch, Validation Loss: 0.004842495545744896 \n",
            "At 183 epoch, Training Loss: 0.002283930662088096 \n",
            "At 183 epoch, Validation Loss: 0.005187406670302153 \n",
            "At 184 epoch, Training Loss: 0.0018710900098085404 \n",
            "At 184 epoch, Validation Loss: 0.0045340945944190025 \n",
            "At 185 epoch, Training Loss: 0.0019366494845598937 \n",
            "At 185 epoch, Validation Loss: 0.005258032586425543 \n",
            "At 186 epoch, Training Loss: 0.002041736710816622 \n",
            "At 186 epoch, Validation Loss: 0.004507227800786495 \n",
            "At 187 epoch, Training Loss: 0.0020166845759376885 \n",
            "At 187 epoch, Validation Loss: 0.0051551456563174725 \n",
            "At 188 epoch, Training Loss: 0.002234128094278276 \n",
            "At 188 epoch, Validation Loss: 0.005660063121467829 \n",
            "At 189 epoch, Training Loss: 0.0029456781689077617 \n",
            "At 189 epoch, Validation Loss: 0.007693755440413952 \n",
            "At 190 epoch, Training Loss: 0.0639139384496957 \n",
            "At 190 epoch, Validation Loss: 0.5078059434890747 \n",
            "At 191 epoch, Training Loss: 0.11988048553466797 \n",
            "At 191 epoch, Validation Loss: 0.43881934881210327 \n",
            "At 192 epoch, Training Loss: 0.07173573821783066 \n",
            "At 192 epoch, Validation Loss: 0.3677073121070862 \n",
            "At 193 epoch, Training Loss: 0.03789891973137856 \n",
            "At 193 epoch, Validation Loss: 0.48008808493614197 \n",
            "At 194 epoch, Training Loss: 0.027855377271771432 \n",
            "At 194 epoch, Validation Loss: 0.24810951948165894 \n",
            "At 195 epoch, Training Loss: 0.020215488597750665 \n",
            "At 195 epoch, Validation Loss: 0.03962181508541107 \n",
            "At 196 epoch, Training Loss: 0.016175415925681592 \n",
            "At 196 epoch, Validation Loss: 0.009849277324974537 \n",
            "At 197 epoch, Training Loss: 0.013013695552945136 \n",
            "At 197 epoch, Validation Loss: 0.005758168641477823 \n",
            "At 198 epoch, Training Loss: 0.010478903725743293 \n",
            "At 198 epoch, Validation Loss: 0.004412263166159391 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 199 epoch, Training Loss: 0.008439345750957727 \n",
            "At 199 epoch, Validation Loss: 0.004260883666574955 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 200 epoch, Training Loss: 0.007272141706198454 \n",
            "At 200 epoch, Validation Loss: 0.0043048555962741375 \n",
            "At 201 epoch, Training Loss: 0.006010130792856216 \n",
            "At 201 epoch, Validation Loss: 0.0046423571184277534 \n",
            "At 202 epoch, Training Loss: 0.0056512652896344665 \n",
            "At 202 epoch, Validation Loss: 0.004690091125667095 \n",
            "At 203 epoch, Training Loss: 0.005278524849563837 \n",
            "At 203 epoch, Validation Loss: 0.004220650531351566 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 204 epoch, Training Loss: 0.0048593939282000065 \n",
            "At 204 epoch, Validation Loss: 0.004341726657003164 \n",
            "At 205 epoch, Training Loss: 0.004369669500738383 \n",
            "At 205 epoch, Validation Loss: 0.004161176737397909 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 206 epoch, Training Loss: 0.004037287272512913 \n",
            "At 206 epoch, Validation Loss: 0.004583192523568869 \n",
            "At 207 epoch, Training Loss: 0.0038164530880749224 \n",
            "At 207 epoch, Validation Loss: 0.004613298922777176 \n",
            "At 208 epoch, Training Loss: 0.003587029781192541 \n",
            "At 208 epoch, Validation Loss: 0.004826257470995188 \n",
            "At 209 epoch, Training Loss: 0.0033662141300737857 \n",
            "At 209 epoch, Validation Loss: 0.0046386756002902985 \n",
            "At 210 epoch, Training Loss: 0.003240439295768738 \n",
            "At 210 epoch, Validation Loss: 0.005206658039242029 \n",
            "At 211 epoch, Training Loss: 0.003137363353744149 \n",
            "At 211 epoch, Validation Loss: 0.004536857362836599 \n",
            "At 212 epoch, Training Loss: 0.0030906878877431155 \n",
            "At 212 epoch, Validation Loss: 0.00472559779882431 \n",
            "At 213 epoch, Training Loss: 0.0029344017151743174 \n",
            "At 213 epoch, Validation Loss: 0.004585119895637035 \n",
            "At 214 epoch, Training Loss: 0.0033922483213245868 \n",
            "At 214 epoch, Validation Loss: 0.004589691758155823 \n",
            "At 215 epoch, Training Loss: 0.002549980906769633 \n",
            "At 215 epoch, Validation Loss: 0.0044555156491696835 \n",
            "At 216 epoch, Training Loss: 0.0024095979519188405 \n",
            "At 216 epoch, Validation Loss: 0.004248621873557568 \n",
            "At 217 epoch, Training Loss: 0.0029045681469142435 \n",
            "At 217 epoch, Validation Loss: 0.0041684857569634914 \n",
            "At 218 epoch, Training Loss: 0.002492554672062397 \n",
            "At 218 epoch, Validation Loss: 0.004255433566868305 \n",
            "At 219 epoch, Training Loss: 0.0021911385469138623 \n",
            "At 219 epoch, Validation Loss: 0.004302080720663071 \n",
            "At 220 epoch, Training Loss: 0.0024363934760913255 \n",
            "At 220 epoch, Validation Loss: 0.004330754745751619 \n",
            "At 221 epoch, Training Loss: 0.0024461439112201332 \n",
            "At 221 epoch, Validation Loss: 0.004503009840846062 \n",
            "At 222 epoch, Training Loss: 0.002464601071551442 \n",
            "At 222 epoch, Validation Loss: 0.004435958806425333 \n",
            "At 223 epoch, Training Loss: 0.002293635089881718 \n",
            "At 223 epoch, Validation Loss: 0.004455721937119961 \n",
            "At 224 epoch, Training Loss: 0.002151197660714388 \n",
            "At 224 epoch, Validation Loss: 0.004427321255207062 \n",
            "At 225 epoch, Training Loss: 0.0020894531859084963 \n",
            "At 225 epoch, Validation Loss: 0.004440272226929665 \n",
            "At 226 epoch, Training Loss: 0.0022438526852056385 \n",
            "At 226 epoch, Validation Loss: 0.004554452374577522 \n",
            "At 227 epoch, Training Loss: 0.0021554820938035845 \n",
            "At 227 epoch, Validation Loss: 0.004956850782036781 \n",
            "At 228 epoch, Training Loss: 0.0022691971622407436 \n",
            "At 228 epoch, Validation Loss: 0.004746087361127138 \n",
            "At 229 epoch, Training Loss: 0.002060355362482369 \n",
            "At 229 epoch, Validation Loss: 0.004595049656927586 \n",
            "At 230 epoch, Training Loss: 0.0021329497918486594 \n",
            "At 230 epoch, Validation Loss: 0.004776169545948505 \n",
            "At 231 epoch, Training Loss: 0.0019453733460977674 \n",
            "At 231 epoch, Validation Loss: 0.004738102201372385 \n",
            "At 232 epoch, Training Loss: 0.0014833463355898857 \n",
            "At 232 epoch, Validation Loss: 0.004369630943983793 \n",
            "At 233 epoch, Training Loss: 0.001736228633671999 \n",
            "At 233 epoch, Validation Loss: 0.004139792174100876 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 234 epoch, Training Loss: 0.0018067415105178953 \n",
            "At 234 epoch, Validation Loss: 0.004375134594738483 \n",
            "At 235 epoch, Training Loss: 0.0017095591640099884 \n",
            "At 235 epoch, Validation Loss: 0.004421083722263575 \n",
            "At 236 epoch, Training Loss: 0.0017928070854395628 \n",
            "At 236 epoch, Validation Loss: 0.004226419143378735 \n",
            "At 237 epoch, Training Loss: 0.001743590529076755 \n",
            "At 237 epoch, Validation Loss: 0.004467050079256296 \n",
            "At 238 epoch, Training Loss: 0.001694589969702065 \n",
            "At 238 epoch, Validation Loss: 0.0055156550370156765 \n",
            "At 239 epoch, Training Loss: 0.00287430239841342 \n",
            "At 239 epoch, Validation Loss: 0.004592819605022669 \n",
            "At 240 epoch, Training Loss: 0.0019843683810904624 \n",
            "At 240 epoch, Validation Loss: 0.005268216133117676 \n",
            "At 241 epoch, Training Loss: 0.0018909645034000278 \n",
            "At 241 epoch, Validation Loss: 0.00461780047044158 \n",
            "At 242 epoch, Training Loss: 0.0019431583117693664 \n",
            "At 242 epoch, Validation Loss: 0.004466382786631584 \n",
            "At 243 epoch, Training Loss: 0.001677325158379972 \n",
            "At 243 epoch, Validation Loss: 0.004988934379070997 \n",
            "At 244 epoch, Training Loss: 0.001736231497488916 \n",
            "At 244 epoch, Validation Loss: 0.004417669028043747 \n",
            "At 245 epoch, Training Loss: 0.0015910422429442407 \n",
            "At 245 epoch, Validation Loss: 0.004466494079679251 \n",
            "At 246 epoch, Training Loss: 0.001663178182207048 \n",
            "At 246 epoch, Validation Loss: 0.004859117791056633 \n",
            "At 247 epoch, Training Loss: 0.0016624357551336289 \n",
            "At 247 epoch, Validation Loss: 0.004409098066389561 \n",
            "At 248 epoch, Training Loss: 0.0015143241849727928 \n",
            "At 248 epoch, Validation Loss: 0.004807508084923029 \n",
            "At 249 epoch, Training Loss: 0.0015497871674597264 \n",
            "At 249 epoch, Validation Loss: 0.004706087522208691 \n",
            "At 250 epoch, Training Loss: 0.0014316435204818844 \n",
            "At 250 epoch, Validation Loss: 0.004527041222900152 \n",
            "At 251 epoch, Training Loss: 0.0016755773453041912 \n",
            "At 251 epoch, Validation Loss: 0.004535971209406853 \n",
            "At 252 epoch, Training Loss: 0.0015358689706772566 \n",
            "At 252 epoch, Validation Loss: 0.004419627599418163 \n",
            "At 253 epoch, Training Loss: 0.0014131316915154458 \n",
            "At 253 epoch, Validation Loss: 0.0043281326070427895 \n",
            "At 254 epoch, Training Loss: 0.0021712643094360827 \n",
            "At 254 epoch, Validation Loss: 0.004704555030912161 \n",
            "At 255 epoch, Training Loss: 0.001914902520366013 \n",
            "At 255 epoch, Validation Loss: 0.006345039699226618 \n",
            "At 256 epoch, Training Loss: 0.0013526676222682 \n",
            "At 256 epoch, Validation Loss: 0.004430402535945177 \n",
            "At 257 epoch, Training Loss: 0.0015499552013352514 \n",
            "At 257 epoch, Validation Loss: 0.005147838965058327 \n",
            "At 258 epoch, Training Loss: 0.0016459305305033922 \n",
            "At 258 epoch, Validation Loss: 0.004542436450719833 \n",
            "At 259 epoch, Training Loss: 0.0015640073921531438 \n",
            "At 259 epoch, Validation Loss: 0.004944942891597748 \n",
            "At 260 epoch, Training Loss: 0.001432026270776987 \n",
            "At 260 epoch, Validation Loss: 0.005399033427238464 \n",
            "At 261 epoch, Training Loss: 0.0013675594702363015 \n",
            "At 261 epoch, Validation Loss: 0.004455715883523226 \n",
            "At 262 epoch, Training Loss: 0.0012503511505201458 \n",
            "At 262 epoch, Validation Loss: 0.004720694851130247 \n",
            "At 263 epoch, Training Loss: 0.0015040310565382243 \n",
            "At 263 epoch, Validation Loss: 0.004700394347310066 \n",
            "At 264 epoch, Training Loss: 0.0017727915663272142 \n",
            "At 264 epoch, Validation Loss: 0.008844573050737381 \n",
            "At 265 epoch, Training Loss: 0.007498826738446951 \n",
            "At 265 epoch, Validation Loss: 0.024119146168231964 \n",
            "At 266 epoch, Training Loss: 0.048908599000424144 \n",
            "At 266 epoch, Validation Loss: 0.3211875259876251 \n",
            "At 267 epoch, Training Loss: 0.042440778389573094 \n",
            "At 267 epoch, Validation Loss: 0.6147118806838989 \n",
            "At 268 epoch, Training Loss: 0.019142231345176695 \n",
            "At 268 epoch, Validation Loss: 0.46221598982810974 \n",
            "At 269 epoch, Training Loss: 0.0146086897701025 \n",
            "At 269 epoch, Validation Loss: 0.13641385734081268 \n",
            "At 270 epoch, Training Loss: 0.010862951166927814 \n",
            "At 270 epoch, Validation Loss: 0.07248246669769287 \n",
            "At 271 epoch, Training Loss: 0.00886070979759097 \n",
            "At 271 epoch, Validation Loss: 0.01770154759287834 \n",
            "At 272 epoch, Training Loss: 0.007321544736623764 \n",
            "At 272 epoch, Validation Loss: 0.009125430136919022 \n",
            "At 273 epoch, Training Loss: 0.005326858349144458 \n",
            "At 273 epoch, Validation Loss: 0.006151484791189432 \n",
            "At 274 epoch, Training Loss: 0.004706760868430138 \n",
            "At 274 epoch, Validation Loss: 0.0047828140668570995 \n",
            "At 275 epoch, Training Loss: 0.003944995766505599 \n",
            "At 275 epoch, Validation Loss: 0.004717797040939331 \n",
            "At 276 epoch, Training Loss: 0.003636091621592641 \n",
            "At 276 epoch, Validation Loss: 0.0041826944798231125 \n",
            "At 277 epoch, Training Loss: 0.003187566064298153 \n",
            "At 277 epoch, Validation Loss: 0.004198047332465649 \n",
            "At 278 epoch, Training Loss: 0.0029191304929554462 \n",
            "At 278 epoch, Validation Loss: 0.0041557978838682175 \n",
            "At 279 epoch, Training Loss: 0.0031419100239872934 \n",
            "At 279 epoch, Validation Loss: 0.004300535190850496 \n",
            "At 280 epoch, Training Loss: 0.0038967808708548547 \n",
            "At 280 epoch, Validation Loss: 0.004126494284719229 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 281 epoch, Training Loss: 0.0027462677098810673 \n",
            "At 281 epoch, Validation Loss: 0.005883907433599234 \n",
            "At 282 epoch, Training Loss: 0.004091486195102334 \n",
            "At 282 epoch, Validation Loss: 0.006495739333331585 \n",
            "At 283 epoch, Training Loss: 0.004724273737519979 \n",
            "At 283 epoch, Validation Loss: 0.010112147778272629 \n",
            "At 284 epoch, Training Loss: 0.003717732150107622 \n",
            "At 284 epoch, Validation Loss: 0.009862138889729977 \n",
            "At 285 epoch, Training Loss: 0.004059000546112657 \n",
            "At 285 epoch, Validation Loss: 0.008917809464037418 \n",
            "At 286 epoch, Training Loss: 0.003756623063236475 \n",
            "At 286 epoch, Validation Loss: 0.004623237531632185 \n",
            "At 287 epoch, Training Loss: 0.002899280609562993 \n",
            "At 287 epoch, Validation Loss: 0.004179593175649643 \n",
            "At 288 epoch, Training Loss: 0.002598466398194432 \n",
            "At 288 epoch, Validation Loss: 0.0040678889490664005 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 289 epoch, Training Loss: 0.003011662466451526 \n",
            "At 289 epoch, Validation Loss: 0.0038841937202960253 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 290 epoch, Training Loss: 0.0029172103386372328 \n",
            "At 290 epoch, Validation Loss: 0.003904165467247367 \n",
            "At 291 epoch, Training Loss: 0.002146346913650632 \n",
            "At 291 epoch, Validation Loss: 0.003962096758186817 \n",
            "At 292 epoch, Training Loss: 0.0020371075021103024 \n",
            "At 292 epoch, Validation Loss: 0.00442467862740159 \n",
            "At 293 epoch, Training Loss: 0.0022867021383717657 \n",
            "At 293 epoch, Validation Loss: 0.004100816790014505 \n",
            "At 294 epoch, Training Loss: 0.0019143052864819764 \n",
            "At 294 epoch, Validation Loss: 0.0039697568863630295 \n",
            "At 295 epoch, Training Loss: 0.0017364248633384705 \n",
            "At 295 epoch, Validation Loss: 0.003914978820830584 \n",
            "At 296 epoch, Training Loss: 0.0019243378425016998 \n",
            "At 296 epoch, Validation Loss: 0.0037872225511819124 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 297 epoch, Training Loss: 0.0016705055255442857 \n",
            "At 297 epoch, Validation Loss: 0.00381283275783062 \n",
            "At 298 epoch, Training Loss: 0.0015634651528671384 \n",
            "At 298 epoch, Validation Loss: 0.0039449105970561504 \n",
            "At 299 epoch, Training Loss: 0.0020091367652639745 \n",
            "At 299 epoch, Validation Loss: 0.003946375567466021 \n",
            "At 300 epoch, Training Loss: 0.0019785040291026234 \n",
            "At 300 epoch, Validation Loss: 0.004364748485386372 \n",
            "At 301 epoch, Training Loss: 0.002020906750112772 \n",
            "At 301 epoch, Validation Loss: 0.003910338506102562 \n",
            "At 302 epoch, Training Loss: 0.0016902113799005746 \n",
            "At 302 epoch, Validation Loss: 0.004067331552505493 \n",
            "At 303 epoch, Training Loss: 0.001446696394123137 \n",
            "At 303 epoch, Validation Loss: 0.004084501415491104 \n",
            "At 304 epoch, Training Loss: 0.0016796187963336706 \n",
            "At 304 epoch, Validation Loss: 0.0049370029009878635 \n",
            "At 305 epoch, Training Loss: 0.0015548505121842026 \n",
            "At 305 epoch, Validation Loss: 0.005122477654367685 \n",
            "At 306 epoch, Training Loss: 0.001487249427009374 \n",
            "At 306 epoch, Validation Loss: 0.004145161714404821 \n",
            "At 307 epoch, Training Loss: 0.0013904180144891144 \n",
            "At 307 epoch, Validation Loss: 0.003960799425840378 \n",
            "At 308 epoch, Training Loss: 0.001474990847054869 \n",
            "At 308 epoch, Validation Loss: 0.0040636854246258736 \n",
            "At 309 epoch, Training Loss: 0.001769453310407698 \n",
            "At 309 epoch, Validation Loss: 0.005454707890748978 \n",
            "At 310 epoch, Training Loss: 0.0020200074650347233 \n",
            "At 310 epoch, Validation Loss: 0.0043671345338225365 \n",
            "At 311 epoch, Training Loss: 0.002316883415915072 \n",
            "At 311 epoch, Validation Loss: 0.0061922562308609486 \n",
            "At 312 epoch, Training Loss: 0.0025148554937914013 \n",
            "At 312 epoch, Validation Loss: 0.013221713714301586 \n",
            "At 313 epoch, Training Loss: 0.002364537608809769 \n",
            "At 313 epoch, Validation Loss: 0.0042469920590519905 \n",
            "At 314 epoch, Training Loss: 0.0023722109384834767 \n",
            "At 314 epoch, Validation Loss: 0.0038695824332535267 \n",
            "At 315 epoch, Training Loss: 0.0017816543113440275 \n",
            "At 315 epoch, Validation Loss: 0.0043763648718595505 \n",
            "At 316 epoch, Training Loss: 0.002252001641318202 \n",
            "At 316 epoch, Validation Loss: 0.003894813358783722 \n",
            "At 317 epoch, Training Loss: 0.0014580330927856267 \n",
            "At 317 epoch, Validation Loss: 0.0042777773924171925 \n",
            "At 318 epoch, Training Loss: 0.0017391295172274114 \n",
            "At 318 epoch, Validation Loss: 0.0041671437211334705 \n",
            "At 319 epoch, Training Loss: 0.002137762261554599 \n",
            "At 319 epoch, Validation Loss: 0.003689486999064684 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 320 epoch, Training Loss: 0.00275120516307652 \n",
            "At 320 epoch, Validation Loss: 3.4992165565490723 \n",
            "At 321 epoch, Training Loss: 0.006005110219120979 \n",
            "At 321 epoch, Validation Loss: 2.082069158554077 \n",
            "At 322 epoch, Training Loss: 0.006974098924547434 \n",
            "At 322 epoch, Validation Loss: 0.36773526668548584 \n",
            "At 323 epoch, Training Loss: 0.005677456129342317 \n",
            "At 323 epoch, Validation Loss: 0.055005256086587906 \n",
            "At 324 epoch, Training Loss: 0.0039865356404334305 \n",
            "At 324 epoch, Validation Loss: 0.014963963069021702 \n",
            "At 325 epoch, Training Loss: 0.0030546926893293856 \n",
            "At 325 epoch, Validation Loss: 0.005179429426789284 \n",
            "At 326 epoch, Training Loss: 0.0027974941534921528 \n",
            "At 326 epoch, Validation Loss: 0.0045307716354727745 \n",
            "At 327 epoch, Training Loss: 0.00250429876614362 \n",
            "At 327 epoch, Validation Loss: 0.012148539535701275 \n",
            "At 328 epoch, Training Loss: 0.0028559121303260325 \n",
            "At 328 epoch, Validation Loss: 0.006419439800083637 \n",
            "At 329 epoch, Training Loss: 0.002162687247619033 \n",
            "At 329 epoch, Validation Loss: 0.009621345438063145 \n",
            "At 330 epoch, Training Loss: 0.002205471904017031 \n",
            "At 330 epoch, Validation Loss: 0.008690248243510723 \n",
            "At 331 epoch, Training Loss: 0.0021200235933065414 \n",
            "At 331 epoch, Validation Loss: 0.00863434188067913 \n",
            "At 332 epoch, Training Loss: 0.001941443863324821 \n",
            "At 332 epoch, Validation Loss: 0.004369232803583145 \n",
            "At 333 epoch, Training Loss: 0.001732166367582977 \n",
            "At 333 epoch, Validation Loss: 0.006311596836894751 \n",
            "At 334 epoch, Training Loss: 0.0019741080002859237 \n",
            "At 334 epoch, Validation Loss: 0.004439978860318661 \n",
            "At 335 epoch, Training Loss: 0.0017869302071630954 \n",
            "At 335 epoch, Validation Loss: 0.003848231630399823 \n",
            "At 336 epoch, Training Loss: 0.0018465812550857662 \n",
            "At 336 epoch, Validation Loss: 0.003967573866248131 \n",
            "At 337 epoch, Training Loss: 0.001527849759440869 \n",
            "At 337 epoch, Validation Loss: 0.004531608894467354 \n",
            "At 338 epoch, Training Loss: 0.001586870290338993 \n",
            "At 338 epoch, Validation Loss: 0.006056903395801783 \n",
            "At 339 epoch, Training Loss: 0.0017032607924193145 \n",
            "At 339 epoch, Validation Loss: 0.004921997431665659 \n",
            "At 340 epoch, Training Loss: 0.0030871837399899958 \n",
            "At 340 epoch, Validation Loss: 0.004503739066421986 \n",
            "At 341 epoch, Training Loss: 0.0023802278330549596 \n",
            "At 341 epoch, Validation Loss: 0.004002861212939024 \n",
            "At 342 epoch, Training Loss: 0.001983957807533443 \n",
            "At 342 epoch, Validation Loss: 0.004874835256487131 \n",
            "At 343 epoch, Training Loss: 0.0018692882964387535 \n",
            "At 343 epoch, Validation Loss: 0.003946715500205755 \n",
            "At 344 epoch, Training Loss: 0.001698353502433747 \n",
            "At 344 epoch, Validation Loss: 0.004488770384341478 \n",
            "At 345 epoch, Training Loss: 0.0015144837787374854 \n",
            "At 345 epoch, Validation Loss: 0.0038402194622904062 \n",
            "At 346 epoch, Training Loss: 0.001552979159168899 \n",
            "At 346 epoch, Validation Loss: 0.003772118827328086 \n",
            "At 347 epoch, Training Loss: 0.0017769642639905215 \n",
            "At 347 epoch, Validation Loss: 0.005015901755541563 \n",
            "At 348 epoch, Training Loss: 0.004362083552405238 \n",
            "At 348 epoch, Validation Loss: 0.004171452019363642 \n",
            "At 349 epoch, Training Loss: 0.10518642067909241 \n",
            "At 349 epoch, Validation Loss: 2.59891676902771 \n",
            "At 350 epoch, Training Loss: 0.06126952320337296 \n",
            "At 350 epoch, Validation Loss: 1.4950200319290161 \n",
            "At 351 epoch, Training Loss: 0.03547995388507843 \n",
            "At 351 epoch, Validation Loss: 0.6592363119125366 \n",
            "At 352 epoch, Training Loss: 0.02585308477282524 \n",
            "At 352 epoch, Validation Loss: 0.18473054468631744 \n",
            "At 353 epoch, Training Loss: 0.016369330510497094 \n",
            "At 353 epoch, Validation Loss: 0.026863541454076767 \n",
            "At 354 epoch, Training Loss: 0.011462579108774662 \n",
            "At 354 epoch, Validation Loss: 0.012416389770805836 \n",
            "At 355 epoch, Training Loss: 0.007972180936485529 \n",
            "At 355 epoch, Validation Loss: 0.005607031751424074 \n",
            "At 356 epoch, Training Loss: 0.006949366629123687 \n",
            "At 356 epoch, Validation Loss: 0.0044529614970088005 \n",
            "At 357 epoch, Training Loss: 0.005549548380076885 \n",
            "At 357 epoch, Validation Loss: 0.004155637230724096 \n",
            "At 358 epoch, Training Loss: 0.004764997866004705 \n",
            "At 358 epoch, Validation Loss: 0.0038406436797231436 \n",
            "At 359 epoch, Training Loss: 0.004345202585682273 \n",
            "At 359 epoch, Validation Loss: 0.003752638353034854 \n",
            "At 360 epoch, Training Loss: 0.0034307510126382113 \n",
            "At 360 epoch, Validation Loss: 0.003889930434525013 \n",
            "At 361 epoch, Training Loss: 0.0033005591481924056 \n",
            "At 361 epoch, Validation Loss: 0.004052171017974615 \n",
            "At 362 epoch, Training Loss: 0.003011887241154909 \n",
            "At 362 epoch, Validation Loss: 0.004074355587363243 \n",
            "At 363 epoch, Training Loss: 0.002683613961562514 \n",
            "At 363 epoch, Validation Loss: 0.004175823647528887 \n",
            "At 364 epoch, Training Loss: 0.0028721615206450226 \n",
            "At 364 epoch, Validation Loss: 0.004228182602673769 \n",
            "At 365 epoch, Training Loss: 0.002614464983344078 \n",
            "At 365 epoch, Validation Loss: 0.0041921092197299 \n",
            "At 366 epoch, Training Loss: 0.0026920282281935213 \n",
            "At 366 epoch, Validation Loss: 0.00417929794639349 \n",
            "At 367 epoch, Training Loss: 0.0021136040333658455 \n",
            "At 367 epoch, Validation Loss: 0.004050346557050943 \n",
            "At 368 epoch, Training Loss: 0.0020491549046710135 \n",
            "At 368 epoch, Validation Loss: 0.004070539493113756 \n",
            "At 369 epoch, Training Loss: 0.002282921620644629 \n",
            "At 369 epoch, Validation Loss: 0.0040815286338329315 \n",
            "At 370 epoch, Training Loss: 0.002023996948264539 \n",
            "At 370 epoch, Validation Loss: 0.003926765639334917 \n",
            "At 371 epoch, Training Loss: 0.0018349505262449384 \n",
            "At 371 epoch, Validation Loss: 0.004194627981632948 \n",
            "At 372 epoch, Training Loss: 0.0018020726274698973 \n",
            "At 372 epoch, Validation Loss: 0.004175782669335604 \n",
            "At 373 epoch, Training Loss: 0.0018066519871354104 \n",
            "At 373 epoch, Validation Loss: 0.004138740710914135 \n",
            "At 374 epoch, Training Loss: 0.001686041010543704 \n",
            "At 374 epoch, Validation Loss: 0.004288565833121538 \n",
            "At 375 epoch, Training Loss: 0.001812577247619629 \n",
            "At 375 epoch, Validation Loss: 0.004120666068047285 \n",
            "At 376 epoch, Training Loss: 0.001982966368086636 \n",
            "At 376 epoch, Validation Loss: 0.004155553877353668 \n",
            "At 377 epoch, Training Loss: 0.0019067152170464396 \n",
            "At 377 epoch, Validation Loss: 0.004282128065824509 \n",
            "At 378 epoch, Training Loss: 0.00169458519667387 \n",
            "At 378 epoch, Validation Loss: 0.004090955480933189 \n",
            "At 379 epoch, Training Loss: 0.0016401092056185008 \n",
            "At 379 epoch, Validation Loss: 0.004136364907026291 \n",
            "At 380 epoch, Training Loss: 0.0014935442595742643 \n",
            "At 380 epoch, Validation Loss: 0.004107562825083733 \n",
            "At 381 epoch, Training Loss: 0.0017718861810863018 \n",
            "At 381 epoch, Validation Loss: 0.004430035129189491 \n",
            "At 382 epoch, Training Loss: 0.0014334076666273176 \n",
            "At 382 epoch, Validation Loss: 0.003958343528211117 \n",
            "At 383 epoch, Training Loss: 0.0014955115038901567 \n",
            "At 383 epoch, Validation Loss: 0.004277537576854229 \n",
            "At 384 epoch, Training Loss: 0.0013312746305018663 \n",
            "At 384 epoch, Validation Loss: 0.004014888778328896 \n",
            "At 385 epoch, Training Loss: 0.001138607261236757 \n",
            "At 385 epoch, Validation Loss: 0.0039721904322505 \n",
            "At 386 epoch, Training Loss: 0.001540892431512475 \n",
            "At 386 epoch, Validation Loss: 0.004385096020996571 \n",
            "At 387 epoch, Training Loss: 0.0017953478731215 \n",
            "At 387 epoch, Validation Loss: 0.004059132654219866 \n",
            "At 388 epoch, Training Loss: 0.0014498522970825434 \n",
            "At 388 epoch, Validation Loss: 0.004031538963317871 \n",
            "At 389 epoch, Training Loss: 0.0013835880206897854 \n",
            "At 389 epoch, Validation Loss: 0.004146958235651255 \n",
            "At 390 epoch, Training Loss: 0.0014307744102552533 \n",
            "At 390 epoch, Validation Loss: 0.004092294257134199 \n",
            "At 391 epoch, Training Loss: 0.0015410084510222078 \n",
            "At 391 epoch, Validation Loss: 0.00402985280379653 \n",
            "At 392 epoch, Training Loss: 0.0014149145572446287 \n",
            "At 392 epoch, Validation Loss: 0.004293430130928755 \n",
            "At 393 epoch, Training Loss: 0.0012823114404454828 \n",
            "At 393 epoch, Validation Loss: 0.004107417073100805 \n",
            "At 394 epoch, Training Loss: 0.001465936121530831 \n",
            "At 394 epoch, Validation Loss: 0.004048890434205532 \n",
            "At 395 epoch, Training Loss: 0.0013310534646734594 \n",
            "At 395 epoch, Validation Loss: 0.0041143689304590225 \n",
            "At 396 epoch, Training Loss: 0.0011890501249581576 \n",
            "At 396 epoch, Validation Loss: 0.0041013858281075954 \n",
            "At 397 epoch, Training Loss: 0.0012813122011721134 \n",
            "At 397 epoch, Validation Loss: 0.004034896846860647 \n",
            "At 398 epoch, Training Loss: 0.0014101149514317512 \n",
            "At 398 epoch, Validation Loss: 0.00409615458920598 \n",
            "At 399 epoch, Training Loss: 0.0013324455823749304 \n",
            "At 399 epoch, Validation Loss: 0.00407316954806447 \n",
            "At 400 epoch, Training Loss: 0.0015643917489796878 \n",
            "At 400 epoch, Validation Loss: 0.004135920200496912 \n",
            "At 401 epoch, Training Loss: 0.001722185662947595 \n",
            "At 401 epoch, Validation Loss: 0.004112847615033388 \n",
            "At 402 epoch, Training Loss: 0.0012080877786502242 \n",
            "At 402 epoch, Validation Loss: 0.003948254510760307 \n",
            "At 403 epoch, Training Loss: 0.0011817988357506692 \n",
            "At 403 epoch, Validation Loss: 0.003959701396524906 \n",
            "At 404 epoch, Training Loss: 0.0012057550717145204 \n",
            "At 404 epoch, Validation Loss: 0.004336557351052761 \n",
            "At 405 epoch, Training Loss: 0.0013013353338465095 \n",
            "At 405 epoch, Validation Loss: 0.003995118197053671 \n",
            "At 406 epoch, Training Loss: 0.0013039890211075545 \n",
            "At 406 epoch, Validation Loss: 0.004345682915300131 \n",
            "At 407 epoch, Training Loss: 0.001161189714912325 \n",
            "At 407 epoch, Validation Loss: 0.004135458264499903 \n",
            "At 408 epoch, Training Loss: 0.0012574432650581003 \n",
            "At 408 epoch, Validation Loss: 0.003976127598434687 \n",
            "At 409 epoch, Training Loss: 0.001274893106892705 \n",
            "At 409 epoch, Validation Loss: 0.004145588260143995 \n",
            "At 410 epoch, Training Loss: 0.001263679820112884 \n",
            "At 410 epoch, Validation Loss: 0.004132939036935568 \n",
            "At 411 epoch, Training Loss: 0.0011901242891326548 \n",
            "At 411 epoch, Validation Loss: 0.004127121064811945 \n",
            "At 412 epoch, Training Loss: 0.0014047814067453146 \n",
            "At 412 epoch, Validation Loss: 0.004368641413748264 \n",
            "At 413 epoch, Training Loss: 0.0014003471471369267 \n",
            "At 413 epoch, Validation Loss: 0.004004527349025011 \n",
            "At 414 epoch, Training Loss: 0.0011614833492785692 \n",
            "At 414 epoch, Validation Loss: 0.004266700707376003 \n",
            "At 415 epoch, Training Loss: 0.0012228411040268838 \n",
            "At 415 epoch, Validation Loss: 0.003884808626025915 \n",
            "At 416 epoch, Training Loss: 0.001286407164297998 \n",
            "At 416 epoch, Validation Loss: 0.0043355850502848625 \n",
            "At 417 epoch, Training Loss: 0.0013110134284943342 \n",
            "At 417 epoch, Validation Loss: 0.00403659138828516 \n",
            "At 418 epoch, Training Loss: 0.0010923842433840036 \n",
            "At 418 epoch, Validation Loss: 0.003926228731870651 \n",
            "At 419 epoch, Training Loss: 0.0011129219550639392 \n",
            "At 419 epoch, Validation Loss: 0.004415713716298342 \n",
            "At 420 epoch, Training Loss: 0.0016105156159028412 \n",
            "At 420 epoch, Validation Loss: 0.003822402562946081 \n",
            "At 421 epoch, Training Loss: 0.0009138034773059189 \n",
            "At 421 epoch, Validation Loss: 0.003931378945708275 \n",
            "At 422 epoch, Training Loss: 0.0011680317111313343 \n",
            "At 422 epoch, Validation Loss: 0.004025783855468035 \n",
            "At 423 epoch, Training Loss: 0.0010082048131152988 \n",
            "At 423 epoch, Validation Loss: 0.004105065017938614 \n",
            "At 424 epoch, Training Loss: 0.0012499224860221148 \n",
            "At 424 epoch, Validation Loss: 0.003941476810723543 \n",
            "At 425 epoch, Training Loss: 0.0012260016170330345 \n",
            "At 425 epoch, Validation Loss: 0.004068797454237938 \n",
            "At 426 epoch, Training Loss: 0.0011553760035894812 \n",
            "At 426 epoch, Validation Loss: 0.004102212376892567 \n",
            "At 427 epoch, Training Loss: 0.0009931830107234418 \n",
            "At 427 epoch, Validation Loss: 0.00424376642331481 \n",
            "At 428 epoch, Training Loss: 0.0011175310588441788 \n",
            "At 428 epoch, Validation Loss: 0.003912358544766903 \n",
            "At 429 epoch, Training Loss: 0.0009521783795207739 \n",
            "At 429 epoch, Validation Loss: 0.004299655091017485 \n",
            "At 430 epoch, Training Loss: 0.000976075604557991 \n",
            "At 430 epoch, Validation Loss: 0.00417036609724164 \n",
            "At 431 epoch, Training Loss: 0.00109437326900661 \n",
            "At 431 epoch, Validation Loss: 0.003876064671203494 \n",
            "At 432 epoch, Training Loss: 0.0010616403073072434 \n",
            "At 432 epoch, Validation Loss: 0.004016281571239233 \n",
            "At 433 epoch, Training Loss: 0.0010955028352327644 \n",
            "At 433 epoch, Validation Loss: 0.004319739528000355 \n",
            "At 434 epoch, Training Loss: 0.0011288771755062043 \n",
            "At 434 epoch, Validation Loss: 0.0038235452957451344 \n",
            "At 435 epoch, Training Loss: 0.0010340148466639221 \n",
            "At 435 epoch, Validation Loss: 0.0037366028409451246 \n",
            "At 436 epoch, Training Loss: 0.0007734463317319751 \n",
            "At 436 epoch, Validation Loss: 0.00418167095631361 \n",
            "At 437 epoch, Training Loss: 0.001094518054742366 \n",
            "At 437 epoch, Validation Loss: 0.0040740277618169785 \n",
            "At 438 epoch, Training Loss: 0.0009864457650110125 \n",
            "At 438 epoch, Validation Loss: 0.0037742184940725565 \n",
            "At 439 epoch, Training Loss: 0.0011341428151354193 \n",
            "At 439 epoch, Validation Loss: 0.00393989123404026 \n",
            "At 440 epoch, Training Loss: 0.0008002252550795674 \n",
            "At 440 epoch, Validation Loss: 0.00400575390085578 \n",
            "At 441 epoch, Training Loss: 0.0009253283147700131 \n",
            "At 441 epoch, Validation Loss: 0.00399371050298214 \n",
            "At 442 epoch, Training Loss: 0.0011047902633436024 \n",
            "At 442 epoch, Validation Loss: 0.003956645727157593 \n",
            "At 443 epoch, Training Loss: 0.0009407790377736092 \n",
            "At 443 epoch, Validation Loss: 0.0039528547786176205 \n",
            "At 444 epoch, Training Loss: 0.0008837079978547991 \n",
            "At 444 epoch, Validation Loss: 0.004123304970562458 \n",
            "At 445 epoch, Training Loss: 0.0009255461394786834 \n",
            "At 445 epoch, Validation Loss: 0.0038691330701112747 \n",
            "At 446 epoch, Training Loss: 0.0009340292657725513 \n",
            "At 446 epoch, Validation Loss: 0.0038729626685380936 \n",
            "At 447 epoch, Training Loss: 0.0008548512123525143 \n",
            "At 447 epoch, Validation Loss: 0.004030050244182348 \n",
            "At 448 epoch, Training Loss: 0.0007944682263769209 \n",
            "At 448 epoch, Validation Loss: 0.0039820680394768715 \n",
            "At 449 epoch, Training Loss: 0.0008483491721563042 \n",
            "At 449 epoch, Validation Loss: 0.004008562304079533 \n",
            "At 450 epoch, Training Loss: 0.0012109908508136868 \n",
            "At 450 epoch, Validation Loss: 0.00560125382617116 \n",
            "At 451 epoch, Training Loss: 0.002657668339088559 \n",
            "At 451 epoch, Validation Loss: 0.004751777742058039 \n",
            "At 452 epoch, Training Loss: 0.002454557316377759 \n",
            "At 452 epoch, Validation Loss: 0.005772446747869253 \n",
            "At 453 epoch, Training Loss: 0.0022255389718338846 \n",
            "At 453 epoch, Validation Loss: 0.007473647594451904 \n",
            "At 454 epoch, Training Loss: 0.001897206250578165 \n",
            "At 454 epoch, Validation Loss: 0.005418428685516119 \n",
            "At 455 epoch, Training Loss: 0.0017773631261661649 \n",
            "At 455 epoch, Validation Loss: 0.004449463449418545 \n",
            "At 456 epoch, Training Loss: 0.001504091056995094 \n",
            "At 456 epoch, Validation Loss: 0.00415050471201539 \n",
            "At 457 epoch, Training Loss: 0.0011808732757344841 \n",
            "At 457 epoch, Validation Loss: 0.0051508028991520405 \n",
            "At 458 epoch, Training Loss: 0.001461628801189363 \n",
            "At 458 epoch, Validation Loss: 0.003985244315117598 \n",
            "At 459 epoch, Training Loss: 0.0018910370767116547 \n",
            "At 459 epoch, Validation Loss: 0.004338583443313837 \n",
            "At 460 epoch, Training Loss: 0.001467410707846284 \n",
            "At 460 epoch, Validation Loss: 0.005101032089442015 \n",
            "At 461 epoch, Training Loss: 0.0014587597805075347 \n",
            "At 461 epoch, Validation Loss: 0.003899445291608572 \n",
            "At 462 epoch, Training Loss: 0.0013974295929074288 \n",
            "At 462 epoch, Validation Loss: 0.004275226499885321 \n",
            "At 463 epoch, Training Loss: 0.0011383589589968324 \n",
            "At 463 epoch, Validation Loss: 0.003734980709850788 \n",
            "At 464 epoch, Training Loss: 0.00105922797229141 \n",
            "At 464 epoch, Validation Loss: 0.004009678028523922 \n",
            "At 465 epoch, Training Loss: 0.0008599780267104506 \n",
            "At 465 epoch, Validation Loss: 0.0040544732473790646 \n",
            "At 466 epoch, Training Loss: 0.0009631018503569067 \n",
            "At 466 epoch, Validation Loss: 0.0037851191591471434 \n",
            "At 467 epoch, Training Loss: 0.000792178261326626 \n",
            "At 467 epoch, Validation Loss: 0.003807192435488105 \n",
            "At 468 epoch, Training Loss: 0.0008975499426014721 \n",
            "At 468 epoch, Validation Loss: 0.004025992937386036 \n",
            "At 469 epoch, Training Loss: 0.0008194448077119887 \n",
            "At 469 epoch, Validation Loss: 0.004071794915944338 \n",
            "At 470 epoch, Training Loss: 0.000978871074039489 \n",
            "At 470 epoch, Validation Loss: 0.003963547758758068 \n",
            "At 471 epoch, Training Loss: 0.0007097612367942929 \n",
            "At 471 epoch, Validation Loss: 0.003909890539944172 \n",
            "At 472 epoch, Training Loss: 0.0008494862355291844 \n",
            "At 472 epoch, Validation Loss: 0.003737253602594137 \n",
            "At 473 epoch, Training Loss: 0.000996860209852457 \n",
            "At 473 epoch, Validation Loss: 0.004041139502078295 \n",
            "At 474 epoch, Training Loss: 0.0008349126204848289 \n",
            "At 474 epoch, Validation Loss: 0.004093561787158251 \n",
            "At 475 epoch, Training Loss: 0.0007962266507092863 \n",
            "At 475 epoch, Validation Loss: 0.003932822495698929 \n",
            "At 476 epoch, Training Loss: 0.0007526666857302188 \n",
            "At 476 epoch, Validation Loss: 0.003743756329640746 \n",
            "At 477 epoch, Training Loss: 0.0007674920605495572 \n",
            "At 477 epoch, Validation Loss: 0.003980468027293682 \n",
            "At 478 epoch, Training Loss: 0.0007660796283744276 \n",
            "At 478 epoch, Validation Loss: 0.004120015539228916 \n",
            "At 479 epoch, Training Loss: 0.0010248053120449185 \n",
            "At 479 epoch, Validation Loss: 0.003940747119486332 \n",
            "At 480 epoch, Training Loss: 0.0008378392551094294 \n",
            "At 480 epoch, Validation Loss: 0.003980389796197414 \n",
            "At 481 epoch, Training Loss: 0.0009308833163231611 \n",
            "At 481 epoch, Validation Loss: 0.003900857176631689 \n",
            "At 482 epoch, Training Loss: 0.0009986141580156983 \n",
            "At 482 epoch, Validation Loss: 0.003877420676872134 \n",
            "At 483 epoch, Training Loss: 0.0009675877168774605 \n",
            "At 483 epoch, Validation Loss: 0.003850483801215887 \n",
            "At 484 epoch, Training Loss: 0.0009605702711269259 \n",
            "At 484 epoch, Validation Loss: 0.0038790558464825153 \n",
            "At 485 epoch, Training Loss: 0.0008746883948333561 \n",
            "At 485 epoch, Validation Loss: 0.003874792717397213 \n",
            "At 486 epoch, Training Loss: 0.0006106870889198035 \n",
            "At 486 epoch, Validation Loss: 0.003856094554066658 \n",
            "At 487 epoch, Training Loss: 0.0006796589063014835 \n",
            "At 487 epoch, Validation Loss: 0.003777765901759267 \n",
            "At 488 epoch, Training Loss: 0.000764052988961339 \n",
            "At 488 epoch, Validation Loss: 0.0038614394143223763 \n",
            "At 489 epoch, Training Loss: 0.0007124148542061448 \n",
            "At 489 epoch, Validation Loss: 0.0040117246098816395 \n",
            "At 490 epoch, Training Loss: 0.0011721431743353604 \n",
            "At 490 epoch, Validation Loss: 0.004689175635576248 \n",
            "At 491 epoch, Training Loss: 0.0009441793197765946 \n",
            "At 491 epoch, Validation Loss: 0.0037323806900531054 \n",
            "At 492 epoch, Training Loss: 0.0007743834867142141 \n",
            "At 492 epoch, Validation Loss: 0.003498145379126072 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 493 epoch, Training Loss: 0.0006469044077675789 \n",
            "At 493 epoch, Validation Loss: 0.0039111534133553505 \n",
            "At 494 epoch, Training Loss: 0.0008830843726173043 \n",
            "At 494 epoch, Validation Loss: 0.003688642755150795 \n",
            "At 495 epoch, Training Loss: 0.000807911972515285 \n",
            "At 495 epoch, Validation Loss: 0.003932447638362646 \n",
            "At 496 epoch, Training Loss: 0.0006486864411272108 \n",
            "At 496 epoch, Validation Loss: 0.0037682161200791597 \n",
            "At 497 epoch, Training Loss: 0.000923276727553457 \n",
            "At 497 epoch, Validation Loss: 0.003755406942218542 \n",
            "At 498 epoch, Training Loss: 0.0011527989408932626 \n",
            "At 498 epoch, Validation Loss: 0.003990999888628721 \n",
            "At 499 epoch, Training Loss: 0.0007682095456402749 \n",
            "At 499 epoch, Validation Loss: 0.004184249322861433 \n",
            "At 500 epoch, Training Loss: 0.0008587404619902373 \n",
            "At 500 epoch, Validation Loss: 0.003895638044923544 \n",
            "At 501 epoch, Training Loss: 0.0008749804692342877 \n",
            "At 501 epoch, Validation Loss: 0.0038814167492091656 \n",
            "At 502 epoch, Training Loss: 0.0007746706018224358 \n",
            "At 502 epoch, Validation Loss: 0.0038902407977730036 \n",
            "At 503 epoch, Training Loss: 0.0008313311147503555 \n",
            "At 503 epoch, Validation Loss: 0.003959422465413809 \n",
            "At 504 epoch, Training Loss: 0.0007921086740680039 \n",
            "At 504 epoch, Validation Loss: 0.004096840042620897 \n",
            "At 505 epoch, Training Loss: 0.000862814555875957 \n",
            "At 505 epoch, Validation Loss: 0.0037852185778319836 \n",
            "At 506 epoch, Training Loss: 0.0005080963164800778 \n",
            "At 506 epoch, Validation Loss: 0.004221960436552763 \n",
            "At 507 epoch, Training Loss: 0.0009253992582671344 \n",
            "At 507 epoch, Validation Loss: 0.0036282476503401995 \n",
            "At 508 epoch, Training Loss: 0.0010282746865414084 \n",
            "At 508 epoch, Validation Loss: 0.0036778650246560574 \n",
            "At 509 epoch, Training Loss: 0.0009545778972096741 \n",
            "At 509 epoch, Validation Loss: 0.004016238730400801 \n",
            "At 510 epoch, Training Loss: 0.0007915921858511865 \n",
            "At 510 epoch, Validation Loss: 0.0037072179839015007 \n",
            "At 511 epoch, Training Loss: 0.000907515175640583 \n",
            "At 511 epoch, Validation Loss: 0.003799021476879716 \n",
            "At 512 epoch, Training Loss: 0.0010505599668249488 \n",
            "At 512 epoch, Validation Loss: 0.0049911304377019405 \n",
            "At 513 epoch, Training Loss: 0.0024798044469207525 \n",
            "At 513 epoch, Validation Loss: 0.008834905922412872 \n",
            "At 514 epoch, Training Loss: 0.004551556496880949 \n",
            "At 514 epoch, Validation Loss: 0.008499656803905964 \n",
            "At 515 epoch, Training Loss: 0.011100751999765634 \n",
            "At 515 epoch, Validation Loss: 0.01226016040891409 \n",
            "At 516 epoch, Training Loss: 0.01097177341580391 \n",
            "At 516 epoch, Validation Loss: 0.015402163378894329 \n",
            "At 517 epoch, Training Loss: 0.0059413091279566285 \n",
            "At 517 epoch, Validation Loss: 0.014113804325461388 \n",
            "At 518 epoch, Training Loss: 0.004911903757601976 \n",
            "At 518 epoch, Validation Loss: 0.007392380386590958 \n",
            "At 519 epoch, Training Loss: 0.0036442081443965437 \n",
            "At 519 epoch, Validation Loss: 0.005762531887739897 \n",
            "At 520 epoch, Training Loss: 0.0024435847532004117 \n",
            "At 520 epoch, Validation Loss: 0.005123700015246868 \n",
            "At 521 epoch, Training Loss: 0.0016173422685824335 \n",
            "At 521 epoch, Validation Loss: 0.004882936831563711 \n",
            "At 522 epoch, Training Loss: 0.001749885198660195 \n",
            "At 522 epoch, Validation Loss: 0.004503353498876095 \n",
            "At 523 epoch, Training Loss: 0.0013381869648583233 \n",
            "At 523 epoch, Validation Loss: 0.004201176110655069 \n",
            "At 524 epoch, Training Loss: 0.0015433839056640864 \n",
            "At 524 epoch, Validation Loss: 0.003926541190594435 \n",
            "At 525 epoch, Training Loss: 0.0013342062011361122 \n",
            "At 525 epoch, Validation Loss: 0.004261995665729046 \n",
            "At 526 epoch, Training Loss: 0.0012070739408954977 \n",
            "At 526 epoch, Validation Loss: 0.0037888390943408012 \n",
            "At 527 epoch, Training Loss: 0.0009995502419769763 \n",
            "At 527 epoch, Validation Loss: 0.00402939785271883 \n",
            "At 528 epoch, Training Loss: 0.0016924996161833405 \n",
            "At 528 epoch, Validation Loss: 0.0036530650686472654 \n",
            "At 529 epoch, Training Loss: 0.0011388844694010913 \n",
            "At 529 epoch, Validation Loss: 0.003818661905825138 \n",
            "At 530 epoch, Training Loss: 0.0010525564197450877 \n",
            "At 530 epoch, Validation Loss: 0.0036323247477412224 \n",
            "At 531 epoch, Training Loss: 0.001317250239662826 \n",
            "At 531 epoch, Validation Loss: 0.004233594052493572 \n",
            "At 532 epoch, Training Loss: 0.001325175736565143 \n",
            "At 532 epoch, Validation Loss: 0.003580972785130143 \n",
            "At 533 epoch, Training Loss: 0.0011753301252610982 \n",
            "At 533 epoch, Validation Loss: 0.003983539529144764 \n",
            "At 534 epoch, Training Loss: 0.0009175377432256937 \n",
            "At 534 epoch, Validation Loss: 0.0035881567746400833 \n",
            "At 535 epoch, Training Loss: 0.0011584360618144274 \n",
            "At 535 epoch, Validation Loss: 0.0036163399927318096 \n",
            "At 536 epoch, Training Loss: 0.0009609156637452543 \n",
            "At 536 epoch, Validation Loss: 0.0036599363666027784 \n",
            "At 537 epoch, Training Loss: 0.0010193820111453534 \n",
            "At 537 epoch, Validation Loss: 0.0038001826032996178 \n",
            "At 538 epoch, Training Loss: 0.0009824221138842403 \n",
            "At 538 epoch, Validation Loss: 0.0036580876912921667 \n",
            "At 539 epoch, Training Loss: 0.0010213235393166542 \n",
            "At 539 epoch, Validation Loss: 0.003590162843465805 \n",
            "At 540 epoch, Training Loss: 0.0008246543467976153 \n",
            "At 540 epoch, Validation Loss: 0.0036950993817299604 \n",
            "At 541 epoch, Training Loss: 0.0008375640260055661 \n",
            "At 541 epoch, Validation Loss: 0.0036293065641075373 \n",
            "At 542 epoch, Training Loss: 0.0009071962093003094 \n",
            "At 542 epoch, Validation Loss: 0.0036217730958014727 \n",
            "At 543 epoch, Training Loss: 0.0010780767537653447 \n",
            "At 543 epoch, Validation Loss: 0.0037986079696565866 \n",
            "At 544 epoch, Training Loss: 0.00099509033607319 \n",
            "At 544 epoch, Validation Loss: 0.0037597506307065487 \n",
            "At 545 epoch, Training Loss: 0.0009002939681522548 \n",
            "At 545 epoch, Validation Loss: 0.003771501127630472 \n",
            "At 546 epoch, Training Loss: 0.0009524684981442987 \n",
            "At 546 epoch, Validation Loss: 0.003888800973072648 \n",
            "At 547 epoch, Training Loss: 0.0010198499774560332 \n",
            "At 547 epoch, Validation Loss: 0.00417315773665905 \n",
            "At 548 epoch, Training Loss: 0.0010498660965822637 \n",
            "At 548 epoch, Validation Loss: 0.0035032585728913546 \n",
            "At 549 epoch, Training Loss: 0.0007672230596654118 \n",
            "At 549 epoch, Validation Loss: 0.003637694986537099 \n",
            "At 550 epoch, Training Loss: 0.0007924312027171254 \n",
            "At 550 epoch, Validation Loss: 0.003908094950020313 \n",
            "At 551 epoch, Training Loss: 0.0007647454971447587 \n",
            "At 551 epoch, Validation Loss: 0.0038306524511426687 \n",
            "At 552 epoch, Training Loss: 0.0009797559934668243 \n",
            "At 552 epoch, Validation Loss: 0.003631477477028966 \n",
            "At 553 epoch, Training Loss: 0.0008496840659063309 \n",
            "At 553 epoch, Validation Loss: 0.0036313277669250965 \n",
            "At 554 epoch, Training Loss: 0.0005800064478535205 \n",
            "At 554 epoch, Validation Loss: 0.003810660447925329 \n",
            "At 555 epoch, Training Loss: 0.0009617935284040868 \n",
            "At 555 epoch, Validation Loss: 0.0036015044897794724 \n",
            "At 556 epoch, Training Loss: 0.0006362354091834277 \n",
            "At 556 epoch, Validation Loss: 0.0035133350174874067 \n",
            "At 557 epoch, Training Loss: 0.0006006815005093813 \n",
            "At 557 epoch, Validation Loss: 0.0037159230560064316 \n",
            "At 558 epoch, Training Loss: 0.0007648380473256111 \n",
            "At 558 epoch, Validation Loss: 0.003903498873114586 \n",
            "At 559 epoch, Training Loss: 0.0008056498016230762 \n",
            "At 559 epoch, Validation Loss: 0.0037652563769370317 \n",
            "At 560 epoch, Training Loss: 0.0006698674289509654 \n",
            "At 560 epoch, Validation Loss: 0.003495939075946808 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 561 epoch, Training Loss: 0.0006983205676078796 \n",
            "At 561 epoch, Validation Loss: 0.003793100593611598 \n",
            "At 562 epoch, Training Loss: 0.0007959348149597645 \n",
            "At 562 epoch, Validation Loss: 0.003765523200854659 \n",
            "At 563 epoch, Training Loss: 0.0009460166795179248 \n",
            "At 563 epoch, Validation Loss: 0.0036147674545645714 \n",
            "At 564 epoch, Training Loss: 0.0006235706503503024 \n",
            "At 564 epoch, Validation Loss: 0.003852668683975935 \n",
            "At 565 epoch, Training Loss: 0.0007739674882031977 \n",
            "At 565 epoch, Validation Loss: 0.0036732691805809736 \n",
            "At 566 epoch, Training Loss: 0.0008030197001062334 \n",
            "At 566 epoch, Validation Loss: 0.0038506356067955494 \n",
            "At 567 epoch, Training Loss: 0.000773399299941957 \n",
            "At 567 epoch, Validation Loss: 0.003566529368981719 \n",
            "At 568 epoch, Training Loss: 0.000751872721593827 \n",
            "At 568 epoch, Validation Loss: 0.003664152231067419 \n",
            "At 569 epoch, Training Loss: 0.0007481576467398554 \n",
            "At 569 epoch, Validation Loss: 0.0036136091221123934 \n",
            "At 570 epoch, Training Loss: 0.0007447451702319085 \n",
            "At 570 epoch, Validation Loss: 0.003399706445634365 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 571 epoch, Training Loss: 0.0007339774747379124 \n",
            "At 571 epoch, Validation Loss: 0.0038155708461999893 \n",
            "At 572 epoch, Training Loss: 0.0005646126926876604 \n",
            "At 572 epoch, Validation Loss: 0.0036317959893494844 \n",
            "At 573 epoch, Training Loss: 0.0006104151718318463 \n",
            "At 573 epoch, Validation Loss: 0.0037244504783302546 \n",
            "At 574 epoch, Training Loss: 0.0007773085904773325 \n",
            "At 574 epoch, Validation Loss: 0.003927441313862801 \n",
            "At 575 epoch, Training Loss: 0.0005937935202382505 \n",
            "At 575 epoch, Validation Loss: 0.003483297536149621 \n",
            "At 576 epoch, Training Loss: 0.0008012035628780723 \n",
            "At 576 epoch, Validation Loss: 0.0036618635058403015 \n",
            "At 577 epoch, Training Loss: 0.0007604181882925332 \n",
            "At 577 epoch, Validation Loss: 0.004128722939640284 \n",
            "At 578 epoch, Training Loss: 0.0007586857071146369 \n",
            "At 578 epoch, Validation Loss: 0.0037176506593823433 \n",
            "At 579 epoch, Training Loss: 0.0007085929741151631 \n",
            "At 579 epoch, Validation Loss: 0.0037221319507807493 \n",
            "At 580 epoch, Training Loss: 0.0010620047920383512 \n",
            "At 580 epoch, Validation Loss: 0.0036725723184645176 \n",
            "At 581 epoch, Training Loss: 0.0007633070345036685 \n",
            "At 581 epoch, Validation Loss: 0.0038753242697566748 \n",
            "At 582 epoch, Training Loss: 0.0008304057526402175 \n",
            "At 582 epoch, Validation Loss: 0.003879600204527378 \n",
            "At 583 epoch, Training Loss: 0.0006521699717268348 \n",
            "At 583 epoch, Validation Loss: 0.0034681283868849277 \n",
            "At 584 epoch, Training Loss: 0.0006589198834262789 \n",
            "At 584 epoch, Validation Loss: 0.0036428524181246758 \n",
            "At 585 epoch, Training Loss: 0.0006644738255999983 \n",
            "At 585 epoch, Validation Loss: 0.003935592714697123 \n",
            "At 586 epoch, Training Loss: 0.0006015017337631434 \n",
            "At 586 epoch, Validation Loss: 0.0035892308223992586 \n",
            "At 587 epoch, Training Loss: 0.0006734778638929128 \n",
            "At 587 epoch, Validation Loss: 0.0033900742419064045 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 588 epoch, Training Loss: 0.000882873625960201 \n",
            "At 588 epoch, Validation Loss: 0.003962997812777758 \n",
            "At 589 epoch, Training Loss: 0.0010458101984113454 \n",
            "At 589 epoch, Validation Loss: 0.0038726218044757843 \n",
            "At 590 epoch, Training Loss: 0.0006726385792717338 \n",
            "At 590 epoch, Validation Loss: 0.0036496107932180166 \n",
            "At 591 epoch, Training Loss: 0.0007212042459286749 \n",
            "At 591 epoch, Validation Loss: 0.0037655194755643606 \n",
            "At 592 epoch, Training Loss: 0.0007403734722174704 \n",
            "At 592 epoch, Validation Loss: 0.004052300006151199 \n",
            "At 593 epoch, Training Loss: 0.0007510177325457335 \n",
            "At 593 epoch, Validation Loss: 0.003554860595613718 \n",
            "At 594 epoch, Training Loss: 0.0004925079527311027 \n",
            "At 594 epoch, Validation Loss: 0.0039943670853972435 \n",
            "At 595 epoch, Training Loss: 0.0007653096516150981 \n",
            "At 595 epoch, Validation Loss: 0.003596205497160554 \n",
            "At 596 epoch, Training Loss: 0.000898030586540699 \n",
            "At 596 epoch, Validation Loss: 0.003986007999628782 \n",
            "At 597 epoch, Training Loss: 0.0006635355530306697 \n",
            "At 597 epoch, Validation Loss: 0.0036060470156371593 \n",
            "At 598 epoch, Training Loss: 0.0006245982716791332 \n",
            "At 598 epoch, Validation Loss: 0.0039297048933804035 \n",
            "At 599 epoch, Training Loss: 0.000743405509274453 \n",
            "At 599 epoch, Validation Loss: 0.003691387828439474 \n",
            "At 600 epoch, Training Loss: 0.0006396731536369771 \n",
            "At 600 epoch, Validation Loss: 0.003710356308147311 \n",
            "At 601 epoch, Training Loss: 0.0006906759517733008 \n",
            "At 601 epoch, Validation Loss: 0.0034501077607274055 \n",
            "At 602 epoch, Training Loss: 0.0005716967047192156 \n",
            "At 602 epoch, Validation Loss: 0.003983378876000643 \n",
            "At 603 epoch, Training Loss: 0.000711096590384841 \n",
            "At 603 epoch, Validation Loss: 0.003423805348575115 \n",
            "At 604 epoch, Training Loss: 0.0005945534852799028 \n",
            "At 604 epoch, Validation Loss: 0.003627765690907836 \n",
            "At 605 epoch, Training Loss: 0.0008189999382011593 \n",
            "At 605 epoch, Validation Loss: 0.003431732999160886 \n",
            "At 606 epoch, Training Loss: 0.0008377408434171229 \n",
            "At 606 epoch, Validation Loss: 0.003689778968691826 \n",
            "At 607 epoch, Training Loss: 0.0008471740875393152 \n",
            "At 607 epoch, Validation Loss: 0.0045011830516159534 \n",
            "At 608 epoch, Training Loss: 0.0008615123690105975 \n",
            "At 608 epoch, Validation Loss: 0.0034738066606223583 \n",
            "At 609 epoch, Training Loss: 0.0006551476544700563 \n",
            "At 609 epoch, Validation Loss: 0.004164996091276407 \n",
            "At 610 epoch, Training Loss: 0.0007251190720126033 \n",
            "At 610 epoch, Validation Loss: 0.0032542317640036345 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 611 epoch, Training Loss: 0.0006328423740342259 \n",
            "At 611 epoch, Validation Loss: 0.003445807844400406 \n",
            "At 612 epoch, Training Loss: 0.0004968942317645997 \n",
            "At 612 epoch, Validation Loss: 0.003643700620159507 \n",
            "At 613 epoch, Training Loss: 0.0007067051192279905 \n",
            "At 613 epoch, Validation Loss: 0.0035716372076421976 \n",
            "At 614 epoch, Training Loss: 0.0007558314071502536 \n",
            "At 614 epoch, Validation Loss: 0.0035279537551105022 \n",
            "At 615 epoch, Training Loss: 0.0006061268679331989 \n",
            "At 615 epoch, Validation Loss: 0.003494168631732464 \n",
            "At 616 epoch, Training Loss: 0.0005618640192551538 \n",
            "At 616 epoch, Validation Loss: 0.003685217583552003 \n",
            "At 617 epoch, Training Loss: 0.0005875642178580165 \n",
            "At 617 epoch, Validation Loss: 0.0033978442661464214 \n",
            "At 618 epoch, Training Loss: 0.0007779119536280632 \n",
            "At 618 epoch, Validation Loss: 0.003163246437907219 \n",
            "Minimum validation loss detected, saving model......................................................................................\n",
            "At 619 epoch, Training Loss: 0.0005194631114136428 \n",
            "At 619 epoch, Validation Loss: 0.0036882038693875074 \n",
            "At 620 epoch, Training Loss: 0.0006360637082252652 \n",
            "At 620 epoch, Validation Loss: 0.0036354432813823223 \n",
            "At 621 epoch, Training Loss: 0.0005350175313651561 \n",
            "At 621 epoch, Validation Loss: 0.0033543549943715334 \n",
            "At 622 epoch, Training Loss: 0.0010392745258286594 \n",
            "At 622 epoch, Validation Loss: 0.003619842929765582 \n",
            "At 623 epoch, Training Loss: 0.0006716203351970762 \n",
            "At 623 epoch, Validation Loss: 0.004554190672934055 \n",
            "At 624 epoch, Training Loss: 0.0027091491501778364 \n",
            "At 624 epoch, Validation Loss: 0.011615403927862644 \n",
            "At 625 epoch, Training Loss: 0.09704026728868484 \n",
            "At 625 epoch, Validation Loss: 0.4092518091201782 \n",
            "At 626 epoch, Training Loss: 0.11306536048650742 \n",
            "At 626 epoch, Validation Loss: 0.11380597203969955 \n",
            "At 627 epoch, Training Loss: 0.04407692477107048 \n",
            "At 627 epoch, Validation Loss: 0.025809574872255325 \n",
            "At 628 epoch, Training Loss: 0.03170862160623074 \n",
            "At 628 epoch, Validation Loss: 0.013081156648695469 \n",
            "At 629 epoch, Training Loss: 0.0225408136844635 \n",
            "At 629 epoch, Validation Loss: 0.011959963478147984 \n",
            "At 630 epoch, Training Loss: 0.01681013498455286 \n",
            "At 630 epoch, Validation Loss: 0.008219026029109955 \n",
            "At 631 epoch, Training Loss: 0.01366430874913931 \n",
            "At 631 epoch, Validation Loss: 0.006721235811710358 \n",
            "At 632 epoch, Training Loss: 0.009885002486407756 \n",
            "At 632 epoch, Validation Loss: 0.0047020395286381245 \n",
            "At 633 epoch, Training Loss: 0.008053426351398229 \n",
            "At 633 epoch, Validation Loss: 0.004285101313143969 \n",
            "At 634 epoch, Training Loss: 0.0068644234910607334 \n",
            "At 634 epoch, Validation Loss: 0.004041916690766811 \n",
            "At 635 epoch, Training Loss: 0.00602125832810998 \n",
            "At 635 epoch, Validation Loss: 0.0039043137803673744 \n",
            "At 636 epoch, Training Loss: 0.005292120203375816 \n",
            "At 636 epoch, Validation Loss: 0.003791428403928876 \n",
            "At 637 epoch, Training Loss: 0.003980294521898031 \n",
            "At 637 epoch, Validation Loss: 0.0037909052334725857 \n",
            "At 638 epoch, Training Loss: 0.003962409775704145 \n",
            "At 638 epoch, Validation Loss: 0.0037740576080977917 \n",
            "At 639 epoch, Training Loss: 0.0035162560176104306 \n",
            "At 639 epoch, Validation Loss: 0.004178886767476797 \n",
            "At 640 epoch, Training Loss: 0.003343218192458153 \n",
            "At 640 epoch, Validation Loss: 0.003894426627084613 \n",
            "At 641 epoch, Training Loss: 0.0027476551244035364 \n",
            "At 641 epoch, Validation Loss: 0.004449018277227879 \n",
            "At 642 epoch, Training Loss: 0.0030624973587691784 \n",
            "At 642 epoch, Validation Loss: 0.0043636104092001915 \n",
            "At 643 epoch, Training Loss: 0.0029042305890470744 \n",
            "At 643 epoch, Validation Loss: 0.004556227475404739 \n",
            "At 644 epoch, Training Loss: 0.0029849563725292683 \n",
            "At 644 epoch, Validation Loss: 0.004173153545707464 \n",
            "At 645 epoch, Training Loss: 0.0028004322666674852 \n",
            "At 645 epoch, Validation Loss: 0.0045288982801139355 \n",
            "At 646 epoch, Training Loss: 0.0023800390539690853 \n",
            "At 646 epoch, Validation Loss: 0.004285990260541439 \n",
            "At 647 epoch, Training Loss: 0.0022987103555351497 \n",
            "At 647 epoch, Validation Loss: 0.004640255123376846 \n",
            "At 648 epoch, Training Loss: 0.0025413871044293046 \n",
            "At 648 epoch, Validation Loss: 0.004188475664705038 \n",
            "At 649 epoch, Training Loss: 0.002344036498107016 \n",
            "At 649 epoch, Validation Loss: 0.004528605844825506 \n",
            "At 650 epoch, Training Loss: 0.00235187541693449 \n",
            "At 650 epoch, Validation Loss: 0.0049562230706214905 \n",
            "At 651 epoch, Training Loss: 0.0021029418567195535 \n",
            "At 651 epoch, Validation Loss: 0.00452802749350667 \n",
            "At 652 epoch, Training Loss: 0.002157562621869147 \n",
            "At 652 epoch, Validation Loss: 0.003912315703928471 \n",
            "At 653 epoch, Training Loss: 0.0021844751900061965 \n",
            "At 653 epoch, Validation Loss: 0.004589136224240065 \n",
            "At 654 epoch, Training Loss: 0.0022095766151323916 \n",
            "At 654 epoch, Validation Loss: 0.004937072284519672 \n",
            "At 655 epoch, Training Loss: 0.00158915554638952 \n",
            "At 655 epoch, Validation Loss: 0.004214553162455559 \n",
            "At 656 epoch, Training Loss: 0.0019961165729910136 \n",
            "At 656 epoch, Validation Loss: 0.00439074682071805 \n",
            "At 657 epoch, Training Loss: 0.0020081027643755077 \n",
            "At 657 epoch, Validation Loss: 0.0049338508397340775 \n",
            "At 658 epoch, Training Loss: 0.0020402540685608983 \n",
            "At 658 epoch, Validation Loss: 0.003992186393588781 \n",
            "At 659 epoch, Training Loss: 0.0017380311153829097 \n",
            "At 659 epoch, Validation Loss: 0.004408045671880245 \n",
            "At 660 epoch, Training Loss: 0.0022580519551411273 \n",
            "At 660 epoch, Validation Loss: 0.004514169413596392 \n",
            "At 661 epoch, Training Loss: 0.001731723453849554 \n",
            "At 661 epoch, Validation Loss: 0.004158657509833574 \n",
            "At 662 epoch, Training Loss: 0.0016882546711713075 \n",
            "At 662 epoch, Validation Loss: 0.004455793183296919 \n",
            "At 663 epoch, Training Loss: 0.0016863215249031782 \n",
            "At 663 epoch, Validation Loss: 0.004580484703183174 \n",
            "At 664 epoch, Training Loss: 0.0016269162064418198 \n",
            "At 664 epoch, Validation Loss: 0.00441179471090436 \n",
            "At 665 epoch, Training Loss: 0.0019048482179641723 \n",
            "At 665 epoch, Validation Loss: 0.00409355852752924 \n",
            "At 666 epoch, Training Loss: 0.0012995789758861065 \n",
            "At 666 epoch, Validation Loss: 0.004447159357368946 \n",
            "At 667 epoch, Training Loss: 0.001334905088879168 \n",
            "At 667 epoch, Validation Loss: 0.004766623489558697 \n",
            "At 668 epoch, Training Loss: 0.001730886264704168 \n",
            "At 668 epoch, Validation Loss: 0.003982537426054478 \n",
            "At 669 epoch, Training Loss: 0.0019049628870561719 \n",
            "At 669 epoch, Validation Loss: 0.004173892084509134 \n",
            "At 670 epoch, Training Loss: 0.0015667120460420847 \n",
            "At 670 epoch, Validation Loss: 0.004245904739946127 \n",
            "At 671 epoch, Training Loss: 0.001504079590085894 \n",
            "At 671 epoch, Validation Loss: 0.004452959634363651 \n",
            "At 672 epoch, Training Loss: 0.0015566763002425432 \n",
            "At 672 epoch, Validation Loss: 0.004465984180569649 \n",
            "At 673 epoch, Training Loss: 0.0015007029520347714 \n",
            "At 673 epoch, Validation Loss: 0.0041289436630904675 \n",
            "At 674 epoch, Training Loss: 0.0015554641606286169 \n",
            "At 674 epoch, Validation Loss: 0.004678528755903244 \n",
            "At 675 epoch, Training Loss: 0.0014973051147535443 \n",
            "At 675 epoch, Validation Loss: 0.004185262601822615 \n",
            "At 676 epoch, Training Loss: 0.0016399601940065623 \n",
            "At 676 epoch, Validation Loss: 0.0042059761472046375 \n",
            "At 677 epoch, Training Loss: 0.0014118253951892258 \n",
            "At 677 epoch, Validation Loss: 0.004151010420173407 \n",
            "At 678 epoch, Training Loss: 0.0015668043866753577 \n",
            "At 678 epoch, Validation Loss: 0.004307168535888195 \n",
            "At 679 epoch, Training Loss: 0.0014605220407247544 \n",
            "At 679 epoch, Validation Loss: 0.004293414298444986 \n",
            "At 680 epoch, Training Loss: 0.0014093013247475027 \n",
            "At 680 epoch, Validation Loss: 0.004275576211512089 \n",
            "At 681 epoch, Training Loss: 0.0013719309587031602 \n",
            "At 681 epoch, Validation Loss: 0.004267000127583742 \n",
            "At 682 epoch, Training Loss: 0.001442867098376155 \n",
            "At 682 epoch, Validation Loss: 0.004323008470237255 \n",
            "At 683 epoch, Training Loss: 0.0012758728233166039 \n",
            "At 683 epoch, Validation Loss: 0.004426523111760616 \n",
            "At 684 epoch, Training Loss: 0.0014314727392047643 \n",
            "At 684 epoch, Validation Loss: 0.004124682862311602 \n",
            "At 685 epoch, Training Loss: 0.0013211911544203758 \n",
            "At 685 epoch, Validation Loss: 0.005032227840274572 \n",
            "At 686 epoch, Training Loss: 0.0016177658922970294 \n",
            "At 686 epoch, Validation Loss: 0.0038441948127001524 \n",
            "At 687 epoch, Training Loss: 0.0012933031306602062 \n",
            "At 687 epoch, Validation Loss: 0.0048866537399590015 \n",
            "At 688 epoch, Training Loss: 0.0011021062266081572 \n",
            "At 688 epoch, Validation Loss: 0.003810869064182043 \n",
            "At 689 epoch, Training Loss: 0.0013049262808635831 \n",
            "At 689 epoch, Validation Loss: 0.00457386951893568 \n",
            "At 690 epoch, Training Loss: 0.0012798508629202842 \n",
            "At 690 epoch, Validation Loss: 0.0039250473491847515 \n",
            "At 691 epoch, Training Loss: 0.0013470103847794234 \n",
            "At 691 epoch, Validation Loss: 0.004032394383102655 \n",
            "At 692 epoch, Training Loss: 0.0013975544134154916 \n",
            "At 692 epoch, Validation Loss: 0.00435735983774066 \n",
            "At 693 epoch, Training Loss: 0.001482336618937552 \n",
            "At 693 epoch, Validation Loss: 0.004041972570121288 \n",
            "At 694 epoch, Training Loss: 0.0013810917269438505 \n",
            "At 694 epoch, Validation Loss: 0.004300971515476704 \n",
            "At 695 epoch, Training Loss: 0.001126756425946951 \n",
            "At 695 epoch, Validation Loss: 0.0038088758010417223 \n",
            "At 696 epoch, Training Loss: 0.0011772228055633605 \n",
            "At 696 epoch, Validation Loss: 0.004457914270460606 \n",
            "At 697 epoch, Training Loss: 0.001221390429418534 \n",
            "At 697 epoch, Validation Loss: 0.003934931010007858 \n",
            "At 698 epoch, Training Loss: 0.0012174937059171499 \n",
            "At 698 epoch, Validation Loss: 0.0041535962373018265 \n",
            "At 699 epoch, Training Loss: 0.0013987670768983661 \n",
            "At 699 epoch, Validation Loss: 0.0040262979455292225 \n",
            "At 700 epoch, Training Loss: 0.0012739781523123384 \n",
            "At 700 epoch, Validation Loss: 0.0041724154725670815 \n",
            "At 701 epoch, Training Loss: 0.0011766799027100206 \n",
            "At 701 epoch, Validation Loss: 0.004153545014560223 \n",
            "At 702 epoch, Training Loss: 0.001139911205973476 \n",
            "At 702 epoch, Validation Loss: 0.003799406811594963 \n",
            "At 703 epoch, Training Loss: 0.0013234379817731678 \n",
            "At 703 epoch, Validation Loss: 0.0041032698936760426 \n",
            "At 704 epoch, Training Loss: 0.0013272263808175922 \n",
            "At 704 epoch, Validation Loss: 0.003967692144215107 \n",
            "At 705 epoch, Training Loss: 0.0011514383018948137 \n",
            "At 705 epoch, Validation Loss: 0.004574052058160305 \n",
            "At 706 epoch, Training Loss: 0.0014470600057393313 \n",
            "At 706 epoch, Validation Loss: 0.00373193621635437 \n",
            "At 707 epoch, Training Loss: 0.0011913130641914905 \n",
            "At 707 epoch, Validation Loss: 0.00426617031916976 \n",
            "At 708 epoch, Training Loss: 0.001210185792297125 \n",
            "At 708 epoch, Validation Loss: 0.003924396820366383 \n",
            "At 709 epoch, Training Loss: 0.001076328568160534 \n",
            "At 709 epoch, Validation Loss: 0.003923686686903238 \n",
            "At 710 epoch, Training Loss: 0.00131254120497033 \n",
            "At 710 epoch, Validation Loss: 0.003962429240345955 \n",
            "At 711 epoch, Training Loss: 0.0014017180306836962 \n",
            "At 711 epoch, Validation Loss: 0.004146699793636799 \n",
            "At 712 epoch, Training Loss: 0.0012433982337825 \n",
            "At 712 epoch, Validation Loss: 0.0036744081880897284 \n",
            "At 713 epoch, Training Loss: 0.0011635586386546492 \n",
            "At 713 epoch, Validation Loss: 0.004573341459035873 \n",
            "At 714 epoch, Training Loss: 0.0011434539686888457 \n",
            "At 714 epoch, Validation Loss: 0.0039007174782454967 \n",
            "At 715 epoch, Training Loss: 0.0009257992496713996 \n",
            "At 715 epoch, Validation Loss: 0.003890738822519779 \n",
            "At 716 epoch, Training Loss: 0.0011509281350299716 \n",
            "At 716 epoch, Validation Loss: 0.004017353057861328 \n",
            "At 717 epoch, Training Loss: 0.001046032807789743 \n",
            "At 717 epoch, Validation Loss: 0.004076359327882528 \n",
            "At 718 epoch, Training Loss: 0.0010933928773738445 \n",
            "At 718 epoch, Validation Loss: 0.0037679229862987995 \n",
            "At 719 epoch, Training Loss: 0.0009326574509032071 \n",
            "At 719 epoch, Validation Loss: 0.004073311574757099 \n",
            "At 720 epoch, Training Loss: 0.001087288255803287 \n",
            "At 720 epoch, Validation Loss: 0.003737463615834713 \n",
            "At 721 epoch, Training Loss: 0.0011882741702720523 \n",
            "At 721 epoch, Validation Loss: 0.0038773338310420513 \n",
            "At 722 epoch, Training Loss: 0.0009764359332621097 \n",
            "At 722 epoch, Validation Loss: 0.003970228135585785 \n",
            "At 723 epoch, Training Loss: 0.0011597958276979626 \n",
            "At 723 epoch, Validation Loss: 0.004002142697572708 \n",
            "At 724 epoch, Training Loss: 0.0010557900532148778 \n",
            "At 724 epoch, Validation Loss: 0.0037085129879415035 \n",
            "At 725 epoch, Training Loss: 0.0011661675525829196 \n",
            "At 725 epoch, Validation Loss: 0.003928534686565399 \n",
            "At 726 epoch, Training Loss: 0.0011471203295513987 \n",
            "At 726 epoch, Validation Loss: 0.0037448215298354626 \n",
            "At 727 epoch, Training Loss: 0.0013210577424615621 \n",
            "At 727 epoch, Validation Loss: 0.004090787842869759 \n",
            "At 728 epoch, Training Loss: 0.0011395399109460414 \n",
            "At 728 epoch, Validation Loss: 0.003984504844993353 \n",
            "At 729 epoch, Training Loss: 0.0009878993034362793 \n",
            "At 729 epoch, Validation Loss: 0.003874974325299263 \n",
            "At 730 epoch, Training Loss: 0.0010526723344810307 \n",
            "At 730 epoch, Validation Loss: 0.0037540846969932318 \n",
            "At 731 epoch, Training Loss: 0.0010289851226843894 \n",
            "At 731 epoch, Validation Loss: 0.003865791019052267 \n",
            "At 732 epoch, Training Loss: 0.0011021367041394114 \n",
            "At 732 epoch, Validation Loss: 0.0038226929027587175 \n",
            "At 733 epoch, Training Loss: 0.0013156541739590467 \n",
            "At 733 epoch, Validation Loss: 0.004181405529379845 \n",
            "At 734 epoch, Training Loss: 0.000952315516769886 \n",
            "At 734 epoch, Validation Loss: 0.0038841129280626774 \n",
            "At 735 epoch, Training Loss: 0.0008199022035114467 \n",
            "At 735 epoch, Validation Loss: 0.004097380675375462 \n",
            "At 736 epoch, Training Loss: 0.0010988022084347903 \n",
            "At 736 epoch, Validation Loss: 0.003902857657521963 \n",
            "At 737 epoch, Training Loss: 0.0011792225181125105 \n",
            "At 737 epoch, Validation Loss: 0.003674709238111973 \n",
            "At 738 epoch, Training Loss: 0.0009452989208512009 \n",
            "At 738 epoch, Validation Loss: 0.004177577793598175 \n",
            "At 739 epoch, Training Loss: 0.0008966977591626346 \n",
            "At 739 epoch, Validation Loss: 0.003970602992922068 \n",
            "At 740 epoch, Training Loss: 0.0010248100850731134 \n",
            "At 740 epoch, Validation Loss: 0.0038713987451046705 \n",
            "At 741 epoch, Training Loss: 0.0012466655112802981 \n",
            "At 741 epoch, Validation Loss: 0.003873002715408802 \n",
            "At 742 epoch, Training Loss: 0.0008527482044883072 \n",
            "At 742 epoch, Validation Loss: 0.0039195953868329525 \n",
            "At 743 epoch, Training Loss: 0.0007484248373657465 \n",
            "At 743 epoch, Validation Loss: 0.0038048098795115948 \n",
            "At 744 epoch, Training Loss: 0.0010659946943633257 \n",
            "At 744 epoch, Validation Loss: 0.0038353456184267998 \n",
            "At 745 epoch, Training Loss: 0.0009595828014425933 \n",
            "At 745 epoch, Validation Loss: 0.003629686078056693 \n",
            "At 746 epoch, Training Loss: 0.000858730310574174 \n",
            "At 746 epoch, Validation Loss: 0.003931671380996704 \n",
            "At 747 epoch, Training Loss: 0.0010622891713865101 \n",
            "At 747 epoch, Validation Loss: 0.0036732167936861515 \n",
            "At 748 epoch, Training Loss: 0.0012140114326030017 \n",
            "At 748 epoch, Validation Loss: 0.004390178248286247 \n",
            "At 749 epoch, Training Loss: 0.0008055569138377905 \n",
            "At 749 epoch, Validation Loss: 0.003641156479716301 \n",
            "At 750 epoch, Training Loss: 0.0009233513963408768 \n",
            "At 750 epoch, Validation Loss: 0.004486734978854656 \n",
            "At 751 epoch, Training Loss: 0.0009719273773953318 \n",
            "At 751 epoch, Validation Loss: 0.003656113753095269 \n",
            "At 752 epoch, Training Loss: 0.0010230285814031958 \n",
            "At 752 epoch, Validation Loss: 0.003908096347004175 \n",
            "At 753 epoch, Training Loss: 0.0010025751893408597 \n",
            "At 753 epoch, Validation Loss: 0.0034950347617268562 \n",
            "At 754 epoch, Training Loss: 0.0011110036168247462 \n",
            "At 754 epoch, Validation Loss: 0.004052048549056053 \n",
            "At 755 epoch, Training Loss: 0.001142294448800385 \n",
            "At 755 epoch, Validation Loss: 0.0036920150741934776 \n",
            "At 756 epoch, Training Loss: 0.0011784958536736668 \n",
            "At 756 epoch, Validation Loss: 0.0038214107044041157 \n",
            "At 757 epoch, Training Loss: 0.0010234614834189414 \n",
            "At 757 epoch, Validation Loss: 0.003757479367777705 \n",
            "At 758 epoch, Training Loss: 0.0011076848837547003 \n",
            "At 758 epoch, Validation Loss: 0.003762806300073862 \n",
            "At 759 epoch, Training Loss: 0.0010315215680748225 \n",
            "At 759 epoch, Validation Loss: 0.003958187531679869 \n",
            "At 760 epoch, Training Loss: 0.00100316972238943 \n",
            "At 760 epoch, Validation Loss: 0.0035470211878418922 \n",
            "At 761 epoch, Training Loss: 0.0009648945415392518 \n",
            "At 761 epoch, Validation Loss: 0.004076744895428419 \n",
            "At 762 epoch, Training Loss: 0.0008714865311048925 \n",
            "At 762 epoch, Validation Loss: 0.003755898680537939 \n",
            "At 763 epoch, Training Loss: 0.001091129647102207 \n",
            "At 763 epoch, Validation Loss: 0.0036711611319333315 \n",
            "At 764 epoch, Training Loss: 0.0009068044950254261 \n",
            "At 764 epoch, Validation Loss: 0.003978752996772528 \n",
            "At 765 epoch, Training Loss: 0.0010122917708940804 \n",
            "At 765 epoch, Validation Loss: 0.003642090829089284 \n",
            "At 766 epoch, Training Loss: 0.000983700016513467 \n",
            "At 766 epoch, Validation Loss: 0.004044406581670046 \n",
            "At 767 epoch, Training Loss: 0.0010300170746631921 \n",
            "At 767 epoch, Validation Loss: 0.0036268611438572407 \n",
            "At 768 epoch, Training Loss: 0.0006476904789451509 \n",
            "At 768 epoch, Validation Loss: 0.004038186278194189 \n",
            "At 769 epoch, Training Loss: 0.0009302007616497576 \n",
            "At 769 epoch, Validation Loss: 0.0036523828748613596 \n",
            "At 770 epoch, Training Loss: 0.0011542252614162862 \n",
            "At 770 epoch, Validation Loss: 0.0037814124953001738 \n",
            "At 771 epoch, Training Loss: 0.0009194412617944181 \n",
            "At 771 epoch, Validation Loss: 0.0039896066300570965 \n",
            "At 772 epoch, Training Loss: 0.0006726133113261312 \n",
            "At 772 epoch, Validation Loss: 0.0036322367377579212 \n",
            "At 773 epoch, Training Loss: 0.0012351559475064279 \n",
            "At 773 epoch, Validation Loss: 0.004085047636181116 \n",
            "At 774 epoch, Training Loss: 0.0008877853048034013 \n",
            "At 774 epoch, Validation Loss: 0.003857802366837859 \n",
            "At 775 epoch, Training Loss: 0.0009099215152673423 \n",
            "At 775 epoch, Validation Loss: 0.0038777098525315523 \n",
            "At 776 epoch, Training Loss: 0.0009161346941255033 \n",
            "At 776 epoch, Validation Loss: 0.003989997319877148 \n",
            "At 777 epoch, Training Loss: 0.0007874748669564724 \n",
            "At 777 epoch, Validation Loss: 0.003805773099884391 \n",
            "At 778 epoch, Training Loss: 0.0009195688646286726 \n",
            "At 778 epoch, Validation Loss: 0.0037666140124201775 \n",
            "At 779 epoch, Training Loss: 0.00087014872697182 \n",
            "At 779 epoch, Validation Loss: 0.0037851708475500345 \n",
            "At 780 epoch, Training Loss: 0.000752200628630817 \n",
            "At 780 epoch, Validation Loss: 0.003812904702499509 \n",
            "At 781 epoch, Training Loss: 0.0008731854613870382 \n",
            "At 781 epoch, Validation Loss: 0.003786726389080286 \n",
            "At 782 epoch, Training Loss: 0.001020012970548123 \n",
            "At 782 epoch, Validation Loss: 0.0037286917213350534 \n",
            "At 783 epoch, Training Loss: 0.0008417350880336016 \n",
            "At 783 epoch, Validation Loss: 0.003821686143055558 \n",
            "At 784 epoch, Training Loss: 0.0010085374000482262 \n",
            "At 784 epoch, Validation Loss: 0.0039192084223032 \n",
            "At 785 epoch, Training Loss: 0.0008160060155205428 \n",
            "At 785 epoch, Validation Loss: 0.003815020201727748 \n",
            "At 786 epoch, Training Loss: 0.0009887077729217708 \n",
            "At 786 epoch, Validation Loss: 0.004121034871786833 \n",
            "At 787 epoch, Training Loss: 0.0008149063913151621 \n",
            "At 787 epoch, Validation Loss: 0.0036363129038363695 \n",
            "At 788 epoch, Training Loss: 0.0007913016248494387 \n",
            "At 788 epoch, Validation Loss: 0.003777841106057167 \n",
            "At 789 epoch, Training Loss: 0.0007888804422691465 \n",
            "At 789 epoch, Validation Loss: 0.0038435279857367277 \n",
            "At 790 epoch, Training Loss: 0.0007422290975227952 \n",
            "At 790 epoch, Validation Loss: 0.0036557482089847326 \n",
            "At 791 epoch, Training Loss: 0.0009672487736679614 \n",
            "At 791 epoch, Validation Loss: 0.0037108720280230045 \n",
            "At 792 epoch, Training Loss: 0.0007240006816573441 \n",
            "At 792 epoch, Validation Loss: 0.0038279867731034756 \n",
            "At 793 epoch, Training Loss: 0.000855169421993196 \n",
            "At 793 epoch, Validation Loss: 0.003684840863570571 \n",
            "At 794 epoch, Training Loss: 0.0010520793206524105 \n",
            "At 794 epoch, Validation Loss: 0.003769312985241413 \n",
            "At 795 epoch, Training Loss: 0.0008395694778300822 \n",
            "At 795 epoch, Validation Loss: 0.003788601141422987 \n",
            "At 796 epoch, Training Loss: 0.0009323223493993282 \n",
            "At 796 epoch, Validation Loss: 0.003604515455663204 \n",
            "At 797 epoch, Training Loss: 0.0008608229807578027 \n",
            "At 797 epoch, Validation Loss: 0.003830143716186285 \n",
            "At 798 epoch, Training Loss: 0.0007932276814244688 \n",
            "At 798 epoch, Validation Loss: 0.0037581489887088537 \n",
            "At 799 epoch, Training Loss: 0.0010173813323490321 \n",
            "At 799 epoch, Validation Loss: 0.003682882059365511 \n",
            "At 800 epoch, Training Loss: 0.0009830224211327732 \n",
            "At 800 epoch, Validation Loss: 0.0037092750426381826 \n",
            "At 801 epoch, Training Loss: 0.0008412114868406207 \n",
            "At 801 epoch, Validation Loss: 0.003712529083713889 \n",
            "At 802 epoch, Training Loss: 0.000774732930585742 \n",
            "At 802 epoch, Validation Loss: 0.0036358367651700974 \n",
            "At 803 epoch, Training Loss: 0.0009369204984977841 \n",
            "At 803 epoch, Validation Loss: 0.003748619696125388 \n",
            "At 804 epoch, Training Loss: 0.0006847394513897597 \n",
            "At 804 epoch, Validation Loss: 0.00383025873452425 \n",
            "At 805 epoch, Training Loss: 0.0007644988247193397 \n",
            "At 805 epoch, Validation Loss: 0.0037358917761594057 \n",
            "At 806 epoch, Training Loss: 0.0008474397938698531 \n",
            "At 806 epoch, Validation Loss: 0.003762310603633523 \n",
            "At 807 epoch, Training Loss: 0.0008129824185743928 \n",
            "At 807 epoch, Validation Loss: 0.003920788876712322 \n",
            "At 808 epoch, Training Loss: 0.0009109797538258135 \n",
            "At 808 epoch, Validation Loss: 0.003705524606630206 \n",
            "At 809 epoch, Training Loss: 0.0008484786376357079 \n",
            "At 809 epoch, Validation Loss: 0.0037413500249385834 \n",
            "At 810 epoch, Training Loss: 0.0006859561894088983 \n",
            "At 810 epoch, Validation Loss: 0.003707935567945242 \n",
            "At 811 epoch, Training Loss: 0.0010764719336293638 \n",
            "At 811 epoch, Validation Loss: 0.0036577952560037374 \n",
            "At 812 epoch, Training Loss: 0.000737958459649235 \n",
            "At 812 epoch, Validation Loss: 0.003612052882090211 \n",
            "At 813 epoch, Training Loss: 0.0009316143929027021 \n",
            "At 813 epoch, Validation Loss: 0.003700384870171547 \n",
            "At 814 epoch, Training Loss: 0.000782872922718525 \n",
            "At 814 epoch, Validation Loss: 0.003692416474223137 \n",
            "At 815 epoch, Training Loss: 0.0008730426314286887 \n",
            "At 815 epoch, Validation Loss: 0.0036249915137887 \n",
            "At 816 epoch, Training Loss: 0.0010173823684453964 \n",
            "At 816 epoch, Validation Loss: 0.0037430531810969114 \n",
            "At 817 epoch, Training Loss: 0.0009048919309861958 \n",
            "At 817 epoch, Validation Loss: 0.00377288437448442 \n",
            "At 818 epoch, Training Loss: 0.0007684805314056575 \n",
            "At 818 epoch, Validation Loss: 0.0036432421766221523 \n",
            "At 819 epoch, Training Loss: 0.0008343996596522629 \n",
            "At 819 epoch, Validation Loss: 0.003720317268744111 \n",
            "At 820 epoch, Training Loss: 0.0008655292971525341 \n",
            "At 820 epoch, Validation Loss: 0.003729155519977212 \n",
            "At 821 epoch, Training Loss: 0.0007555153919383883 \n",
            "At 821 epoch, Validation Loss: 0.003643554635345936 \n",
            "At 822 epoch, Training Loss: 0.000878007325809449 \n",
            "At 822 epoch, Validation Loss: 0.0038298829458653927 \n",
            "At 823 epoch, Training Loss: 0.0007628538063727319 \n",
            "At 823 epoch, Validation Loss: 0.003504713298752904 \n",
            "At 824 epoch, Training Loss: 0.0009341405704617501 \n",
            "At 824 epoch, Validation Loss: 0.003872096072882414 \n",
            "At 825 epoch, Training Loss: 0.000877240119734779 \n",
            "At 825 epoch, Validation Loss: 0.00378184812143445 \n",
            "At 826 epoch, Training Loss: 0.0008395368116907775 \n",
            "At 826 epoch, Validation Loss: 0.00355056906118989 \n",
            "At 827 epoch, Training Loss: 0.0007834985386580229 \n",
            "At 827 epoch, Validation Loss: 0.0038927546702325344 \n",
            "At 828 epoch, Training Loss: 0.0008203432313166559 \n",
            "At 828 epoch, Validation Loss: 0.0036235793959349394 \n",
            "At 829 epoch, Training Loss: 0.0009185008937492966 \n",
            "At 829 epoch, Validation Loss: 0.003746814327314496 \n",
            "At 830 epoch, Training Loss: 0.0007563639082945883 \n",
            "At 830 epoch, Validation Loss: 0.0034575394820421934 \n",
            "At 831 epoch, Training Loss: 0.0008038818836212158 \n",
            "At 831 epoch, Validation Loss: 0.0036361992824822664 \n",
            "At 832 epoch, Training Loss: 0.0009990036371164024 \n",
            "At 832 epoch, Validation Loss: 0.0036007710732519627 \n",
            "At 833 epoch, Training Loss: 0.000764874811284244 \n",
            "At 833 epoch, Validation Loss: 0.0036563274916261435 \n",
            "At 834 epoch, Training Loss: 0.0008965976478066296 \n",
            "At 834 epoch, Validation Loss: 0.003489365568384528 \n",
            "At 835 epoch, Training Loss: 0.0008670547627843917 \n",
            "At 835 epoch, Validation Loss: 0.0036948337219655514 \n",
            "At 836 epoch, Training Loss: 0.0007211823249235749 \n",
            "At 836 epoch, Validation Loss: 0.0036267158575356007 \n",
            "At 837 epoch, Training Loss: 0.0005777901911642403 \n",
            "At 837 epoch, Validation Loss: 0.003589242696762085 \n",
            "At 838 epoch, Training Loss: 0.0006119474419392645 \n",
            "At 838 epoch, Validation Loss: 0.0036654712166637182 \n",
            "At 839 epoch, Training Loss: 0.0005799768143333495 \n",
            "At 839 epoch, Validation Loss: 0.003531258087605238 \n",
            "At 840 epoch, Training Loss: 0.0010835890541784465 \n",
            "At 840 epoch, Validation Loss: 0.0035852196160703897 \n",
            "At 841 epoch, Training Loss: 0.0010305670672096311 \n",
            "At 841 epoch, Validation Loss: 0.0035631032660603523 \n",
            "At 842 epoch, Training Loss: 0.0008486134349368513 \n",
            "At 842 epoch, Validation Loss: 0.003685273230075836 \n",
            "At 843 epoch, Training Loss: 0.0007736700063105673 \n",
            "At 843 epoch, Validation Loss: 0.0036934867966920137 \n",
            "At 844 epoch, Training Loss: 0.0006783552351407706 \n",
            "At 844 epoch, Validation Loss: 0.0035430502612143755 \n",
            "At 845 epoch, Training Loss: 0.00084338792366907 \n",
            "At 845 epoch, Validation Loss: 0.0036142028402537107 \n",
            "At 846 epoch, Training Loss: 0.0008354684920050204 \n",
            "At 846 epoch, Validation Loss: 0.0035535593051463366 \n",
            "At 847 epoch, Training Loss: 0.0006833604886196554 \n",
            "At 847 epoch, Validation Loss: 0.0036688284017145634 \n",
            "At 848 epoch, Training Loss: 0.0007796091260388494 \n",
            "At 848 epoch, Validation Loss: 0.003628568258136511 \n",
            "At 849 epoch, Training Loss: 0.0008310768404044211 \n",
            "At 849 epoch, Validation Loss: 0.00356476241722703 \n",
            "At 850 epoch, Training Loss: 0.0006556172738783062 \n",
            "At 850 epoch, Validation Loss: 0.0036948041524738073 \n",
            "At 851 epoch, Training Loss: 0.0007661481038667261 \n",
            "At 851 epoch, Validation Loss: 0.003656208049505949 \n",
            "At 852 epoch, Training Loss: 0.0006513492553494871 \n",
            "At 852 epoch, Validation Loss: 0.0036193591076880693 \n",
            "At 853 epoch, Training Loss: 0.0007801388739608229 \n",
            "At 853 epoch, Validation Loss: 0.0035211779177188873 \n",
            "At 854 epoch, Training Loss: 0.0009358157753013074 \n",
            "At 854 epoch, Validation Loss: 0.0035964816343039274 \n",
            "At 855 epoch, Training Loss: 0.0007195434358436614 \n",
            "At 855 epoch, Validation Loss: 0.0037069653626531363 \n",
            "At 856 epoch, Training Loss: 0.0006366484158206731 \n",
            "At 856 epoch, Validation Loss: 0.0035074891056865454 \n",
            "At 857 epoch, Training Loss: 0.0007399972062557935 \n",
            "At 857 epoch, Validation Loss: 0.003648769576102495 \n",
            "At 858 epoch, Training Loss: 0.0008688413887284696 \n",
            "At 858 epoch, Validation Loss: 0.003668172052130103 \n",
            "At 859 epoch, Training Loss: 0.0007534372853115201 \n",
            "At 859 epoch, Validation Loss: 0.003521256148815155 \n",
            "At 860 epoch, Training Loss: 0.0007649354520253837 \n",
            "At 860 epoch, Validation Loss: 0.003523272229358554 \n",
            "At 861 epoch, Training Loss: 0.0007995259831659496 \n",
            "At 861 epoch, Validation Loss: 0.0036145870108157396 \n",
            "At 862 epoch, Training Loss: 0.0006742942787241191 \n",
            "At 862 epoch, Validation Loss: 0.0037017138674855232 \n",
            "At 863 epoch, Training Loss: 0.0007532286050263792 \n",
            "At 863 epoch, Validation Loss: 0.003487596521154046 \n",
            "At 864 epoch, Training Loss: 0.000752131303306669 \n",
            "At 864 epoch, Validation Loss: 0.0035312885884195566 \n",
            "At 865 epoch, Training Loss: 0.0009113980107940733 \n",
            "At 865 epoch, Validation Loss: 0.0035336946602910757 \n",
            "At 866 epoch, Training Loss: 0.0007894059701357037 \n",
            "At 866 epoch, Validation Loss: 0.003569897497072816 \n",
            "At 867 epoch, Training Loss: 0.0008307930605951696 \n",
            "At 867 epoch, Validation Loss: 0.0035450542345643044 \n",
            "At 868 epoch, Training Loss: 0.0007095070031937212 \n",
            "At 868 epoch, Validation Loss: 0.003593444125726819 \n",
            "At 869 epoch, Training Loss: 0.0007637545291800052 \n",
            "At 869 epoch, Validation Loss: 0.003513277042657137 \n",
            "At 870 epoch, Training Loss: 0.0006851340585853904 \n",
            "At 870 epoch, Validation Loss: 0.003590851556509733 \n",
            "At 871 epoch, Training Loss: 0.0007368050282821059 \n",
            "At 871 epoch, Validation Loss: 0.0035201336722820997 \n",
            "At 872 epoch, Training Loss: 0.0007144449453335255 \n",
            "At 872 epoch, Validation Loss: 0.00353485019877553 \n",
            "At 873 epoch, Training Loss: 0.0006549394107423723 \n",
            "At 873 epoch, Validation Loss: 0.003790186485275626 \n",
            "At 874 epoch, Training Loss: 0.000993085268419236 \n",
            "At 874 epoch, Validation Loss: 0.0035077573265880346 \n",
            "At 875 epoch, Training Loss: 0.000630858913064003 \n",
            "At 875 epoch, Validation Loss: 0.0035109997261315584 \n",
            "At 876 epoch, Training Loss: 0.0007466784154530615 \n",
            "At 876 epoch, Validation Loss: 0.003495204960927367 \n",
            "At 877 epoch, Training Loss: 0.0007068484555929899 \n",
            "At 877 epoch, Validation Loss: 0.0036698253825306892 \n",
            "At 878 epoch, Training Loss: 0.0005196945508942008 \n",
            "At 878 epoch, Validation Loss: 0.0035886403638869524 \n",
            "At 879 epoch, Training Loss: 0.0007360185554716736 \n",
            "At 879 epoch, Validation Loss: 0.0035091829486191273 \n",
            "At 880 epoch, Training Loss: 0.000685910601168871 \n",
            "At 880 epoch, Validation Loss: 0.0037175684701651335 \n",
            "At 881 epoch, Training Loss: 0.0007188197690993547 \n",
            "At 881 epoch, Validation Loss: 0.0035710767842829227 \n",
            "At 882 epoch, Training Loss: 0.0007378104375675321 \n",
            "At 882 epoch, Validation Loss: 0.0036614604759961367 \n",
            "At 883 epoch, Training Loss: 0.000772695045452565 \n",
            "At 883 epoch, Validation Loss: 0.003472335869446397 \n",
            "At 884 epoch, Training Loss: 0.0008836369030177593 \n",
            "At 884 epoch, Validation Loss: 0.003728008596226573 \n",
            "At 885 epoch, Training Loss: 0.0007732016150839627 \n",
            "At 885 epoch, Validation Loss: 0.0034077856689691544 \n",
            "At 886 epoch, Training Loss: 0.0007543567975517362 \n",
            "At 886 epoch, Validation Loss: 0.0036252252757549286 \n",
            "At 887 epoch, Training Loss: 0.0007772839511744678 \n",
            "At 887 epoch, Validation Loss: 0.0037314530927687883 \n",
            "At 888 epoch, Training Loss: 0.0008412482799030841 \n",
            "At 888 epoch, Validation Loss: 0.00371266296133399 \n",
            "At 889 epoch, Training Loss: 0.0007577017066068947 \n",
            "At 889 epoch, Validation Loss: 0.0034100692719221115 \n",
            "At 890 epoch, Training Loss: 0.0007977667497470975 \n",
            "At 890 epoch, Validation Loss: 0.0035640448331832886 \n",
            "At 891 epoch, Training Loss: 0.0007636988535523415 \n",
            "At 891 epoch, Validation Loss: 0.003669068217277527 \n",
            "At 892 epoch, Training Loss: 0.000646533106919378 \n",
            "At 892 epoch, Validation Loss: 0.003491349518299103 \n",
            "At 893 epoch, Training Loss: 0.0008142999955452979 \n",
            "At 893 epoch, Validation Loss: 0.003436305094510317 \n",
            "At 894 epoch, Training Loss: 0.0006419010809622705 \n",
            "At 894 epoch, Validation Loss: 0.003664567368105054 \n",
            "At 895 epoch, Training Loss: 0.000787908147322014 \n",
            "At 895 epoch, Validation Loss: 0.003417388303205371 \n",
            "At 896 epoch, Training Loss: 0.0008322362555190921 \n",
            "At 896 epoch, Validation Loss: 0.003651184495538473 \n",
            "At 897 epoch, Training Loss: 0.0007128820987418294 \n",
            "At 897 epoch, Validation Loss: 0.0036050179041922092 \n",
            "At 898 epoch, Training Loss: 0.0006587554409634322 \n",
            "At 898 epoch, Validation Loss: 0.0035080634988844395 \n",
            "At 899 epoch, Training Loss: 0.0008016465348191559 \n",
            "At 899 epoch, Validation Loss: 0.003434952115640044 \n",
            "At 900 epoch, Training Loss: 0.0006206136022228748 \n",
            "At 900 epoch, Validation Loss: 0.00353766861371696 \n",
            "At 901 epoch, Training Loss: 0.000689408095786348 \n",
            "At 901 epoch, Validation Loss: 0.0035501981619745493 \n",
            "At 902 epoch, Training Loss: 0.0006530620623379946 \n",
            "At 902 epoch, Validation Loss: 0.003454400459304452 \n",
            "At 903 epoch, Training Loss: 0.000662092468701303 \n",
            "At 903 epoch, Validation Loss: 0.003511602059006691 \n",
            "At 904 epoch, Training Loss: 0.0008101966581307351 \n",
            "At 904 epoch, Validation Loss: 0.0034847776405513287 \n",
            "At 905 epoch, Training Loss: 0.0006781688542105257 \n",
            "At 905 epoch, Validation Loss: 0.003482487518340349 \n",
            "At 906 epoch, Training Loss: 0.0007888499647378921 \n",
            "At 906 epoch, Validation Loss: 0.003408456454053521 \n",
            "At 907 epoch, Training Loss: 0.0004436540009919554 \n",
            "At 907 epoch, Validation Loss: 0.003578901756554842 \n",
            "At 908 epoch, Training Loss: 0.0006710588466376066 \n",
            "At 908 epoch, Validation Loss: 0.0035472612362354994 \n",
            "At 909 epoch, Training Loss: 0.000519051868468523 \n",
            "At 909 epoch, Validation Loss: 0.0035096912179142237 \n",
            "At 910 epoch, Training Loss: 0.0006887488707434386 \n",
            "At 910 epoch, Validation Loss: 0.0036308998242020607 \n",
            "At 911 epoch, Training Loss: 0.0007567074848338961 \n",
            "At 911 epoch, Validation Loss: 0.0034575164318084717 \n",
            "At 912 epoch, Training Loss: 0.0005929406499490142 \n",
            "At 912 epoch, Validation Loss: 0.00338591355830431 \n",
            "At 913 epoch, Training Loss: 0.0006813590705860406 \n",
            "At 913 epoch, Validation Loss: 0.0035096516367048025 \n",
            "At 914 epoch, Training Loss: 0.0007378371898084879 \n",
            "At 914 epoch, Validation Loss: 0.0033803165424615145 \n",
            "At 915 epoch, Training Loss: 0.0006371422787196934 \n",
            "At 915 epoch, Validation Loss: 0.003626609221100807 \n",
            "At 916 epoch, Training Loss: 0.0005830325011629611 \n",
            "At 916 epoch, Validation Loss: 0.003369959071278572 \n",
            "At 917 epoch, Training Loss: 0.0007082557829562574 \n",
            "At 917 epoch, Validation Loss: 0.003542910795658827 \n",
            "At 918 epoch, Training Loss: 0.0007548196008428931 \n",
            "At 918 epoch, Validation Loss: 0.003595109097659588 \n",
            "At 919 epoch, Training Loss: 0.0007484040572308004 \n",
            "At 919 epoch, Validation Loss: 0.003357843728736043 \n",
            "At 920 epoch, Training Loss: 0.0006376791512593627 \n",
            "At 920 epoch, Validation Loss: 0.003567214822396636 \n",
            "At 921 epoch, Training Loss: 0.0007029938860796392 \n",
            "At 921 epoch, Validation Loss: 0.0035022832453250885 \n",
            "At 922 epoch, Training Loss: 0.0006001404311973601 \n",
            "At 922 epoch, Validation Loss: 0.0034000363666564226 \n",
            "At 923 epoch, Training Loss: 0.0008341515669599176 \n",
            "At 923 epoch, Validation Loss: 0.0035399601329118013 \n",
            "At 924 epoch, Training Loss: 0.0007356693502515554 \n",
            "At 924 epoch, Validation Loss: 0.0035647244658321142 \n",
            "At 925 epoch, Training Loss: 0.0007089713704772294 \n",
            "At 925 epoch, Validation Loss: 0.0034989945124834776 \n",
            "At 926 epoch, Training Loss: 0.0005413210077676922 \n",
            "At 926 epoch, Validation Loss: 0.003450586926192045 \n",
            "At 927 epoch, Training Loss: 0.0007037482282612473 \n",
            "At 927 epoch, Validation Loss: 0.003472521435469389 \n",
            "At 928 epoch, Training Loss: 0.0007089111488312483 \n",
            "At 928 epoch, Validation Loss: 0.0034690978936851025 \n",
            "At 929 epoch, Training Loss: 0.0006550692021846771 \n",
            "At 929 epoch, Validation Loss: 0.0034337814431637526 \n",
            "At 930 epoch, Training Loss: 0.0008758708252571523 \n",
            "At 930 epoch, Validation Loss: 0.0035719892475754023 \n",
            "At 931 epoch, Training Loss: 0.0006810743012465537 \n",
            "At 931 epoch, Validation Loss: 0.0034237599465996027 \n",
            "At 932 epoch, Training Loss: 0.0006756793009117246 \n",
            "At 932 epoch, Validation Loss: 0.003442256012931466 \n",
            "At 933 epoch, Training Loss: 0.0006234161381144076 \n",
            "At 933 epoch, Validation Loss: 0.003527869237586856 \n",
            "At 934 epoch, Training Loss: 0.0007458786480128765 \n",
            "At 934 epoch, Validation Loss: 0.003368757665157318 \n",
            "At 935 epoch, Training Loss: 0.0005567972257267684 \n",
            "At 935 epoch, Validation Loss: 0.0034705197904258966 \n",
            "At 936 epoch, Training Loss: 0.0005792660173028708 \n",
            "At 936 epoch, Validation Loss: 0.0035030748695135117 \n",
            "At 937 epoch, Training Loss: 0.0005905786529183388 \n",
            "At 937 epoch, Validation Loss: 0.003478550584986806 \n",
            "At 938 epoch, Training Loss: 0.0005426494695711881 \n",
            "At 938 epoch, Validation Loss: 0.00346084451302886 \n",
            "At 939 epoch, Training Loss: 0.0006928648392204196 \n",
            "At 939 epoch, Validation Loss: 0.003392190206795931 \n",
            "At 940 epoch, Training Loss: 0.0005778403952717781 \n",
            "At 940 epoch, Validation Loss: 0.0036498643457889557 \n",
            "At 941 epoch, Training Loss: 0.0007102847856003791 \n",
            "At 941 epoch, Validation Loss: 0.00358110130764544 \n",
            "At 942 epoch, Training Loss: 0.000733052846044302 \n",
            "At 942 epoch, Validation Loss: 0.003385425778105855 \n",
            "At 943 epoch, Training Loss: 0.0006417256256099791 \n",
            "At 943 epoch, Validation Loss: 0.003573325928300619 \n",
            "At 944 epoch, Training Loss: 0.0007125053554773331 \n",
            "At 944 epoch, Validation Loss: 0.003491820301860571 \n",
            "At 945 epoch, Training Loss: 0.000599452352616936 \n",
            "At 945 epoch, Validation Loss: 0.0033693111035972834 \n",
            "At 946 epoch, Training Loss: 0.0006725706451106816 \n",
            "At 946 epoch, Validation Loss: 0.003408207092434168 \n",
            "At 947 epoch, Training Loss: 0.0005245915148407221 \n",
            "At 947 epoch, Validation Loss: 0.003405815688893199 \n",
            "At 948 epoch, Training Loss: 0.0005493248405400664 \n",
            "At 948 epoch, Validation Loss: 0.003456519916653633 \n",
            "At 949 epoch, Training Loss: 0.0007062497723381966 \n",
            "At 949 epoch, Validation Loss: 0.003490380709990859 \n",
            "At 950 epoch, Training Loss: 0.0005020765122026205 \n",
            "At 950 epoch, Validation Loss: 0.0033932137303054333 \n",
            "At 951 epoch, Training Loss: 0.0005806830362416804 \n",
            "At 951 epoch, Validation Loss: 0.0034560023341327906 \n",
            "At 952 epoch, Training Loss: 0.0006247846060432494 \n",
            "At 952 epoch, Validation Loss: 0.0035174789372831583 \n",
            "At 953 epoch, Training Loss: 0.0006816037755925208 \n",
            "At 953 epoch, Validation Loss: 0.0034971185959875584 \n",
            "At 954 epoch, Training Loss: 0.000596639170544222 \n",
            "At 954 epoch, Validation Loss: 0.00350795965641737 \n",
            "At 955 epoch, Training Loss: 0.0006647909991443157 \n",
            "At 955 epoch, Validation Loss: 0.003651972161605954 \n",
            "At 956 epoch, Training Loss: 0.0007029830012470484 \n",
            "At 956 epoch, Validation Loss: 0.0035091200843453407 \n",
            "At 957 epoch, Training Loss: 0.0006140959332697094 \n",
            "At 957 epoch, Validation Loss: 0.003413354279473424 \n",
            "At 958 epoch, Training Loss: 0.0005510221177246422 \n",
            "At 958 epoch, Validation Loss: 0.0034148506820201874 \n",
            "At 959 epoch, Training Loss: 0.0006727551342919468 \n",
            "At 959 epoch, Validation Loss: 0.0033582262694835663 \n",
            "At 960 epoch, Training Loss: 0.0005538889206945896 \n",
            "At 960 epoch, Validation Loss: 0.0035186323802918196 \n",
            "At 961 epoch, Training Loss: 0.0007294830633327365 \n",
            "At 961 epoch, Validation Loss: 0.003439831780269742 \n",
            "At 962 epoch, Training Loss: 0.000648213765816763 \n",
            "At 962 epoch, Validation Loss: 0.003370080143213272 \n",
            "At 963 epoch, Training Loss: 0.0005570605280809104 \n",
            "At 963 epoch, Validation Loss: 0.0035678273998200893 \n",
            "At 964 epoch, Training Loss: 0.0006626565009355545 \n",
            "At 964 epoch, Validation Loss: 0.003447967115789652 \n",
            "At 965 epoch, Training Loss: 0.0007407811877783388 \n",
            "At 965 epoch, Validation Loss: 0.0032722984906286 \n",
            "At 966 epoch, Training Loss: 0.0006697792909108102 \n",
            "At 966 epoch, Validation Loss: 0.003377409651875496 \n",
            "At 967 epoch, Training Loss: 0.0005600705975666642 \n",
            "At 967 epoch, Validation Loss: 0.0033940933644771576 \n",
            "At 968 epoch, Training Loss: 0.000625826750183478 \n",
            "At 968 epoch, Validation Loss: 0.003326605772599578 \n",
            "At 969 epoch, Training Loss: 0.0007053133216686547 \n",
            "At 969 epoch, Validation Loss: 0.0033412559423595667 \n",
            "At 970 epoch, Training Loss: 0.0006915344973094761 \n",
            "At 970 epoch, Validation Loss: 0.0033929443452507257 \n",
            "At 971 epoch, Training Loss: 0.0006148116197437048 \n",
            "At 971 epoch, Validation Loss: 0.0033487239852547646 \n",
            "At 972 epoch, Training Loss: 0.0005913045606575906 \n",
            "At 972 epoch, Validation Loss: 0.003433921840041876 \n",
            "At 973 epoch, Training Loss: 0.0005733881203923375 \n",
            "At 973 epoch, Validation Loss: 0.003327398793771863 \n",
            "At 974 epoch, Training Loss: 0.0005898485076613724 \n",
            "At 974 epoch, Validation Loss: 0.0034006822388619184 \n",
            "At 975 epoch, Training Loss: 0.0005045815370976925 \n",
            "At 975 epoch, Validation Loss: 0.0036364055704325438 \n",
            "At 976 epoch, Training Loss: 0.0006252147606573999 \n",
            "At 976 epoch, Validation Loss: 0.0034581790678203106 \n",
            "At 977 epoch, Training Loss: 0.0007125169970095158 \n",
            "At 977 epoch, Validation Loss: 0.0033904830925166607 \n",
            "At 978 epoch, Training Loss: 0.0006842646311270073 \n",
            "At 978 epoch, Validation Loss: 0.003501804545521736 \n",
            "At 979 epoch, Training Loss: 0.000566948828054592 \n",
            "At 979 epoch, Validation Loss: 0.0034594284370541573 \n",
            "At 980 epoch, Training Loss: 0.0005973349849227816 \n",
            "At 980 epoch, Validation Loss: 0.0034278121311217546 \n",
            "At 981 epoch, Training Loss: 0.0006480321811977774 \n",
            "At 981 epoch, Validation Loss: 0.003411957761272788 \n",
            "At 982 epoch, Training Loss: 0.0005997959640808403 \n",
            "At 982 epoch, Validation Loss: 0.003323876764625311 \n",
            "At 983 epoch, Training Loss: 0.0006985995220020414 \n",
            "At 983 epoch, Validation Loss: 0.0033217528834939003 \n",
            "At 984 epoch, Training Loss: 0.0006739605858456343 \n",
            "At 984 epoch, Validation Loss: 0.0033625178039073944 \n",
            "At 985 epoch, Training Loss: 0.000562592496862635 \n",
            "At 985 epoch, Validation Loss: 0.003313274821266532 \n",
            "At 986 epoch, Training Loss: 0.0005547514127101749 \n",
            "At 986 epoch, Validation Loss: 0.003341063391417265 \n",
            "At 987 epoch, Training Loss: 0.0006474931375123561 \n",
            "At 987 epoch, Validation Loss: 0.0034262530971318483 \n",
            "At 988 epoch, Training Loss: 0.0005684839503373951 \n",
            "At 988 epoch, Validation Loss: 0.0034859376028180122 \n",
            "At 989 epoch, Training Loss: 0.0005949376849457621 \n",
            "At 989 epoch, Validation Loss: 0.0033653764985501766 \n",
            "At 990 epoch, Training Loss: 0.0006128416571300477 \n",
            "At 990 epoch, Validation Loss: 0.003322320058941841 \n",
            "At 991 epoch, Training Loss: 0.0004399488912895322 \n",
            "At 991 epoch, Validation Loss: 0.003346771467477083 \n",
            "At 992 epoch, Training Loss: 0.0007856550044380128 \n",
            "At 992 epoch, Validation Loss: 0.0033506217878311872 \n",
            "At 993 epoch, Training Loss: 0.0007738608401268721 \n",
            "At 993 epoch, Validation Loss: 0.003345446428284049 \n",
            "At 994 epoch, Training Loss: 0.0005768617673311382 \n",
            "At 994 epoch, Validation Loss: 0.0033209016546607018 \n",
            "At 995 epoch, Training Loss: 0.0005005777988117188 \n",
            "At 995 epoch, Validation Loss: 0.0034423256292939186 \n",
            "At 996 epoch, Training Loss: 0.0006599739484954625 \n",
            "At 996 epoch, Validation Loss: 0.003376547247171402 \n",
            "At 997 epoch, Training Loss: 0.0005294482572935522 \n",
            "At 997 epoch, Validation Loss: 0.0033278604969382286 \n",
            "At 998 epoch, Training Loss: 0.0005365428747609258 \n",
            "At 998 epoch, Validation Loss: 0.003359121037647128 \n",
            "At 999 epoch, Training Loss: 0.0004940940591040999 \n",
            "At 999 epoch, Validation Loss: 0.003295314032584429 \n",
            "At 1000 epoch, Training Loss: 0.0005785009358078242 \n",
            "At 1000 epoch, Validation Loss: 0.0033652056008577347 \n",
            "At 1001 epoch, Training Loss: 0.0007184052024967969 \n",
            "At 1001 epoch, Validation Loss: 0.003493671305477619 \n",
            "At 1002 epoch, Training Loss: 0.0006243514537345618 \n",
            "At 1002 epoch, Validation Loss: 0.00344129279255867 \n",
            "At 1003 epoch, Training Loss: 0.0006445313396397978 \n",
            "At 1003 epoch, Validation Loss: 0.0033079576678574085 \n",
            "At 1004 epoch, Training Loss: 0.0007050828018691391 \n",
            "At 1004 epoch, Validation Loss: 0.003431740216910839 \n",
            "At 1005 epoch, Training Loss: 0.0005329056410118937 \n",
            "At 1005 epoch, Validation Loss: 0.00341340946033597 \n",
            "At 1006 epoch, Training Loss: 0.0004721402539871633 \n",
            "At 1006 epoch, Validation Loss: 0.00335626769810915 \n",
            "At 1007 epoch, Training Loss: 0.0006650211405940353 \n",
            "At 1007 epoch, Validation Loss: 0.0034038815647363663 \n",
            "At 1008 epoch, Training Loss: 0.00035623703151941297 \n",
            "At 1008 epoch, Validation Loss: 0.003293268382549286 \n",
            "At 1009 epoch, Training Loss: 0.0004780802177265286 \n",
            "At 1009 epoch, Validation Loss: 0.003381458343937993 \n",
            "At 1010 epoch, Training Loss: 0.0005052686552517116 \n",
            "At 1010 epoch, Validation Loss: 0.0033208143431693316 \n",
            "At 1011 epoch, Training Loss: 0.0004383522900752723 \n",
            "At 1011 epoch, Validation Loss: 0.003339412622153759 \n",
            "At 1012 epoch, Training Loss: 0.000491078954655677 \n",
            "At 1012 epoch, Validation Loss: 0.003415114711970091 \n",
            "At 1013 epoch, Training Loss: 0.00057810980360955 \n",
            "At 1013 epoch, Validation Loss: 0.003356749890372157 \n",
            "At 1014 epoch, Training Loss: 0.0004929217335302383 \n",
            "At 1014 epoch, Validation Loss: 0.0032990348991006613 \n",
            "At 1015 epoch, Training Loss: 0.0006079601822420955 \n",
            "At 1015 epoch, Validation Loss: 0.003464689012616873 \n",
            "At 1016 epoch, Training Loss: 0.0005464300571475178 \n",
            "At 1016 epoch, Validation Loss: 0.003464498557150364 \n",
            "At 1017 epoch, Training Loss: 0.0005248395318631083 \n",
            "At 1017 epoch, Validation Loss: 0.003363226540386677 \n",
            "At 1018 epoch, Training Loss: 0.0006438819866161794 \n",
            "At 1018 epoch, Validation Loss: 0.0033610144164413214 \n",
            "At 1019 epoch, Training Loss: 0.0006454793619923294 \n",
            "At 1019 epoch, Validation Loss: 0.003351690713316202 \n",
            "At 1020 epoch, Training Loss: 0.0005743346526287496 \n",
            "At 1020 epoch, Validation Loss: 0.0034510097466409206 \n",
            "At 1021 epoch, Training Loss: 0.0005123898561578244 \n",
            "At 1021 epoch, Validation Loss: 0.0034165706019848585 \n",
            "At 1022 epoch, Training Loss: 0.0005260192730929703 \n",
            "At 1022 epoch, Validation Loss: 0.00330396369099617 \n",
            "At 1023 epoch, Training Loss: 0.00047992782783694564 \n",
            "At 1023 epoch, Validation Loss: 0.003412393154576421 \n",
            "At 1024 epoch, Training Loss: 0.00041937369387596847 \n",
            "At 1024 epoch, Validation Loss: 0.003474053693935275 \n",
            "At 1025 epoch, Training Loss: 0.0005459085572510958 \n",
            "At 1025 epoch, Validation Loss: 0.0034221308305859566 \n",
            "At 1026 epoch, Training Loss: 0.0005864445876795799 \n",
            "At 1026 epoch, Validation Loss: 0.0033095560502260923 \n",
            "At 1027 epoch, Training Loss: 0.0005932923988439143 \n",
            "At 1027 epoch, Validation Loss: 0.0034075959119945765 \n",
            "At 1028 epoch, Training Loss: 0.0004905560926999897 \n",
            "At 1028 epoch, Validation Loss: 0.0034802162554115057 \n",
            "At 1029 epoch, Training Loss: 0.0005985854775644838 \n",
            "At 1029 epoch, Validation Loss: 0.0034384254831820726 \n",
            "At 1030 epoch, Training Loss: 0.0005438545951619745 \n",
            "At 1030 epoch, Validation Loss: 0.0033589068334549665 \n",
            "At 1031 epoch, Training Loss: 0.000498840888030827 \n",
            "At 1031 epoch, Validation Loss: 0.003330603241920471 \n",
            "At 1032 epoch, Training Loss: 0.0004956203163601458 \n",
            "At 1032 epoch, Validation Loss: 0.0033044491428881884 \n",
            "At 1033 epoch, Training Loss: 0.0006217128073330968 \n",
            "At 1033 epoch, Validation Loss: 0.0033393779303878546 \n",
            "At 1034 epoch, Training Loss: 0.0003619894094299525 \n",
            "At 1034 epoch, Validation Loss: 0.0033956996630877256 \n",
            "At 1035 epoch, Training Loss: 0.0003975429048296064 \n",
            "At 1035 epoch, Validation Loss: 0.003292023902758956 \n",
            "At 1036 epoch, Training Loss: 0.0006615175458136946 \n",
            "At 1036 epoch, Validation Loss: 0.003396314335986972 \n",
            "At 1037 epoch, Training Loss: 0.0004738221410661936 \n",
            "At 1037 epoch, Validation Loss: 0.0034331611823290586 \n",
            "At 1038 epoch, Training Loss: 0.00044984701089560984 \n",
            "At 1038 epoch, Validation Loss: 0.003453301964327693 \n",
            "At 1039 epoch, Training Loss: 0.000766534311696887 \n",
            "At 1039 epoch, Validation Loss: 0.003365463810041547 \n",
            "At 1040 epoch, Training Loss: 0.0004410134337376803 \n",
            "At 1040 epoch, Validation Loss: 0.0034255210775882006 \n",
            "At 1041 epoch, Training Loss: 0.0005255269323242828 \n",
            "At 1041 epoch, Validation Loss: 0.0034000761806964874 \n",
            "At 1042 epoch, Training Loss: 0.0005284135695546865 \n",
            "At 1042 epoch, Validation Loss: 0.003389757825061679 \n",
            "At 1043 epoch, Training Loss: 0.0005380970891565084 \n",
            "At 1043 epoch, Validation Loss: 0.0033268143888562918 \n",
            "At 1044 epoch, Training Loss: 0.0006735605071298778 \n",
            "At 1044 epoch, Validation Loss: 0.0033804504200816154 \n",
            "At 1045 epoch, Training Loss: 0.0005898029252421111 \n",
            "At 1045 epoch, Validation Loss: 0.0033613834530115128 \n",
            "At 1046 epoch, Training Loss: 0.0005527829634957016 \n",
            "At 1046 epoch, Validation Loss: 0.0033914910163730383 \n",
            "At 1047 epoch, Training Loss: 0.0003995883744210005 \n",
            "At 1047 epoch, Validation Loss: 0.003461710410192609 \n",
            "At 1048 epoch, Training Loss: 0.0005622743745334446 \n",
            "At 1048 epoch, Validation Loss: 0.0033512194640934467 \n",
            "At 1049 epoch, Training Loss: 0.00045620641321875155 \n",
            "At 1049 epoch, Validation Loss: 0.003376894863322377 \n",
            "At 1050 epoch, Training Loss: 0.000617662095464766 \n",
            "At 1050 epoch, Validation Loss: 0.0033568639773875475 \n",
            "At 1051 epoch, Training Loss: 0.00045193523983471094 \n",
            "At 1051 epoch, Validation Loss: 0.0034235587809234858 \n",
            "At 1052 epoch, Training Loss: 0.0005135461222380399 \n",
            "At 1052 epoch, Validation Loss: 0.00347141339443624 \n",
            "At 1053 epoch, Training Loss: 0.0003590395499486476 \n",
            "At 1053 epoch, Validation Loss: 0.0033667206298559904 \n",
            "At 1054 epoch, Training Loss: 0.00046116624725982546 \n",
            "At 1054 epoch, Validation Loss: 0.003369669895619154 \n",
            "At 1055 epoch, Training Loss: 0.00045048119500279427 \n",
            "At 1055 epoch, Validation Loss: 0.003327298676595092 \n",
            "At 1056 epoch, Training Loss: 0.0004194629436824471 \n",
            "At 1056 epoch, Validation Loss: 0.003254433162510395 \n",
            "At 1057 epoch, Training Loss: 0.00047402952041011306 \n",
            "At 1057 epoch, Validation Loss: 0.003419863758608699 \n",
            "At 1058 epoch, Training Loss: 0.0005209589668083936 \n",
            "At 1058 epoch, Validation Loss: 0.0034200067166239023 \n",
            "At 1059 epoch, Training Loss: 0.0005015848379116506 \n",
            "At 1059 epoch, Validation Loss: 0.0032980425748974085 \n",
            "At 1060 epoch, Training Loss: 0.00044519437942653896 \n",
            "At 1060 epoch, Validation Loss: 0.003316227113828063 \n",
            "At 1061 epoch, Training Loss: 0.00047571519971825185 \n",
            "At 1061 epoch, Validation Loss: 0.0033182420302182436 \n",
            "At 1062 epoch, Training Loss: 0.00048741213395260275 \n",
            "At 1062 epoch, Validation Loss: 0.0032893300522118807 \n",
            "At 1063 epoch, Training Loss: 0.00046625895774923267 \n",
            "At 1063 epoch, Validation Loss: 0.0034057723823934793 \n",
            "At 1064 epoch, Training Loss: 0.0006182617624290287 \n",
            "At 1064 epoch, Validation Loss: 0.0033748438581824303 \n",
            "At 1065 epoch, Training Loss: 0.0005494817451108247 \n",
            "At 1065 epoch, Validation Loss: 0.003371705999597907 \n",
            "At 1066 epoch, Training Loss: 0.000600569334346801 \n",
            "At 1066 epoch, Validation Loss: 0.0032944646663963795 \n",
            "At 1067 epoch, Training Loss: 0.00038168010069057346 \n",
            "At 1067 epoch, Validation Loss: 0.0032821858767420053 \n",
            "At 1068 epoch, Training Loss: 0.0006106875604018569 \n",
            "At 1068 epoch, Validation Loss: 0.003244843101128936 \n",
            "At 1069 epoch, Training Loss: 0.00040527259989175946 \n",
            "At 1069 epoch, Validation Loss: 0.003330011386424303 \n",
            "At 1070 epoch, Training Loss: 0.0005751972901634872 \n",
            "At 1070 epoch, Validation Loss: 0.003391494508832693 \n",
            "At 1071 epoch, Training Loss: 0.0006384235864970833 \n",
            "At 1071 epoch, Validation Loss: 0.003279251977801323 \n",
            "At 1072 epoch, Training Loss: 0.0004117846925510094 \n",
            "At 1072 epoch, Validation Loss: 0.003287519095465541 \n",
            "At 1073 epoch, Training Loss: 0.0006138408556580543 \n",
            "At 1073 epoch, Validation Loss: 0.003385079326108098 \n",
            "At 1074 epoch, Training Loss: 0.00045895053190179167 \n",
            "At 1074 epoch, Validation Loss: 0.003319024108350277 \n",
            "At 1075 epoch, Training Loss: 0.0003480299812508747 \n",
            "At 1075 epoch, Validation Loss: 0.0034011914394795895 \n",
            "At 1076 epoch, Training Loss: 0.0006906778959091753 \n",
            "At 1076 epoch, Validation Loss: 0.0034298051614314318 \n",
            "At 1077 epoch, Training Loss: 0.0004084287560544908 \n",
            "At 1077 epoch, Validation Loss: 0.003277214476838708 \n",
            "At 1078 epoch, Training Loss: 0.0004581903980579227 \n",
            "At 1078 epoch, Validation Loss: 0.0032792778220027685 \n",
            "At 1079 epoch, Training Loss: 0.00046314456267282367 \n",
            "At 1079 epoch, Validation Loss: 0.0033338142093271017 \n",
            "At 1080 epoch, Training Loss: 0.00038674767129123213 \n",
            "At 1080 epoch, Validation Loss: 0.003405618481338024 \n",
            "At 1081 epoch, Training Loss: 0.0005155720748007298 \n",
            "At 1081 epoch, Validation Loss: 0.003474426455795765 \n",
            "At 1082 epoch, Training Loss: 0.0005666747514624148 \n",
            "At 1082 epoch, Validation Loss: 0.0033850260078907013 \n",
            "At 1083 epoch, Training Loss: 0.00040066043147817253 \n",
            "At 1083 epoch, Validation Loss: 0.003320511896163225 \n",
            "At 1084 epoch, Training Loss: 0.00042794315377250316 \n",
            "At 1084 epoch, Validation Loss: 0.0033605783246457577 \n",
            "At 1085 epoch, Training Loss: 0.0004498631751630455 \n",
            "At 1085 epoch, Validation Loss: 0.0033772061578929424 \n",
            "At 1086 epoch, Training Loss: 0.00042007726151496174 \n",
            "At 1086 epoch, Validation Loss: 0.0033168522641062737 \n",
            "At 1087 epoch, Training Loss: 0.0005640877643600106 \n",
            "At 1087 epoch, Validation Loss: 0.003388092154636979 \n",
            "At 1088 epoch, Training Loss: 0.0004711913294158876 \n",
            "At 1088 epoch, Validation Loss: 0.003428613068535924 \n",
            "At 1089 epoch, Training Loss: 0.00041656558169052006 \n",
            "At 1089 epoch, Validation Loss: 0.003381340531632304 \n",
            "At 1090 epoch, Training Loss: 0.00046656116610392927 \n",
            "At 1090 epoch, Validation Loss: 0.003353893756866455 \n",
            "At 1091 epoch, Training Loss: 0.0003259125602198765 \n",
            "At 1091 epoch, Validation Loss: 0.0033489849884063005 \n",
            "At 1092 epoch, Training Loss: 0.00034217212814837696 \n",
            "At 1092 epoch, Validation Loss: 0.0033326917327940464 \n",
            "At 1093 epoch, Training Loss: 0.00043257458601146935 \n",
            "At 1093 epoch, Validation Loss: 0.003343451302498579 \n",
            "At 1094 epoch, Training Loss: 0.0005474903155118227 \n",
            "At 1094 epoch, Validation Loss: 0.0033118189312517643 \n",
            "At 1095 epoch, Training Loss: 0.0005315229180268943 \n",
            "At 1095 epoch, Validation Loss: 0.0032686307094991207 \n",
            "At 1096 epoch, Training Loss: 0.00041074323235079646 \n",
            "At 1096 epoch, Validation Loss: 0.003354543587192893 \n",
            "At 1097 epoch, Training Loss: 0.00048352096346206965 \n",
            "At 1097 epoch, Validation Loss: 0.003480602754279971 \n",
            "At 1098 epoch, Training Loss: 0.00046653617173433306 \n",
            "At 1098 epoch, Validation Loss: 0.003417548956349492 \n",
            "At 1099 epoch, Training Loss: 0.0004954305069986731 \n",
            "At 1099 epoch, Validation Loss: 0.0033148489892482758 \n",
            "At 1100 epoch, Training Loss: 0.0004020025895442814 \n",
            "At 1100 epoch, Validation Loss: 0.0032848455011844635 \n",
            "At 1101 epoch, Training Loss: 0.00044922452070750296 \n",
            "At 1101 epoch, Validation Loss: 0.003278604242950678 \n",
            "At 1102 epoch, Training Loss: 0.00047351149842143057 \n",
            "At 1102 epoch, Validation Loss: 0.0033203503116965294 \n",
            "At 1103 epoch, Training Loss: 0.00028231892501935364 \n",
            "At 1103 epoch, Validation Loss: 0.003387653734534979 \n",
            "At 1104 epoch, Training Loss: 0.0004571590921841562 \n",
            "At 1104 epoch, Validation Loss: 0.003389622550457716 \n",
            "At 1105 epoch, Training Loss: 0.0004098328354302794 \n",
            "At 1105 epoch, Validation Loss: 0.003386903088539839 \n",
            "At 1106 epoch, Training Loss: 0.0005572657042648643 \n",
            "At 1106 epoch, Validation Loss: 0.0034733328502625227 \n",
            "At 1107 epoch, Training Loss: 0.0004899560299236327 \n",
            "At 1107 epoch, Validation Loss: 0.0033308477140963078 \n",
            "At 1108 epoch, Training Loss: 0.0006501061783637851 \n",
            "At 1108 epoch, Validation Loss: 0.003302962053567171 \n",
            "At 1109 epoch, Training Loss: 0.0004741296666907147 \n",
            "At 1109 epoch, Validation Loss: 0.0033809556625783443 \n",
            "At 1110 epoch, Training Loss: 0.0003991594043327495 \n",
            "At 1110 epoch, Validation Loss: 0.0033773283939808607 \n",
            "At 1111 epoch, Training Loss: 0.000490348250605166 \n",
            "At 1111 epoch, Validation Loss: 0.0033932169899344444 \n",
            "At 1112 epoch, Training Loss: 0.0004038871615193784 \n",
            "At 1112 epoch, Validation Loss: 0.0032975266221910715 \n",
            "At 1113 epoch, Training Loss: 0.0004485261219087988 \n",
            "At 1113 epoch, Validation Loss: 0.0032504182308912277 \n",
            "At 1114 epoch, Training Loss: 0.0003486337140202522 \n",
            "At 1114 epoch, Validation Loss: 0.003250931389629841 \n",
            "At 1115 epoch, Training Loss: 0.0005619511764962226 \n",
            "At 1115 epoch, Validation Loss: 0.0033262339420616627 \n",
            "At 1116 epoch, Training Loss: 0.0005208148155361414 \n",
            "At 1116 epoch, Validation Loss: 0.003364953910931945 \n",
            "At 1117 epoch, Training Loss: 0.0005461091583129019 \n",
            "At 1117 epoch, Validation Loss: 0.0036057087127119303 \n",
            "At 1118 epoch, Training Loss: 0.0005154852755367756 \n",
            "At 1118 epoch, Validation Loss: 0.0034682953264564276 \n",
            "At 1119 epoch, Training Loss: 0.00040608156123198567 \n",
            "At 1119 epoch, Validation Loss: 0.0033545151818543673 \n",
            "At 1120 epoch, Training Loss: 0.0004958898527547718 \n",
            "At 1120 epoch, Validation Loss: 0.003299220697954297 \n",
            "At 1121 epoch, Training Loss: 0.000348067851155065 \n",
            "At 1121 epoch, Validation Loss: 0.0032967212609946728 \n",
            "At 1122 epoch, Training Loss: 0.0006368727888911962 \n",
            "At 1122 epoch, Validation Loss: 0.003444911213591695 \n",
            "At 1123 epoch, Training Loss: 0.00035998590756207703 \n",
            "At 1123 epoch, Validation Loss: 0.0033927063923329115 \n",
            "At 1124 epoch, Training Loss: 0.0004546711512375623 \n",
            "At 1124 epoch, Validation Loss: 0.003312273183837533 \n",
            "At 1125 epoch, Training Loss: 0.0006365866807755083 \n",
            "At 1125 epoch, Validation Loss: 0.0034141186624765396 \n",
            "At 1126 epoch, Training Loss: 0.00034260358079336585 \n",
            "At 1126 epoch, Validation Loss: 0.003351459978148341 \n",
            "At 1127 epoch, Training Loss: 0.0004408366046845913 \n",
            "At 1127 epoch, Validation Loss: 0.0033637278247624636 \n",
            "At 1128 epoch, Training Loss: 0.00038107689761091024 \n",
            "At 1128 epoch, Validation Loss: 0.0033364444971084595 \n",
            "At 1129 epoch, Training Loss: 0.00043076898145955056 \n",
            "At 1129 epoch, Validation Loss: 0.003302527591586113 \n",
            "At 1130 epoch, Training Loss: 0.0004025057365652174 \n",
            "At 1130 epoch, Validation Loss: 0.00333105749450624 \n",
            "At 1131 epoch, Training Loss: 0.00048480998957529665 \n",
            "At 1131 epoch, Validation Loss: 0.0033654365688562393 \n",
            "At 1132 epoch, Training Loss: 0.0004482749674934894 \n",
            "At 1132 epoch, Validation Loss: 0.0033382256515324116 \n",
            "At 1133 epoch, Training Loss: 0.0004705515224486589 \n",
            "At 1133 epoch, Validation Loss: 0.003383258357644081 \n",
            "At 1134 epoch, Training Loss: 0.00038720854790881277 \n",
            "At 1134 epoch, Validation Loss: 0.003434993326663971 \n",
            "At 1135 epoch, Training Loss: 0.0005174469610210508 \n",
            "At 1135 epoch, Validation Loss: 0.0033106370829045773 \n",
            "At 1136 epoch, Training Loss: 0.0005259633879177272 \n",
            "At 1136 epoch, Validation Loss: 0.003356702160090208 \n",
            "At 1137 epoch, Training Loss: 0.0005891767563298344 \n",
            "At 1137 epoch, Validation Loss: 0.0033966051414608955 \n",
            "At 1138 epoch, Training Loss: 0.0005537114397156983 \n",
            "At 1138 epoch, Validation Loss: 0.003406054340302944 \n",
            "At 1139 epoch, Training Loss: 0.00042221483890898527 \n",
            "At 1139 epoch, Validation Loss: 0.0033554667606949806 \n",
            "At 1140 epoch, Training Loss: 0.00033728588314261285 \n",
            "At 1140 epoch, Validation Loss: 0.003272003261372447 \n",
            "At 1141 epoch, Training Loss: 0.0004370845708763227 \n",
            "At 1141 epoch, Validation Loss: 0.003326290287077427 \n",
            "At 1142 epoch, Training Loss: 0.0005235276534222066 \n",
            "At 1142 epoch, Validation Loss: 0.0033902700524777174 \n",
            "At 1143 epoch, Training Loss: 0.00039733220473863183 \n",
            "At 1143 epoch, Validation Loss: 0.0033472885843366385 \n",
            "At 1144 epoch, Training Loss: 0.0003129042743239552 \n",
            "At 1144 epoch, Validation Loss: 0.003333741333335638 \n",
            "At 1145 epoch, Training Loss: 0.0005490880110301077 \n",
            "At 1145 epoch, Validation Loss: 0.003321262076497078 \n",
            "At 1146 epoch, Training Loss: 0.00039679773326497527 \n",
            "At 1146 epoch, Validation Loss: 0.003325454890727997 \n",
            "At 1147 epoch, Training Loss: 0.0005200027721002698 \n",
            "At 1147 epoch, Validation Loss: 0.0032447194680571556 \n",
            "At 1148 epoch, Training Loss: 0.0003526572574628517 \n",
            "At 1148 epoch, Validation Loss: 0.003340826602652669 \n",
            "At 1149 epoch, Training Loss: 0.0004019240557681769 \n",
            "At 1149 epoch, Validation Loss: 0.00340276095084846 \n",
            "At 1150 epoch, Training Loss: 0.00041900961368810383 \n",
            "At 1150 epoch, Validation Loss: 0.0032937151845544577 \n",
            "At 1151 epoch, Training Loss: 0.0003622729156631976 \n",
            "At 1151 epoch, Validation Loss: 0.003261811565607786 \n",
            "At 1152 epoch, Training Loss: 0.0003862422978272662 \n",
            "At 1152 epoch, Validation Loss: 0.003271750407293439 \n",
            "At 1153 epoch, Training Loss: 0.0003930215782020241 \n",
            "At 1153 epoch, Validation Loss: 0.0033345224801450968 \n",
            "At 1154 epoch, Training Loss: 0.0006132365786470472 \n",
            "At 1154 epoch, Validation Loss: 0.003318440169095993 \n",
            "At 1155 epoch, Training Loss: 0.0005239427206106484 \n",
            "At 1155 epoch, Validation Loss: 0.003310547908768058 \n",
            "At 1156 epoch, Training Loss: 0.00045533112133853135 \n",
            "At 1156 epoch, Validation Loss: 0.0034013716503977776 \n",
            "At 1157 epoch, Training Loss: 0.00040472726104781034 \n",
            "At 1157 epoch, Validation Loss: 0.0033371089957654476 \n",
            "At 1158 epoch, Training Loss: 0.0005807706387713551 \n",
            "At 1158 epoch, Validation Loss: 0.003329118248075247 \n",
            "At 1159 epoch, Training Loss: 0.0004682099970523268 \n",
            "At 1159 epoch, Validation Loss: 0.0033964416943490505 \n",
            "At 1160 epoch, Training Loss: 0.00044237649417482315 \n",
            "At 1160 epoch, Validation Loss: 0.0033571147359907627 \n",
            "At 1161 epoch, Training Loss: 0.0005001111945603043 \n",
            "At 1161 epoch, Validation Loss: 0.0033561133313924074 \n",
            "At 1162 epoch, Training Loss: 0.0005637149995891377 \n",
            "At 1162 epoch, Validation Loss: 0.0033552551176398993 \n",
            "At 1163 epoch, Training Loss: 0.00046508694649674 \n",
            "At 1163 epoch, Validation Loss: 0.003351001301780343 \n",
            "At 1164 epoch, Training Loss: 0.00039089482452254743 \n",
            "At 1164 epoch, Validation Loss: 0.0033306817058473825 \n",
            "At 1165 epoch, Training Loss: 0.0003526843502186239 \n",
            "At 1165 epoch, Validation Loss: 0.003255048766732216 \n",
            "At 1166 epoch, Training Loss: 0.0003325796773424372 \n",
            "At 1166 epoch, Validation Loss: 0.0032657140400260687 \n",
            "At 1167 epoch, Training Loss: 0.00046245120465755464 \n",
            "At 1167 epoch, Validation Loss: 0.0032891288865357637 \n",
            "At 1168 epoch, Training Loss: 0.00045900161494500936 \n",
            "At 1168 epoch, Validation Loss: 0.003320725169032812 \n",
            "At 1169 epoch, Training Loss: 0.0003915855893865228 \n",
            "At 1169 epoch, Validation Loss: 0.003318682312965393 \n",
            "At 1170 epoch, Training Loss: 0.0004498046182561666 \n",
            "At 1170 epoch, Validation Loss: 0.003277300391346216 \n",
            "At 1171 epoch, Training Loss: 0.0005369304039049893 \n",
            "At 1171 epoch, Validation Loss: 0.0033784196712076664 \n",
            "At 1172 epoch, Training Loss: 0.0003829287190455943 \n",
            "At 1172 epoch, Validation Loss: 0.0034057176671922207 \n",
            "At 1173 epoch, Training Loss: 0.0003933404921554029 \n",
            "At 1173 epoch, Validation Loss: 0.0033523528836667538 \n",
            "At 1174 epoch, Training Loss: 0.0003454206307651475 \n",
            "At 1174 epoch, Validation Loss: 0.003304689424112439 \n",
            "At 1175 epoch, Training Loss: 0.0003177329956088215 \n",
            "At 1175 epoch, Validation Loss: 0.0033370046876370907 \n",
            "At 1176 epoch, Training Loss: 0.0005008477630326525 \n",
            "At 1176 epoch, Validation Loss: 0.0033082179725170135 \n",
            "At 1177 epoch, Training Loss: 0.0003876875271089375 \n",
            "At 1177 epoch, Validation Loss: 0.0032348460517823696 \n",
            "At 1178 epoch, Training Loss: 0.00035660908033605663 \n",
            "At 1178 epoch, Validation Loss: 0.0033540695440024137 \n",
            "At 1179 epoch, Training Loss: 0.0003773800563067198 \n",
            "At 1179 epoch, Validation Loss: 0.0034208735451102257 \n",
            "At 1180 epoch, Training Loss: 0.0004118307959288359 \n",
            "At 1180 epoch, Validation Loss: 0.003384598297998309 \n",
            "At 1181 epoch, Training Loss: 0.0004302594839828089 \n",
            "At 1181 epoch, Validation Loss: 0.003349949838593602 \n",
            "At 1182 epoch, Training Loss: 0.0003791194234509021 \n",
            "At 1182 epoch, Validation Loss: 0.003291663248091936 \n",
            "At 1183 epoch, Training Loss: 0.00046791473869234324 \n",
            "At 1183 epoch, Validation Loss: 0.0033035161904990673 \n",
            "At 1184 epoch, Training Loss: 0.0004430775530636311 \n",
            "At 1184 epoch, Validation Loss: 0.0033603250049054623 \n",
            "At 1185 epoch, Training Loss: 0.0004136561241466552 \n",
            "At 1185 epoch, Validation Loss: 0.003322055796161294 \n",
            "At 1186 epoch, Training Loss: 0.00036477616522461175 \n",
            "At 1186 epoch, Validation Loss: 0.0033341008238494396 \n",
            "At 1187 epoch, Training Loss: 0.000359395140549168 \n",
            "At 1187 epoch, Validation Loss: 0.00339435669593513 \n",
            "At 1188 epoch, Training Loss: 0.00042540593713056294 \n",
            "At 1188 epoch, Validation Loss: 0.003388844197615981 \n",
            "At 1189 epoch, Training Loss: 0.0003986316383816302 \n",
            "At 1189 epoch, Validation Loss: 0.003399599576368928 \n",
            "At 1190 epoch, Training Loss: 0.00045842964900657537 \n",
            "At 1190 epoch, Validation Loss: 0.003385010873898864 \n",
            "At 1191 epoch, Training Loss: 0.00043613756424747406 \n",
            "At 1191 epoch, Validation Loss: 0.003366755787283182 \n",
            "At 1192 epoch, Training Loss: 0.00040154616872314366 \n",
            "At 1192 epoch, Validation Loss: 0.0032956802751868963 \n",
            "At 1193 epoch, Training Loss: 0.00031139553466346114 \n",
            "At 1193 epoch, Validation Loss: 0.0032540266402065754 \n",
            "At 1194 epoch, Training Loss: 0.00033276637841481713 \n",
            "At 1194 epoch, Validation Loss: 0.0033154094126075506 \n",
            "At 1195 epoch, Training Loss: 0.0004624183813575655 \n",
            "At 1195 epoch, Validation Loss: 0.0033143204636871815 \n",
            "At 1196 epoch, Training Loss: 0.00036796648637391625 \n",
            "At 1196 epoch, Validation Loss: 0.003313471330329776 \n",
            "At 1197 epoch, Training Loss: 0.00047949026338756084 \n",
            "At 1197 epoch, Validation Loss: 0.0033495118841528893 \n",
            "At 1198 epoch, Training Loss: 0.0004087504697963595 \n",
            "At 1198 epoch, Validation Loss: 0.003350373823195696 \n",
            "At 1199 epoch, Training Loss: 0.0003629330429248512 \n",
            "At 1199 epoch, Validation Loss: 0.0033375730272382498 \n",
            "At 1200 epoch, Training Loss: 0.00042433138005435465 \n",
            "At 1200 epoch, Validation Loss: 0.003295665606856346 \n",
            "At 1201 epoch, Training Loss: 0.00040863901958800854 \n",
            "At 1201 epoch, Validation Loss: 0.003256083931773901 \n",
            "At 1202 epoch, Training Loss: 0.00037878255534451455 \n",
            "At 1202 epoch, Validation Loss: 0.0033037804532796144 \n",
            "At 1203 epoch, Training Loss: 0.00039698235923424363 \n",
            "At 1203 epoch, Validation Loss: 0.0033801011741161346 \n",
            "At 1204 epoch, Training Loss: 0.0005146393086761236 \n",
            "At 1204 epoch, Validation Loss: 0.003367850324138999 \n",
            "At 1205 epoch, Training Loss: 0.00045310566783882675 \n",
            "At 1205 epoch, Validation Loss: 0.0033479344565421343 \n",
            "At 1206 epoch, Training Loss: 0.0004208015918266028 \n",
            "At 1206 epoch, Validation Loss: 0.003451061202213168 \n",
            "At 1207 epoch, Training Loss: 0.0006165462313219905 \n",
            "At 1207 epoch, Validation Loss: 0.003355245338752866 \n",
            "At 1208 epoch, Training Loss: 0.00042788682039827106 \n",
            "At 1208 epoch, Validation Loss: 0.0033295040484517813 \n",
            "At 1209 epoch, Training Loss: 0.0003908042766852304 \n",
            "At 1209 epoch, Validation Loss: 0.0032975052017718554 \n",
            "At 1210 epoch, Training Loss: 0.00048160371370613574 \n",
            "At 1210 epoch, Validation Loss: 0.0033093190286308527 \n",
            "At 1211 epoch, Training Loss: 0.0004436241055373102 \n",
            "At 1211 epoch, Validation Loss: 0.0034201594535261393 \n",
            "At 1212 epoch, Training Loss: 0.0004657188546843827 \n",
            "At 1212 epoch, Validation Loss: 0.003373692976310849 \n",
            "At 1213 epoch, Training Loss: 0.000352701309020631 \n",
            "At 1213 epoch, Validation Loss: 0.0032944781705737114 \n",
            "At 1214 epoch, Training Loss: 0.00032546776346862314 \n",
            "At 1214 epoch, Validation Loss: 0.003278936492279172 \n",
            "At 1215 epoch, Training Loss: 0.00034584521781653164 \n",
            "At 1215 epoch, Validation Loss: 0.0032778498716652393 \n",
            "At 1216 epoch, Training Loss: 0.00034010246163234114 \n",
            "At 1216 epoch, Validation Loss: 0.0032972211483865976 \n",
            "At 1217 epoch, Training Loss: 0.0004734705318696797 \n",
            "At 1217 epoch, Validation Loss: 0.0033308914862573147 \n",
            "At 1218 epoch, Training Loss: 0.0004495915141887963 \n",
            "At 1218 epoch, Validation Loss: 0.0032935994677245617 \n",
            "At 1219 epoch, Training Loss: 0.0004998714954126626 \n",
            "At 1219 epoch, Validation Loss: 0.0033148732036352158 \n",
            "At 1220 epoch, Training Loss: 0.0004190808482235298 \n",
            "At 1220 epoch, Validation Loss: 0.0033959648571908474 \n",
            "At 1221 epoch, Training Loss: 0.0004992907284758985 \n",
            "At 1221 epoch, Validation Loss: 0.0033477721735835075 \n",
            "At 1222 epoch, Training Loss: 0.0004350701317889616 \n",
            "At 1222 epoch, Validation Loss: 0.0033834711648523808 \n",
            "At 1223 epoch, Training Loss: 0.0004489937098696828 \n",
            "At 1223 epoch, Validation Loss: 0.0033974149264395237 \n",
            "At 1224 epoch, Training Loss: 0.0002980807417770848 \n",
            "At 1224 epoch, Validation Loss: 0.00335423625074327 \n",
            "At 1225 epoch, Training Loss: 0.00040265278075821696 \n",
            "At 1225 epoch, Validation Loss: 0.0032990584149956703 \n",
            "At 1226 epoch, Training Loss: 0.00038856704195495696 \n",
            "At 1226 epoch, Validation Loss: 0.0032955019269138575 \n",
            "At 1227 epoch, Training Loss: 0.0005710593366529792 \n",
            "At 1227 epoch, Validation Loss: 0.0033597596921026707 \n",
            "At 1228 epoch, Training Loss: 0.0003948511613998562 \n",
            "At 1228 epoch, Validation Loss: 0.003306120168417692 \n",
            "At 1229 epoch, Training Loss: 0.0003493408788926899 \n",
            "At 1229 epoch, Validation Loss: 0.003306384664028883 \n",
            "At 1230 epoch, Training Loss: 0.00038458756753243504 \n",
            "At 1230 epoch, Validation Loss: 0.0033015224616974592 \n",
            "At 1231 epoch, Training Loss: 0.0006048284179996699 \n",
            "At 1231 epoch, Validation Loss: 0.003349337028339505 \n",
            "At 1232 epoch, Training Loss: 0.0004990700748749077 \n",
            "At 1232 epoch, Validation Loss: 0.0033681923523545265 \n",
            "At 1233 epoch, Training Loss: 0.0005188677867408841 \n",
            "At 1233 epoch, Validation Loss: 0.003355245105922222 \n",
            "At 1234 epoch, Training Loss: 0.000439089594874531 \n",
            "At 1234 epoch, Validation Loss: 0.003346072044223547 \n",
            "At 1235 epoch, Training Loss: 0.00038090497255325315 \n",
            "At 1235 epoch, Validation Loss: 0.003353886539116502 \n",
            "At 1236 epoch, Training Loss: 0.00042545851902104915 \n",
            "At 1236 epoch, Validation Loss: 0.0033625864889472723 \n",
            "At 1237 epoch, Training Loss: 0.0004067912232130766 \n",
            "At 1237 epoch, Validation Loss: 0.0033554125111550093 \n",
            "At 1238 epoch, Training Loss: 0.0004765918536577374 \n",
            "At 1238 epoch, Validation Loss: 0.003327046288177371 \n",
            "At 1239 epoch, Training Loss: 0.0003187249938491732 \n",
            "At 1239 epoch, Validation Loss: 0.0033281061332672834 \n",
            "At 1240 epoch, Training Loss: 0.0003355826047481969 \n",
            "At 1240 epoch, Validation Loss: 0.003321627154946327 \n",
            "At 1241 epoch, Training Loss: 0.00046387819456867875 \n",
            "At 1241 epoch, Validation Loss: 0.0032774386927485466 \n",
            "At 1242 epoch, Training Loss: 0.0003588623774703592 \n",
            "At 1242 epoch, Validation Loss: 0.0033284432720392942 \n",
            "At 1243 epoch, Training Loss: 0.000378859115880914 \n",
            "At 1243 epoch, Validation Loss: 0.0033671269193291664 \n",
            "At 1244 epoch, Training Loss: 0.00048375240876339374 \n",
            "At 1244 epoch, Validation Loss: 0.003345344215631485 \n",
            "At 1245 epoch, Training Loss: 0.00039701411151327193 \n",
            "At 1245 epoch, Validation Loss: 0.003331915009766817 \n",
            "At 1246 epoch, Training Loss: 0.0003196771664079279 \n",
            "At 1246 epoch, Validation Loss: 0.00332841114141047 \n",
            "At 1247 epoch, Training Loss: 0.0003284988430095837 \n",
            "At 1247 epoch, Validation Loss: 0.003320342628285289 \n",
            "At 1248 epoch, Training Loss: 0.00041898293420672417 \n",
            "At 1248 epoch, Validation Loss: 0.0033074680250138044 \n",
            "At 1249 epoch, Training Loss: 0.0005843603808898479 \n",
            "At 1249 epoch, Validation Loss: 0.003287675091996789 \n",
            "At 1250 epoch, Training Loss: 0.00032203480368480085 \n",
            "At 1250 epoch, Validation Loss: 0.0033315573818981647 \n",
            "At 1251 epoch, Training Loss: 0.0003442119952524081 \n",
            "At 1251 epoch, Validation Loss: 0.0033579415176063776 \n",
            "At 1252 epoch, Training Loss: 0.0004687934590037912 \n",
            "At 1252 epoch, Validation Loss: 0.00330620096065104 \n",
            "At 1253 epoch, Training Loss: 0.0003869473759550601 \n",
            "At 1253 epoch, Validation Loss: 0.003313731402158737 \n",
            "At 1254 epoch, Training Loss: 0.000314386494574137 \n",
            "At 1254 epoch, Validation Loss: 0.0033780005760490894 \n",
            "At 1255 epoch, Training Loss: 0.0002935420023277402 \n",
            "At 1255 epoch, Validation Loss: 0.003442571498453617 \n",
            "At 1256 epoch, Training Loss: 0.00040594110614620147 \n",
            "At 1256 epoch, Validation Loss: 0.0034225909039378166 \n",
            "At 1257 epoch, Training Loss: 0.0003948257915908471 \n",
            "At 1257 epoch, Validation Loss: 0.003354530083015561 \n",
            "At 1258 epoch, Training Loss: 0.0004192542226519436 \n",
            "At 1258 epoch, Validation Loss: 0.0033015254884958267 \n",
            "At 1259 epoch, Training Loss: 0.00029548316961154343 \n",
            "At 1259 epoch, Validation Loss: 0.003285809652879834 \n",
            "At 1260 epoch, Training Loss: 0.0003250682522775605 \n",
            "At 1260 epoch, Validation Loss: 0.0033158231526613235 \n",
            "At 1261 epoch, Training Loss: 0.00042099942511413245 \n",
            "At 1261 epoch, Validation Loss: 0.0033451137132942677 \n",
            "At 1262 epoch, Training Loss: 0.00047619076212868094 \n",
            "At 1262 epoch, Validation Loss: 0.003347186604514718 \n",
            "At 1263 epoch, Training Loss: 0.0003556483483407646 \n",
            "At 1263 epoch, Validation Loss: 0.003315950511023402 \n",
            "At 1264 epoch, Training Loss: 0.00040102768107317386 \n",
            "At 1264 epoch, Validation Loss: 0.003329123370349407 \n",
            "At 1265 epoch, Training Loss: 0.0003235864045564085 \n",
            "At 1265 epoch, Validation Loss: 0.003317998256534338 \n",
            "At 1266 epoch, Training Loss: 0.00043333539506420495 \n",
            "At 1266 epoch, Validation Loss: 0.0033262809738516808 \n",
            "At 1267 epoch, Training Loss: 0.00033460350532550366 \n",
            "At 1267 epoch, Validation Loss: 0.00334076676517725 \n",
            "At 1268 epoch, Training Loss: 0.0004850323079153895 \n",
            "At 1268 epoch, Validation Loss: 0.0033326914999634027 \n",
            "At 1269 epoch, Training Loss: 0.0003501689381664619 \n",
            "At 1269 epoch, Validation Loss: 0.0033491812646389008 \n",
            "At 1270 epoch, Training Loss: 0.00031623790855519476 \n",
            "At 1270 epoch, Validation Loss: 0.0033508811611682177 \n",
            "At 1271 epoch, Training Loss: 0.0003990792145486921 \n",
            "At 1271 epoch, Validation Loss: 0.003278754884377122 \n",
            "At 1272 epoch, Training Loss: 0.0004410035762703046 \n",
            "At 1272 epoch, Validation Loss: 0.0032991834450513124 \n",
            "At 1273 epoch, Training Loss: 0.0004854886792600155 \n",
            "At 1273 epoch, Validation Loss: 0.0033404617570340633 \n",
            "At 1274 epoch, Training Loss: 0.0003776372759602964 \n",
            "At 1274 epoch, Validation Loss: 0.0033139775041490793 \n",
            "At 1275 epoch, Training Loss: 0.0004094231000635773 \n",
            "At 1275 epoch, Validation Loss: 0.0033324584364891052 \n",
            "At 1276 epoch, Training Loss: 0.00033588705118745563 \n",
            "At 1276 epoch, Validation Loss: 0.003317770082503557 \n",
            "At 1277 epoch, Training Loss: 0.00038025594549253585 \n",
            "At 1277 epoch, Validation Loss: 0.003344191936776042 \n",
            "At 1278 epoch, Training Loss: 0.00041588971798773856 \n",
            "At 1278 epoch, Validation Loss: 0.0033499719575047493 \n",
            "At 1279 epoch, Training Loss: 0.00038232492515817286 \n",
            "At 1279 epoch, Validation Loss: 0.0033529612701386213 \n",
            "At 1280 epoch, Training Loss: 0.00032343023340217767 \n",
            "At 1280 epoch, Validation Loss: 0.0033384810667485 \n",
            "At 1281 epoch, Training Loss: 0.00044915448233950883 \n",
            "At 1281 epoch, Validation Loss: 0.0033100598957389593 \n",
            "At 1282 epoch, Training Loss: 0.0004937562189297751 \n",
            "At 1282 epoch, Validation Loss: 0.0033522697631269693 \n",
            "At 1283 epoch, Training Loss: 0.0005012408888433129 \n",
            "At 1283 epoch, Validation Loss: 0.0033579031005501747 \n",
            "At 1284 epoch, Training Loss: 0.00042379434453323486 \n",
            "At 1284 epoch, Validation Loss: 0.0033054996747523546 \n",
            "At 1285 epoch, Training Loss: 0.00039596619608346374 \n",
            "At 1285 epoch, Validation Loss: 0.003300069598481059 \n",
            "At 1286 epoch, Training Loss: 0.000366012126323767 \n",
            "At 1286 epoch, Validation Loss: 0.003305490594357252 \n",
            "At 1287 epoch, Training Loss: 0.0002647579211043194 \n",
            "At 1287 epoch, Validation Loss: 0.003308680607005954 \n",
            "At 1288 epoch, Training Loss: 0.0002670193469384685 \n",
            "At 1288 epoch, Validation Loss: 0.00332557107321918 \n",
            "At 1289 epoch, Training Loss: 0.0003818523488007486 \n",
            "At 1289 epoch, Validation Loss: 0.003324276302009821 \n",
            "At 1290 epoch, Training Loss: 0.0002978198870550841 \n",
            "At 1290 epoch, Validation Loss: 0.0033050274942070246 \n",
            "At 1291 epoch, Training Loss: 0.000399856292642653 \n",
            "At 1291 epoch, Validation Loss: 0.003307500621303916 \n",
            "At 1292 epoch, Training Loss: 0.0003079907735809684 \n",
            "At 1292 epoch, Validation Loss: 0.0033292281441390514 \n",
            "At 1293 epoch, Training Loss: 0.00035958075313828884 \n",
            "At 1293 epoch, Validation Loss: 0.0033595869317650795 \n",
            "At 1294 epoch, Training Loss: 0.00037970242265146227 \n",
            "At 1294 epoch, Validation Loss: 0.00335215637460351 \n",
            "At 1295 epoch, Training Loss: 0.0004224096628604457 \n",
            "At 1295 epoch, Validation Loss: 0.0033776566851884127 \n",
            "At 1296 epoch, Training Loss: 0.00046002314193174245 \n",
            "At 1296 epoch, Validation Loss: 0.003373656654730439 \n",
            "At 1297 epoch, Training Loss: 0.00032045416301116345 \n",
            "At 1297 epoch, Validation Loss: 0.0033365327399224043 \n",
            "At 1298 epoch, Training Loss: 0.0004273131140507758 \n",
            "At 1298 epoch, Validation Loss: 0.0033065497409552336 \n",
            "At 1299 epoch, Training Loss: 0.0003481381805613637 \n",
            "At 1299 epoch, Validation Loss: 0.0032843148801475763 \n",
            "At 1300 epoch, Training Loss: 0.0003132775775156915 \n",
            "At 1300 epoch, Validation Loss: 0.0033555752597749233 \n",
            "At 1301 epoch, Training Loss: 0.00039250903064385055 \n",
            "At 1301 epoch, Validation Loss: 0.003324988530948758 \n",
            "At 1302 epoch, Training Loss: 0.0003996907762484625 \n",
            "At 1302 epoch, Validation Loss: 0.003309443360194564 \n",
            "At 1303 epoch, Training Loss: 0.0003807439701631665 \n",
            "At 1303 epoch, Validation Loss: 0.0033327576238662004 \n",
            "At 1304 epoch, Training Loss: 0.000291823671432212 \n",
            "At 1304 epoch, Validation Loss: 0.0033460594713687897 \n",
            "At 1305 epoch, Training Loss: 0.0003281329583842307 \n",
            "At 1305 epoch, Validation Loss: 0.003329915227368474 \n",
            "At 1306 epoch, Training Loss: 0.0003324099874589592 \n",
            "At 1306 epoch, Validation Loss: 0.0033412000630050898 \n",
            "At 1307 epoch, Training Loss: 0.0003698403714224696 \n",
            "At 1307 epoch, Validation Loss: 0.003359139896929264 \n",
            "At 1308 epoch, Training Loss: 0.0003552791313268244 \n",
            "At 1308 epoch, Validation Loss: 0.003335698042064905 \n",
            "At 1309 epoch, Training Loss: 0.0005527233355678618 \n",
            "At 1309 epoch, Validation Loss: 0.0033318509813398123 \n",
            "At 1310 epoch, Training Loss: 0.00041905280959326775 \n",
            "At 1310 epoch, Validation Loss: 0.0033812422771006823 \n",
            "At 1311 epoch, Training Loss: 0.0003812927781837061 \n",
            "At 1311 epoch, Validation Loss: 0.003378011751919985 \n",
            "At 1312 epoch, Training Loss: 0.0003654020925750956 \n",
            "At 1312 epoch, Validation Loss: 0.0033218353055417538 \n",
            "At 1313 epoch, Training Loss: 0.0003756801423151046 \n",
            "At 1313 epoch, Validation Loss: 0.00332241109572351 \n",
            "At 1314 epoch, Training Loss: 0.0003092758008278906 \n",
            "At 1314 epoch, Validation Loss: 0.003336337162181735 \n",
            "At 1315 epoch, Training Loss: 0.00031583539093844594 \n",
            "At 1315 epoch, Validation Loss: 0.0033304665703326464 \n",
            "At 1316 epoch, Training Loss: 0.0003585544298402965 \n",
            "At 1316 epoch, Validation Loss: 0.00334560452029109 \n",
            "At 1317 epoch, Training Loss: 0.00034353359951637684 \n",
            "At 1317 epoch, Validation Loss: 0.0033298241905868053 \n",
            "At 1318 epoch, Training Loss: 0.0004312166420277208 \n",
            "At 1318 epoch, Validation Loss: 0.0033137015998363495 \n",
            "At 1319 epoch, Training Loss: 0.0003618547925725579 \n",
            "At 1319 epoch, Validation Loss: 0.0033153793774545193 \n",
            "At 1320 epoch, Training Loss: 0.00031424225890077653 \n",
            "At 1320 epoch, Validation Loss: 0.0033116864506155252 \n",
            "At 1321 epoch, Training Loss: 0.0004464851226657629 \n",
            "At 1321 epoch, Validation Loss: 0.0033453430514782667 \n",
            "At 1322 epoch, Training Loss: 0.0004182630538707599 \n",
            "At 1322 epoch, Validation Loss: 0.0033786736894398928 \n",
            "At 1323 epoch, Training Loss: 0.00030914606468286366 \n",
            "At 1323 epoch, Validation Loss: 0.0034394722897559404 \n",
            "At 1324 epoch, Training Loss: 0.0004005160531960428 \n",
            "At 1324 epoch, Validation Loss: 0.003404186572879553 \n",
            "At 1325 epoch, Training Loss: 0.0003040698997210711 \n",
            "At 1325 epoch, Validation Loss: 0.0033330160658806562 \n",
            "At 1326 epoch, Training Loss: 0.0003982158930739388 \n",
            "At 1326 epoch, Validation Loss: 0.0032960076350718737 \n",
            "At 1327 epoch, Training Loss: 0.0004586536786518991 \n",
            "At 1327 epoch, Validation Loss: 0.0033043231815099716 \n",
            "At 1328 epoch, Training Loss: 0.0004555064864689484 \n",
            "At 1328 epoch, Validation Loss: 0.0033667320385575294 \n",
            "At 1329 epoch, Training Loss: 0.00035974857746623456 \n",
            "At 1329 epoch, Validation Loss: 0.0033811999019235373 \n",
            "At 1330 epoch, Training Loss: 0.00033665298542473465 \n",
            "At 1330 epoch, Validation Loss: 0.003322283271700144 \n",
            "At 1331 epoch, Training Loss: 0.00044294632971286774 \n",
            "At 1331 epoch, Validation Loss: 0.003337171161547303 \n",
            "At 1332 epoch, Training Loss: 0.00033139598090201614 \n",
            "At 1332 epoch, Validation Loss: 0.003405182622373104 \n",
            "At 1333 epoch, Training Loss: 0.00043392906663939355 \n",
            "At 1333 epoch, Validation Loss: 0.003388647921383381 \n",
            "At 1334 epoch, Training Loss: 0.0003830369445495307 \n",
            "At 1334 epoch, Validation Loss: 0.0033255531452596188 \n",
            "At 1335 epoch, Training Loss: 0.0002069644775474444 \n",
            "At 1335 epoch, Validation Loss: 0.003301774151623249 \n",
            "At 1336 epoch, Training Loss: 0.0003596502705477178 \n",
            "At 1336 epoch, Validation Loss: 0.0033139586448669434 \n",
            "At 1337 epoch, Training Loss: 0.00046836648834869266 \n",
            "At 1337 epoch, Validation Loss: 0.003366075688973069 \n",
            "At 1338 epoch, Training Loss: 0.0003616000700276345 \n",
            "At 1338 epoch, Validation Loss: 0.0033540986478328705 \n",
            "At 1339 epoch, Training Loss: 0.0002802014845656231 \n",
            "At 1339 epoch, Validation Loss: 0.003330576466396451 \n",
            "At 1340 epoch, Training Loss: 0.00041865048697218297 \n",
            "At 1340 epoch, Validation Loss: 0.003297870047390461 \n",
            "At 1341 epoch, Training Loss: 0.00046800796990282835 \n",
            "At 1341 epoch, Validation Loss: 0.003282042220234871 \n",
            "At 1342 epoch, Training Loss: 0.0004978871031198651 \n",
            "At 1342 epoch, Validation Loss: 0.0032565894071012735 \n",
            "At 1343 epoch, Training Loss: 0.00044169549946673217 \n",
            "At 1343 epoch, Validation Loss: 0.003275140654295683 \n",
            "At 1344 epoch, Training Loss: 0.0003812051058048382 \n",
            "At 1344 epoch, Validation Loss: 0.003380414331331849 \n",
            "At 1345 epoch, Training Loss: 0.0004510634986218065 \n",
            "At 1345 epoch, Validation Loss: 0.0033888635225594044 \n",
            "At 1346 epoch, Training Loss: 0.00040092826238833366 \n",
            "At 1346 epoch, Validation Loss: 0.0033583275508135557 \n",
            "At 1347 epoch, Training Loss: 0.00038389867986552415 \n",
            "At 1347 epoch, Validation Loss: 0.003324151271954179 \n",
            "At 1348 epoch, Training Loss: 0.000508886284660548 \n",
            "At 1348 epoch, Validation Loss: 0.00329900742508471 \n",
            "At 1349 epoch, Training Loss: 0.00037540710181929173 \n",
            "At 1349 epoch, Validation Loss: 0.0032706144265830517 \n",
            "At 1350 epoch, Training Loss: 0.0004104406863916665 \n",
            "At 1350 epoch, Validation Loss: 0.003321876283735037 \n",
            "At 1351 epoch, Training Loss: 0.000365658279042691 \n",
            "At 1351 epoch, Validation Loss: 0.003397308988496661 \n",
            "At 1352 epoch, Training Loss: 0.00027658862236421553 \n",
            "At 1352 epoch, Validation Loss: 0.0033826003782451153 \n",
            "At 1353 epoch, Training Loss: 0.00037731939228251575 \n",
            "At 1353 epoch, Validation Loss: 0.0033337639179080725 \n",
            "At 1354 epoch, Training Loss: 0.0002868837444111705 \n",
            "At 1354 epoch, Validation Loss: 0.0033136592246592045 \n",
            "At 1355 epoch, Training Loss: 0.0003535201714839786 \n",
            "At 1355 epoch, Validation Loss: 0.003307277336716652 \n",
            "At 1356 epoch, Training Loss: 0.000494421913754195 \n",
            "At 1356 epoch, Validation Loss: 0.0033054302912205458 \n",
            "At 1357 epoch, Training Loss: 0.0004088535701157525 \n",
            "At 1357 epoch, Validation Loss: 0.0033038672991096973 \n",
            "At 1358 epoch, Training Loss: 0.00041887336410582063 \n",
            "At 1358 epoch, Validation Loss: 0.0033094820100814104 \n",
            "At 1359 epoch, Training Loss: 0.00033060692658182234 \n",
            "At 1359 epoch, Validation Loss: 0.0033591329120099545 \n",
            "At 1360 epoch, Training Loss: 0.00049314129864797 \n",
            "At 1360 epoch, Validation Loss: 0.0033484576269984245 \n",
            "At 1361 epoch, Training Loss: 0.0004042035900056362 \n",
            "At 1361 epoch, Validation Loss: 0.003280113684013486 \n",
            "At 1362 epoch, Training Loss: 0.0003305199963506311 \n",
            "At 1362 epoch, Validation Loss: 0.0032666479237377644 \n",
            "At 1363 epoch, Training Loss: 0.00041202646680176257 \n",
            "At 1363 epoch, Validation Loss: 0.0032994458451867104 \n",
            "At 1364 epoch, Training Loss: 0.0003483497770503163 \n",
            "At 1364 epoch, Validation Loss: 0.0033198616001755 \n",
            "At 1365 epoch, Training Loss: 0.0004018802253995091 \n",
            "At 1365 epoch, Validation Loss: 0.003336088266223669 \n",
            "At 1366 epoch, Training Loss: 0.00040389153000433 \n",
            "At 1366 epoch, Validation Loss: 0.0033376701176166534 \n",
            "At 1367 epoch, Training Loss: 0.000399945565732196 \n",
            "At 1367 epoch, Validation Loss: 0.003321872092783451 \n",
            "At 1368 epoch, Training Loss: 0.00045629586675204336 \n",
            "At 1368 epoch, Validation Loss: 0.003340927418321371 \n",
            "At 1369 epoch, Training Loss: 0.0003118349006399512 \n",
            "At 1369 epoch, Validation Loss: 0.0033413548953831196 \n",
            "At 1370 epoch, Training Loss: 0.00033785146952141075 \n",
            "At 1370 epoch, Validation Loss: 0.0033170885872095823 \n",
            "At 1371 epoch, Training Loss: 0.00031651720637455584 \n",
            "At 1371 epoch, Validation Loss: 0.0033089148346334696 \n",
            "At 1372 epoch, Training Loss: 0.00038382083585020155 \n",
            "At 1372 epoch, Validation Loss: 0.003326146863400936 \n",
            "At 1373 epoch, Training Loss: 0.00029729168745689093 \n",
            "At 1373 epoch, Validation Loss: 0.0033804054837673903 \n",
            "At 1374 epoch, Training Loss: 0.00044142032274976375 \n",
            "At 1374 epoch, Validation Loss: 0.0033473449293524027 \n",
            "At 1375 epoch, Training Loss: 0.0004410927649587393 \n",
            "At 1375 epoch, Validation Loss: 0.0033168571535497904 \n",
            "At 1376 epoch, Training Loss: 0.00047493181191384794 \n",
            "At 1376 epoch, Validation Loss: 0.0033245456870645285 \n",
            "At 1377 epoch, Training Loss: 0.00028715355147141964 \n",
            "At 1377 epoch, Validation Loss: 0.0033508751075714827 \n",
            "At 1378 epoch, Training Loss: 0.00032830295676831154 \n",
            "At 1378 epoch, Validation Loss: 0.003352460218593478 \n",
            "At 1379 epoch, Training Loss: 0.00036563053145073355 \n",
            "At 1379 epoch, Validation Loss: 0.00332967983558774 \n",
            "At 1380 epoch, Training Loss: 0.0003509291331283748 \n",
            "At 1380 epoch, Validation Loss: 0.0032939831726253033 \n",
            "At 1381 epoch, Training Loss: 0.00040355474629905075 \n",
            "At 1381 epoch, Validation Loss: 0.0033338998910039663 \n",
            "At 1382 epoch, Training Loss: 0.00040328289032913743 \n",
            "At 1382 epoch, Validation Loss: 0.0033756480552256107 \n",
            "At 1383 epoch, Training Loss: 0.00025299892586190255 \n",
            "At 1383 epoch, Validation Loss: 0.0033660035114735365 \n",
            "At 1384 epoch, Training Loss: 0.0003872532455716282 \n",
            "At 1384 epoch, Validation Loss: 0.003321000374853611 \n",
            "At 1385 epoch, Training Loss: 0.0003810883208643645 \n",
            "At 1385 epoch, Validation Loss: 0.003291749395430088 \n",
            "At 1386 epoch, Training Loss: 0.00028408867365214974 \n",
            "At 1386 epoch, Validation Loss: 0.003329530591145158 \n",
            "At 1387 epoch, Training Loss: 0.0002487740246579051 \n",
            "At 1387 epoch, Validation Loss: 0.0033464476000517607 \n",
            "At 1388 epoch, Training Loss: 0.0002762473886832595 \n",
            "At 1388 epoch, Validation Loss: 0.003357016947120428 \n",
            "At 1389 epoch, Training Loss: 0.0003393959195818752 \n",
            "At 1389 epoch, Validation Loss: 0.0033298388589173555 \n",
            "At 1390 epoch, Training Loss: 0.00035478396457619963 \n",
            "At 1390 epoch, Validation Loss: 0.0033244676887989044 \n",
            "At 1391 epoch, Training Loss: 0.0002435909176710993 \n",
            "At 1391 epoch, Validation Loss: 0.0033183519262820482 \n",
            "At 1392 epoch, Training Loss: 0.00032442731608171014 \n",
            "At 1392 epoch, Validation Loss: 0.0033451737836003304 \n",
            "At 1393 epoch, Training Loss: 0.00044858570327050985 \n",
            "At 1393 epoch, Validation Loss: 0.003339318558573723 \n",
            "At 1394 epoch, Training Loss: 0.000287404254777357 \n",
            "At 1394 epoch, Validation Loss: 0.003318029223009944 \n",
            "At 1395 epoch, Training Loss: 0.0003296636277809739 \n",
            "At 1395 epoch, Validation Loss: 0.0033044747542589903 \n",
            "At 1396 epoch, Training Loss: 0.00039658849709667265 \n",
            "At 1396 epoch, Validation Loss: 0.003326174570247531 \n",
            "At 1397 epoch, Training Loss: 0.0004247814125847071 \n",
            "At 1397 epoch, Validation Loss: 0.003305384423583746 \n",
            "At 1398 epoch, Training Loss: 0.00032762217451818285 \n",
            "At 1398 epoch, Validation Loss: 0.003302479861304164 \n",
            "At 1399 epoch, Training Loss: 0.0003185520035913214 \n",
            "At 1399 epoch, Validation Loss: 0.003315707203000784 \n",
            "At 1400 epoch, Training Loss: 0.00034563669178169223 \n",
            "At 1400 epoch, Validation Loss: 0.0033303452655673027 \n",
            "At 1401 epoch, Training Loss: 0.0003989827295299619 \n",
            "At 1401 epoch, Validation Loss: 0.003322865115478635 \n",
            "At 1402 epoch, Training Loss: 0.00035676467523444445 \n",
            "At 1402 epoch, Validation Loss: 0.0033409278839826584 \n",
            "At 1403 epoch, Training Loss: 0.0003855996998026967 \n",
            "At 1403 epoch, Validation Loss: 0.00334173790179193 \n",
            "At 1404 epoch, Training Loss: 0.0002810461533954367 \n",
            "At 1404 epoch, Validation Loss: 0.003306964412331581 \n",
            "At 1405 epoch, Training Loss: 0.0003841832105536014 \n",
            "At 1405 epoch, Validation Loss: 0.003304912708699703 \n",
            "At 1406 epoch, Training Loss: 0.00030919690616428854 \n",
            "At 1406 epoch, Validation Loss: 0.003336420049890876 \n",
            "At 1407 epoch, Training Loss: 0.0003053324791835621 \n",
            "At 1407 epoch, Validation Loss: 0.0033387935254722834 \n",
            "At 1408 epoch, Training Loss: 0.0003064527350943536 \n",
            "At 1408 epoch, Validation Loss: 0.0033303601667284966 \n",
            "At 1409 epoch, Training Loss: 0.0004352646297775209 \n",
            "At 1409 epoch, Validation Loss: 0.0033401886466890574 \n",
            "At 1410 epoch, Training Loss: 0.0003950630722101778 \n",
            "At 1410 epoch, Validation Loss: 0.0033206308726221323 \n",
            "At 1411 epoch, Training Loss: 0.0004630619951058179 \n",
            "At 1411 epoch, Validation Loss: 0.0033813186455518007 \n",
            "At 1412 epoch, Training Loss: 0.00030413389031309636 \n",
            "At 1412 epoch, Validation Loss: 0.003471895819529891 \n",
            "At 1413 epoch, Training Loss: 0.00038389353721868247 \n",
            "At 1413 epoch, Validation Loss: 0.0034208272118121386 \n",
            "At 1414 epoch, Training Loss: 0.00029938365041743965 \n",
            "At 1414 epoch, Validation Loss: 0.003341804491356015 \n",
            "At 1415 epoch, Training Loss: 0.0004066732944920659 \n",
            "At 1415 epoch, Validation Loss: 0.0033156692516058683 \n",
            "At 1416 epoch, Training Loss: 0.00039015924267005175 \n",
            "At 1416 epoch, Validation Loss: 0.0033334344625473022 \n",
            "At 1417 epoch, Training Loss: 0.0004547072079731151 \n",
            "At 1417 epoch, Validation Loss: 0.0033248623367398977 \n",
            "At 1418 epoch, Training Loss: 0.0003027525614015758 \n",
            "At 1418 epoch, Validation Loss: 0.00333604053594172 \n",
            "At 1419 epoch, Training Loss: 0.00041422105859965084 \n",
            "At 1419 epoch, Validation Loss: 0.0033567396458238363 \n",
            "At 1420 epoch, Training Loss: 0.0003439708438236266 \n",
            "At 1420 epoch, Validation Loss: 0.0033223784994333982 \n",
            "At 1421 epoch, Training Loss: 0.00038504720432683823 \n",
            "At 1421 epoch, Validation Loss: 0.0033026847522705793 \n",
            "At 1422 epoch, Training Loss: 0.00037156219477765264 \n",
            "At 1422 epoch, Validation Loss: 0.00329959555529058 \n",
            "At 1423 epoch, Training Loss: 0.00023224784235935657 \n",
            "At 1423 epoch, Validation Loss: 0.0033172459807246923 \n",
            "At 1424 epoch, Training Loss: 0.0003596634662244469 \n",
            "At 1424 epoch, Validation Loss: 0.0033095087856054306 \n",
            "At 1425 epoch, Training Loss: 0.0003152960940496996 \n",
            "At 1425 epoch, Validation Loss: 0.00330070941708982 \n",
            "At 1426 epoch, Training Loss: 0.0002716996765229851 \n",
            "At 1426 epoch, Validation Loss: 0.003314997535198927 \n",
            "At 1427 epoch, Training Loss: 0.00043984718504361806 \n",
            "At 1427 epoch, Validation Loss: 0.0033595887944102287 \n",
            "At 1428 epoch, Training Loss: 0.00048448767920490355 \n",
            "At 1428 epoch, Validation Loss: 0.0033781547099351883 \n",
            "At 1429 epoch, Training Loss: 0.0003611908410675824 \n",
            "At 1429 epoch, Validation Loss: 0.003378403838723898 \n",
            "At 1430 epoch, Training Loss: 0.0003116652893368155 \n",
            "At 1430 epoch, Validation Loss: 0.003348772181198001 \n",
            "At 1431 epoch, Training Loss: 0.0004323210217989981 \n",
            "At 1431 epoch, Validation Loss: 0.0033245861995965242 \n",
            "At 1432 epoch, Training Loss: 0.00037118956097401676 \n",
            "At 1432 epoch, Validation Loss: 0.003318685106933117 \n",
            "At 1433 epoch, Training Loss: 0.00035469760769046843 \n",
            "At 1433 epoch, Validation Loss: 0.003306051716208458 \n",
            "At 1434 epoch, Training Loss: 0.0004532076127361506 \n",
            "At 1434 epoch, Validation Loss: 0.003321299562230706 \n",
            "At 1435 epoch, Training Loss: 0.00039916267269290984 \n",
            "At 1435 epoch, Validation Loss: 0.0033339785877615213 \n",
            "At 1436 epoch, Training Loss: 0.0003859249292872846 \n",
            "At 1436 epoch, Validation Loss: 0.003340920200571418 \n",
            "At 1437 epoch, Training Loss: 0.0002758302289294079 \n",
            "At 1437 epoch, Validation Loss: 0.0033235205337405205 \n",
            "At 1438 epoch, Training Loss: 0.0005067866935860365 \n",
            "At 1438 epoch, Validation Loss: 0.003330718260258436 \n",
            "At 1439 epoch, Training Loss: 0.0002745297912042588 \n",
            "At 1439 epoch, Validation Loss: 0.0033413725905120373 \n",
            "At 1440 epoch, Training Loss: 0.00026694301050156355 \n",
            "At 1440 epoch, Validation Loss: 0.003349370788782835 \n",
            "At 1441 epoch, Training Loss: 0.0002819080284098163 \n",
            "At 1441 epoch, Validation Loss: 0.0033322020899504423 \n",
            "At 1442 epoch, Training Loss: 0.00041689293284434824 \n",
            "At 1442 epoch, Validation Loss: 0.00330946478061378 \n",
            "At 1443 epoch, Training Loss: 0.00029394251469057054 \n",
            "At 1443 epoch, Validation Loss: 0.0033325988333672285 \n",
            "At 1444 epoch, Training Loss: 0.00029408162226900457 \n",
            "At 1444 epoch, Validation Loss: 0.0033382177352905273 \n",
            "At 1445 epoch, Training Loss: 0.0003290878696134314 \n",
            "At 1445 epoch, Validation Loss: 0.0033217989839613438 \n",
            "At 1446 epoch, Training Loss: 0.0003303954377770424 \n",
            "At 1446 epoch, Validation Loss: 0.0033062328584492207 \n",
            "At 1447 epoch, Training Loss: 0.00048768750857561825 \n",
            "At 1447 epoch, Validation Loss: 0.0032759266905486584 \n",
            "At 1448 epoch, Training Loss: 0.00032392106659244745 \n",
            "At 1448 epoch, Validation Loss: 0.0032806803938001394 \n",
            "At 1449 epoch, Training Loss: 0.00026323336642235516 \n",
            "At 1449 epoch, Validation Loss: 0.0033194134011864662 \n",
            "At 1450 epoch, Training Loss: 0.0002664917788933963 \n",
            "At 1450 epoch, Validation Loss: 0.0033617797307670116 \n",
            "At 1451 epoch, Training Loss: 0.0003495422221021727 \n",
            "At 1451 epoch, Validation Loss: 0.003351847641170025 \n",
            "At 1452 epoch, Training Loss: 0.00035202512226533144 \n",
            "At 1452 epoch, Validation Loss: 0.0033327541314065456 \n",
            "At 1453 epoch, Training Loss: 0.0002932400180725381 \n",
            "At 1453 epoch, Validation Loss: 0.003327404847368598 \n",
            "At 1454 epoch, Training Loss: 0.00028159903304185717 \n",
            "At 1454 epoch, Validation Loss: 0.003318862523883581 \n",
            "At 1455 epoch, Training Loss: 0.0002129842294380069 \n",
            "At 1455 epoch, Validation Loss: 0.003305398393422365 \n",
            "At 1456 epoch, Training Loss: 0.0004734427377115935 \n",
            "At 1456 epoch, Validation Loss: 0.003289694432169199 \n",
            "At 1457 epoch, Training Loss: 0.00035746142966672776 \n",
            "At 1457 epoch, Validation Loss: 0.003301607444882393 \n",
            "At 1458 epoch, Training Loss: 0.00038460629875771704 \n",
            "At 1458 epoch, Validation Loss: 0.0033082598820328712 \n",
            "At 1459 epoch, Training Loss: 0.0003735107544343919 \n",
            "At 1459 epoch, Validation Loss: 0.0033210739493370056 \n",
            "At 1460 epoch, Training Loss: 0.00034264319983776657 \n",
            "At 1460 epoch, Validation Loss: 0.003298253985121846 \n",
            "At 1461 epoch, Training Loss: 0.00035099334199912847 \n",
            "At 1461 epoch, Validation Loss: 0.0033013990614563227 \n",
            "At 1462 epoch, Training Loss: 0.0004287648946046829 \n",
            "At 1462 epoch, Validation Loss: 0.0033098547719419003 \n",
            "At 1463 epoch, Training Loss: 0.0003545645624399185 \n",
            "At 1463 epoch, Validation Loss: 0.0032999881077557802 \n",
            "At 1464 epoch, Training Loss: 0.0003033007844351232 \n",
            "At 1464 epoch, Validation Loss: 0.0033173945266753435 \n",
            "At 1465 epoch, Training Loss: 0.00023533264466095715 \n",
            "At 1465 epoch, Validation Loss: 0.003309317398816347 \n",
            "At 1466 epoch, Training Loss: 0.00039759297214914113 \n",
            "At 1466 epoch, Validation Loss: 0.0033025650773197412 \n",
            "At 1467 epoch, Training Loss: 0.00025819556030910463 \n",
            "At 1467 epoch, Validation Loss: 0.003304050536826253 \n",
            "At 1468 epoch, Training Loss: 0.00031667877919971943 \n",
            "At 1468 epoch, Validation Loss: 0.0032994537614285946 \n",
            "At 1469 epoch, Training Loss: 0.0002491846156772226 \n",
            "At 1469 epoch, Validation Loss: 0.003288046922534704 \n",
            "At 1470 epoch, Training Loss: 0.0003487885318463668 \n",
            "At 1470 epoch, Validation Loss: 0.0032951703760772943 \n",
            "At 1471 epoch, Training Loss: 0.00022737102408427746 \n",
            "At 1471 epoch, Validation Loss: 0.003318621776998043 \n",
            "At 1472 epoch, Training Loss: 0.0003917623485904187 \n",
            "At 1472 epoch, Validation Loss: 0.003348318161442876 \n",
            "At 1473 epoch, Training Loss: 0.00039937561377882956 \n",
            "At 1473 epoch, Validation Loss: 0.003354738699272275 \n",
            "At 1474 epoch, Training Loss: 0.0005196383921429515 \n",
            "At 1474 epoch, Validation Loss: 0.0033122876193374395 \n",
            "At 1475 epoch, Training Loss: 0.0003733625140739605 \n",
            "At 1475 epoch, Validation Loss: 0.0033114938996732235 \n",
            "At 1476 epoch, Training Loss: 0.00031762322469148786 \n",
            "At 1476 epoch, Validation Loss: 0.0033333657775074244 \n",
            "At 1477 epoch, Training Loss: 0.00029256000416353343 \n",
            "At 1477 epoch, Validation Loss: 0.003319984534755349 \n",
            "At 1478 epoch, Training Loss: 0.0003684125724248588 \n",
            "At 1478 epoch, Validation Loss: 0.003301557619124651 \n",
            "At 1479 epoch, Training Loss: 0.00040429839864373205 \n",
            "At 1479 epoch, Validation Loss: 0.003306536003947258 \n",
            "At 1480 epoch, Training Loss: 0.0004477893089642748 \n",
            "At 1480 epoch, Validation Loss: 0.0033133297692984343 \n",
            "At 1481 epoch, Training Loss: 0.00040467525250278415 \n",
            "At 1481 epoch, Validation Loss: 0.003306859638541937 \n",
            "At 1482 epoch, Training Loss: 0.00023446469276677817 \n",
            "At 1482 epoch, Validation Loss: 0.0033365818671882153 \n",
            "At 1483 epoch, Training Loss: 0.000237882902729325 \n",
            "At 1483 epoch, Validation Loss: 0.003352057421579957 \n",
            "At 1484 epoch, Training Loss: 0.00026688989892136304 \n",
            "At 1484 epoch, Validation Loss: 0.0033615222200751305 \n",
            "At 1485 epoch, Training Loss: 0.0002801835595164448 \n",
            "At 1485 epoch, Validation Loss: 0.003334292210638523 \n",
            "At 1486 epoch, Training Loss: 0.00043524658540263774 \n",
            "At 1486 epoch, Validation Loss: 0.0032949387095868587 \n",
            "At 1487 epoch, Training Loss: 0.00029448792338371277 \n",
            "At 1487 epoch, Validation Loss: 0.0032811672426760197 \n",
            "At 1488 epoch, Training Loss: 0.00037461622268892826 \n",
            "At 1488 epoch, Validation Loss: 0.0032719653099775314 \n",
            "At 1489 epoch, Training Loss: 0.00034130203421227633 \n",
            "At 1489 epoch, Validation Loss: 0.0032578594982624054 \n",
            "At 1490 epoch, Training Loss: 0.0004171921609668061 \n",
            "At 1490 epoch, Validation Loss: 0.003279787255451083 \n",
            "At 1491 epoch, Training Loss: 0.00037669381708838046 \n",
            "At 1491 epoch, Validation Loss: 0.0033232399728149176 \n",
            "At 1492 epoch, Training Loss: 0.00031243911653291436 \n",
            "At 1492 epoch, Validation Loss: 0.003342759096994996 \n",
            "At 1493 epoch, Training Loss: 0.00031147518602665515 \n",
            "At 1493 epoch, Validation Loss: 0.0033432908821851015 \n",
            "At 1494 epoch, Training Loss: 0.00028201972891110926 \n",
            "At 1494 epoch, Validation Loss: 0.003334952751174569 \n",
            "At 1495 epoch, Training Loss: 0.0003753549593966454 \n",
            "At 1495 epoch, Validation Loss: 0.0033146385103464127 \n",
            "At 1496 epoch, Training Loss: 0.00039022971759550273 \n",
            "At 1496 epoch, Validation Loss: 0.0033008139580488205 \n",
            "At 1497 epoch, Training Loss: 0.00024844466825015845 \n",
            "At 1497 epoch, Validation Loss: 0.0033010386396199465 \n",
            "At 1498 epoch, Training Loss: 0.00029860464856028556 \n",
            "At 1498 epoch, Validation Loss: 0.003309329506009817 \n",
            "At 1499 epoch, Training Loss: 0.000310158176580444 \n",
            "At 1499 epoch, Validation Loss: 0.0033072561491280794 \n",
            "At 1500 epoch, Training Loss: 0.00046535962028428914 \n",
            "At 1500 epoch, Validation Loss: 0.003298934083431959 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p07uzT62Btqa",
        "colab_type": "text"
      },
      "source": [
        "## 4. Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQzpgh0XF0hg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "23a62a16-3227-41ec-fe49-b62bb325936d"
      },
      "source": [
        "torch.load(\"ccFraud.pt\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('fc1.weight',\n",
              "              tensor([[ 0.2314, -0.1134,  0.0136,  ..., -0.0597,  0.0665,  0.0294],\n",
              "                      [ 0.0434, -0.1046,  0.0911,  ..., -0.0486,  0.0188,  0.0521],\n",
              "                      [ 0.2133, -0.1097,  0.1121,  ...,  0.0658, -0.0226, -0.0623],\n",
              "                      ...,\n",
              "                      [ 0.1819, -0.3275, -0.0796,  ...,  0.0620, -0.0090,  0.2317],\n",
              "                      [-0.0936,  0.0137, -0.0260,  ...,  0.0236,  0.0053, -0.0254],\n",
              "                      [-0.1202,  0.0105, -0.1749,  ..., -0.0395, -0.0195,  0.0666]],\n",
              "                     device='cuda:0')),\n",
              "             ('fc1.bias',\n",
              "              tensor([-1.9146e-01, -1.7273e-02, -2.3832e-02, -9.9987e-02, -8.7694e-03,\n",
              "                      -5.1067e-03,  2.7225e-04, -2.2760e-01, -1.0634e-02, -2.9390e-02,\n",
              "                      -1.0328e-03, -1.5116e-01,  1.1520e-02,  2.2386e-02,  4.1059e-02,\n",
              "                      -7.7364e-03, -2.7169e-01,  1.7177e-01, -5.4847e-02, -1.0535e-02,\n",
              "                      -1.3051e-01,  1.9569e-01, -2.6316e-01,  5.5866e-02,  8.3431e-03,\n",
              "                      -1.9664e-01,  1.7020e-02, -2.7851e-02,  2.6200e-03,  4.5613e-02,\n",
              "                      -6.2058e-02,  7.2905e-03,  4.7878e-03,  2.1315e-04,  1.3875e-03,\n",
              "                      -2.2325e-02, -5.1911e-02, -1.0455e-02, -6.5322e-02, -2.5310e-02,\n",
              "                      -9.7348e-02, -1.2241e-01, -1.7868e-02, -2.6949e-01, -1.1581e-01,\n",
              "                       2.0724e-01, -2.9114e-03, -2.4426e-01, -3.8115e-02, -6.0752e-02,\n",
              "                      -1.9860e-01, -6.2075e-02, -5.3568e-04,  8.5959e-02,  2.1229e-02,\n",
              "                       4.7470e-03, -5.3917e-02,  7.1190e-02, -5.3144e-02, -1.6119e-03,\n",
              "                      -3.5637e-03, -1.7548e-03, -2.3778e-01, -2.1834e-01, -3.0788e-02,\n",
              "                      -2.7175e-01,  6.9432e-02, -1.5492e-02, -9.5346e-03, -1.8791e-02,\n",
              "                      -1.6383e-02, -1.1088e-01, -1.2320e-01, -2.1722e-01, -2.5480e-02,\n",
              "                      -1.9967e-02, -1.3334e-01, -9.7410e-02, -1.6451e-01, -5.7514e-02,\n",
              "                      -9.8459e-03, -4.9069e-02, -1.0305e-01, -4.2540e-03, -2.1547e-01,\n",
              "                       5.1038e-03, -7.2646e-02, -5.0952e-02, -1.5641e-01, -9.1463e-02,\n",
              "                      -1.6523e-01, -9.9593e-02,  1.1327e-01, -2.5151e-01, -2.1323e-01,\n",
              "                      -3.3179e-02, -4.8650e-02,  7.2756e-02, -2.5781e-02,  2.7866e-04],\n",
              "                     device='cuda:0')),\n",
              "             ('fc2.weight',\n",
              "              tensor([[-3.8839e-02,  3.2901e-02, -1.2107e-01,  ...,  1.0514e-01,\n",
              "                        3.3269e-02,  2.1856e-01],\n",
              "                      [-5.7549e-02, -8.1561e-03, -2.1272e-02,  ...,  4.5874e-02,\n",
              "                       -1.7628e-02, -1.4782e-01],\n",
              "                      [-1.2983e-01, -1.6422e-01, -2.7571e-01,  ...,  7.3779e-02,\n",
              "                        5.9359e-02, -1.0932e-01],\n",
              "                      ...,\n",
              "                      [ 3.7660e-03, -8.6271e-03,  3.9388e-03,  ...,  5.0882e-04,\n",
              "                        6.5822e-04, -7.0315e-02],\n",
              "                      [-2.2653e-03,  4.0074e-03, -9.1623e-04,  ...,  5.9772e-03,\n",
              "                       -8.7201e-03,  1.0789e-02],\n",
              "                      [-5.6143e-03, -3.6823e-04,  1.2944e-03,  ...,  2.7644e-03,\n",
              "                       -1.0060e-04, -3.5669e-02]], device='cuda:0')),\n",
              "             ('norm2.weight',\n",
              "              tensor([ 0.5146,  0.5789,  0.8810,  0.0253,  0.2094,  0.4256,  0.1278,  0.2964,\n",
              "                       0.4416,  0.5956,  0.2077,  0.3052,  0.0850,  0.4434,  0.0763,  0.2532,\n",
              "                      -0.0102,  0.1295,  0.0341,  0.1520,  0.2937,  0.0373,  0.0271,  0.1134,\n",
              "                       0.0308,  0.2865,  0.1004,  0.0287,  0.0684,  0.0295,  0.0481,  0.2049,\n",
              "                       0.3276,  0.1981,  0.2327,  0.1956,  0.0903,  0.7110,  0.2909,  0.4251,\n",
              "                       1.0737,  0.2604, -0.0127,  0.4653,  0.6441,  0.3132,  0.0175,  0.0698,\n",
              "                       0.1334,  0.0702], device='cuda:0')),\n",
              "             ('norm2.bias',\n",
              "              tensor([ 1.3801e-02, -2.1151e-02,  2.4330e-01,  5.2317e-03,  7.5481e-02,\n",
              "                       1.6383e-02, -6.0297e-03, -9.0601e-04,  6.2165e-02,  1.1604e-01,\n",
              "                       1.7246e-03,  7.0569e-03, -7.2667e-03, -3.4166e-02, -2.0480e-02,\n",
              "                      -3.9062e-03,  7.6052e-03,  5.6620e-03, -1.5577e-03, -1.8163e-02,\n",
              "                      -2.1013e-02, -9.2869e-03, -2.4199e-02, -3.0293e-03,  2.4518e-02,\n",
              "                      -2.2550e-02, -5.0907e-03, -1.5348e-02, -8.6489e-03,  1.2037e-02,\n",
              "                       5.4144e-03, -2.0718e-02,  2.1391e-02,  1.4313e-04, -8.3690e-03,\n",
              "                      -1.0873e-02, -3.3880e-03, -5.7091e-02,  7.1466e-02, -4.5260e-02,\n",
              "                       6.4445e-01, -9.5745e-03,  7.2498e-04, -5.5493e-02,  4.9025e-04,\n",
              "                      -3.9938e-02, -5.3658e-03, -5.4919e-03,  3.0199e-03, -1.6068e-03],\n",
              "                     device='cuda:0')),\n",
              "             ('norm2.running_mean',\n",
              "              tensor([11.4109, -6.0015,  0.8459,  4.0733,  9.5069,  0.8817, -0.2817,  0.8564,\n",
              "                       7.0298,  3.1891, -0.3647,  2.9895, -0.3560, -1.6912,  1.3705,  1.0322,\n",
              "                      -0.2732, -0.9968,  0.2342,  0.4844, -0.6245, -0.3698,  2.1610,  0.4231,\n",
              "                       2.1761, -5.2210, -4.7168,  0.2864, -0.3432, -0.8179,  0.2106,  1.6143,\n",
              "                       3.0415,  0.9029,  7.4430, -2.1739, -0.5884, -4.0366, -1.5537,  1.6453,\n",
              "                       1.2934, -0.4806, -1.5774,  3.5447, -3.2991,  0.4904,  0.1479, -1.7462,\n",
              "                      -0.2432, -1.3044], device='cuda:0')),\n",
              "             ('norm2.running_var',\n",
              "              tensor([2.7350e+02, 7.9676e+01, 1.1846e+00, 3.8598e+01, 1.8099e+02, 2.0000e+00,\n",
              "                      1.8349e-01, 1.9215e+00, 1.0519e+02, 1.9271e+01, 3.4280e-01, 1.9271e+01,\n",
              "                      2.3873e-01, 6.1945e+00, 4.1046e+00, 2.8118e+00, 1.7182e-01, 2.4473e+00,\n",
              "                      1.1847e-01, 6.0138e-01, 8.2596e-01, 2.7158e-01, 9.9474e+00, 3.3845e-01,\n",
              "                      9.8278e+00, 6.2994e+01, 5.1774e+01, 1.8598e-01, 2.9002e-01, 1.4622e+00,\n",
              "                      8.0093e-02, 7.0120e+00, 2.0657e+01, 2.1447e+00, 1.1516e+02, 1.1713e+01,\n",
              "                      8.5517e-01, 5.0622e+01, 6.2397e+00, 7.1043e+00, 2.6230e+00, 5.1171e-01,\n",
              "                      6.1631e+00, 3.1505e+01, 2.4254e+01, 1.2507e+00, 5.6357e-02, 7.5745e+00,\n",
              "                      9.8598e-02, 3.8417e+00], device='cuda:0')),\n",
              "             ('norm2.num_batches_tracked', tensor(3090, device='cuda:0')),\n",
              "             ('fc3.weight',\n",
              "              tensor([[ 0.0073, -0.0003,  0.0025,  ...,  0.0032, -0.0093, -0.0024],\n",
              "                      [-0.1618,  0.2124,  0.1948,  ...,  0.0616, -0.0091,  0.0873],\n",
              "                      [-0.0359, -0.0423,  0.1834,  ...,  0.0149, -0.0180,  0.0093],\n",
              "                      ...,\n",
              "                      [ 0.1337, -0.1859, -0.1255,  ..., -0.1044, -0.0645, -0.0433],\n",
              "                      [-0.0285,  0.0797, -0.2717,  ...,  0.0260, -0.0096,  0.0343],\n",
              "                      [ 0.0081, -0.0308, -0.0088,  ..., -0.0046,  0.0116,  0.0011]],\n",
              "                     device='cuda:0')),\n",
              "             ('norm3.weight',\n",
              "              tensor([ 0.2877,  0.3094,  0.7863,  0.5283,  0.2752,  0.3729,  0.6048,  0.5056,\n",
              "                       0.4307,  0.7719,  0.5606,  0.2781,  0.2797,  0.7271,  0.4093,  0.7663,\n",
              "                       0.3494,  0.3219, -0.0358,  0.0377,  0.2857,  0.6640,  0.4168,  0.8283,\n",
              "                      -0.0288], device='cuda:0')),\n",
              "             ('norm3.bias',\n",
              "              tensor([-0.0390, -0.4514,  0.0269, -0.0708, -0.0618, -0.0194,  0.4879,  0.0520,\n",
              "                       0.0540, -0.2597,  0.5691,  0.0583, -0.0033,  0.1318,  0.0256, -0.0301,\n",
              "                      -0.0066,  0.0793,  0.0547, -0.0040,  0.1416, -0.1421,  0.0823, -1.0050,\n",
              "                      -0.0220], device='cuda:0')),\n",
              "             ('norm3.running_mean',\n",
              "              tensor([ 0.0740, -0.0709, -0.0638,  0.2538,  0.1094, -0.0644, -0.1286, -0.1175,\n",
              "                       0.1905, -0.1750, -0.2684,  0.1416,  0.1364,  0.0006, -0.0540, -0.1915,\n",
              "                      -0.2474,  0.0398,  0.1277, -0.0161, -0.0384,  0.0574, -0.0477,  0.4752,\n",
              "                       0.0772], device='cuda:0')),\n",
              "             ('norm3.running_var',\n",
              "              tensor([0.0320, 0.7172, 0.0442, 0.9613, 0.0552, 0.1332, 0.0516, 0.7943, 0.4403,\n",
              "                      0.6588, 0.2762, 0.9342, 0.3159, 0.6720, 0.2837, 0.0711, 1.3140, 0.9129,\n",
              "                      0.1130, 0.0725, 1.0439, 0.0292, 0.6209, 0.4262, 0.0229],\n",
              "                     device='cuda:0')),\n",
              "             ('norm3.num_batches_tracked', tensor(3090, device='cuda:0')),\n",
              "             ('fc4.weight',\n",
              "              tensor([[-0.1502, -0.1933, -0.1236,  0.0147, -0.1652,  0.1921, -0.1792,  0.1767,\n",
              "                        0.1128, -0.2947, -0.2076,  0.1044,  0.1608,  0.0864, -0.1560, -0.0497,\n",
              "                       -0.0973, -0.0034,  0.1650, -0.1664,  0.0816, -0.0920,  0.1918,  0.2179,\n",
              "                        0.0816],\n",
              "                      [-0.0298, -0.1364,  0.1698,  0.0877,  0.0012, -0.0263, -0.0564,  0.0920,\n",
              "                       -0.0212, -0.1434, -0.1148,  0.0385, -0.0334,  0.0043, -0.1807,  0.1370,\n",
              "                       -0.0303,  0.1016,  0.0595, -0.0876,  0.0177, -0.1926,  0.0910,  0.0217,\n",
              "                        0.0625],\n",
              "                      [ 0.0491, -0.0064, -0.0338, -0.1088,  0.0285,  0.0177, -0.0689, -0.1953,\n",
              "                        0.1703, -0.1063, -0.2676, -0.1078, -0.0256, -0.1259, -0.0870,  0.0127,\n",
              "                       -0.2301, -0.1638,  0.1683,  0.0231, -0.1743,  0.0009, -0.2154,  0.2069,\n",
              "                        0.1197],\n",
              "                      [-0.0798, -0.1979, -0.0319,  0.0011, -0.1383, -0.0125, -0.0701,  0.1209,\n",
              "                        0.0691, -0.3032, -0.3407,  0.0878,  0.0032,  0.0762, -0.1425,  0.0774,\n",
              "                       -0.0939,  0.1080,  0.0221, -0.0663,  0.0238, -0.1783,  0.0799,  0.2251,\n",
              "                        0.0985],\n",
              "                      [ 0.2313, -0.0420, -0.4061,  0.2569,  0.2128, -0.1239, -0.4225, -0.1838,\n",
              "                        0.2614, -0.0624, -0.4947,  0.0499,  0.0424,  0.3821, -0.0867, -0.3607,\n",
              "                       -0.1742,  0.0015, -0.0584,  0.0942, -0.0109,  0.3792, -0.0353,  0.6092,\n",
              "                       -0.0281],\n",
              "                      [ 0.0160, -0.0681, -0.1368, -0.1755,  0.1652, -0.0230, -0.0598,  0.0420,\n",
              "                        0.3025,  0.2659,  0.0433, -0.0834,  0.0010,  0.3704,  0.1145, -0.0417,\n",
              "                       -0.1920,  0.0804,  0.0273,  0.0136,  0.1383,  0.1248,  0.0092, -0.0329,\n",
              "                       -0.0575],\n",
              "                      [ 0.0441,  0.0864, -0.1282, -0.1535, -0.1152, -0.0208,  0.0542, -0.0599,\n",
              "                        0.0348,  0.0371, -0.0415, -0.0409, -0.0563,  0.0091,  0.0605, -0.1193,\n",
              "                       -0.0524, -0.0759,  0.0461,  0.0411, -0.1415,  0.0080, -0.1326,  0.0518,\n",
              "                       -0.0010],\n",
              "                      [ 0.1698, -0.0639, -0.2401, -0.0488,  0.0728, -0.2252, -0.1678, -0.2406,\n",
              "                       -0.0296,  0.2333,  0.0714, -0.1046, -0.1644, -0.2654,  0.0464, -0.1208,\n",
              "                        0.0351,  0.0671, -0.1405,  0.0146,  0.0678,  0.2367, -0.0919, -0.0786,\n",
              "                       -0.1075]], device='cuda:0')),\n",
              "             ('norm4.weight',\n",
              "              tensor([0.6502, 0.3322, 0.5395, 0.9377, 1.0625, 0.7790, 0.3487, 0.8572],\n",
              "                     device='cuda:0')),\n",
              "             ('norm4.bias',\n",
              "              tensor([-0.2678, -0.0331,  0.2334, -0.4283, -1.6685, -0.1908,  0.0022, -0.5364],\n",
              "                     device='cuda:0')),\n",
              "             ('norm4.running_mean',\n",
              "              tensor([-0.1470,  0.0359, -0.3526, -0.1633, -1.0685,  0.0784, -0.0760, -0.0371],\n",
              "                     device='cuda:0')),\n",
              "             ('norm4.running_var',\n",
              "              tensor([0.4167, 0.0862, 0.3980, 0.1287, 3.0582, 0.5214, 0.1233, 0.1296],\n",
              "                     device='cuda:0')),\n",
              "             ('norm4.num_batches_tracked', tensor(3090, device='cuda:0')),\n",
              "             ('fc5.weight',\n",
              "              tensor([[ 0.1485,  0.1362,  0.2876,  0.4414,  0.4203, -0.1508, -0.1577, -0.1898],\n",
              "                      [-0.2214, -0.0536, -0.3429, -0.1788, -0.5826,  0.1695,  0.0238,  0.4773],\n",
              "                      [ 0.1671,  0.2358,  0.0448,  0.3566,  0.5172, -0.0117,  0.1530, -0.3718]],\n",
              "                     device='cuda:0')),\n",
              "             ('norm5.weight',\n",
              "              tensor([2.0991, 2.5385, 2.1089], device='cuda:0')),\n",
              "             ('norm5.bias', tensor([0.3902, 0.9458, 0.3838], device='cuda:0')),\n",
              "             ('norm5.running_mean',\n",
              "              tensor([-0.5427,  0.4835, -0.6363], device='cuda:0')),\n",
              "             ('norm5.running_var',\n",
              "              tensor([0.4076, 0.1522, 0.2303], device='cuda:0')),\n",
              "             ('norm5.num_batches_tracked', tensor(3090, device='cuda:0')),\n",
              "             ('fc6.weight',\n",
              "              tensor([[ 1.4811, -1.4707,  1.4675]], device='cuda:0')),\n",
              "             ('fc6.bias', tensor([-0.2280], device='cuda:0'))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMdyJSe_Btqa",
        "colab_type": "text"
      },
      "source": [
        "We plot the graph of the loss for training and validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN4bh4s1Btqb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "c92567ff-7571-43d6-e933-c89ed875e3a0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "x_epochs=np.linspace(1,n_epoch,n_epoch).astype(dtype=int)\n",
        "plt.plot(x_epochs,train_loss)\n",
        "plt.plot(x_epochs,valid_loss)\n",
        "plt.xlabel(\"N Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid()\n",
        "plt.legend([\"Training\",\"Validation\"])\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSTklAhGRlFMiEBFJOSUCEZGU+/+42pbZ9u9YGwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny6b0rwOBtqe",
        "colab_type": "text"
      },
      "source": [
        "### 4.1 ROC and AUC\n",
        "We will now evaluate the model using the ROC Curve and AUC. You can find more information about it at this [link](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cik550VNBtqf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "1d25e5b2-0d54-4320-b934-fc31a5578ccb"
      },
      "source": [
        "k=np.linspace(0,1,10)\n",
        "TPR=[]\n",
        "FPR=[]\n",
        "for n in k:\n",
        "    outputs=[]\n",
        "    targets=[]\n",
        "    for data,target in test_loader:\n",
        "        data,target=data.cuda(),target.cuda()\n",
        "        output=model(data.float())\n",
        "        output=torch.sigmoid(output.cpu())\n",
        "        output=list(np.where(output<n,0,1)[:])\n",
        "        target=list(target.cpu().numpy())\n",
        "        outputs+=output\n",
        "        targets+=target\n",
        "\n",
        "    outputs_of_model=np.stack(outputs,axis=0).squeeze().astype(dtype=int)\n",
        "    targets_of_model=np.array(targets).astype(dtype=int)\n",
        "    \n",
        "    #detect True Negatives, False Positives, False Negatives, True Positives\n",
        "    TN=np.array(targets_of_model[outputs_of_model==0]==0).sum()\n",
        "    FP=np.array(targets_of_model[outputs_of_model==1]==0).sum()\n",
        "    FN=np.array(targets_of_model[outputs_of_model==0]==1).sum()\n",
        "    TP=np.array(targets_of_model[outputs_of_model==1]==1).sum()\n",
        "\n",
        "    TPR.append(TP/(TP+FN))\n",
        "    FPR.append(FP/(FP+TN))\n",
        "\n",
        "#Plot True Positive Rate and False Positive Rate\n",
        "plt.plot(FPR,TPR)\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "\n",
        "AUC=0.0;\n",
        "for i in range(len(TPR)-1):\n",
        "    AUC+=np.abs((TPR[i]+TPR[i+1])*(FPR[i+1]-FPR[i])/2)\n",
        "  \n",
        "  \n",
        "print(\"Estimated AUC: {}\".format(AUC))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Estimated AUC: 0.9422156952407754\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnLrUIK4UAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax-VkJQKBtqh",
        "colab_type": "text"
      },
      "source": [
        "The AUC for a perfect model would be exactly 1. Our model achieves a pretty good result on the test set. <br /> \n",
        "Even though AUC is usually a good metric of the quality of our model, cases of anomaly detection may bear high AUC even in case of low overall model quality due to the large amount of false positives predicted by the model. F1-score calculation is, in this case, necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSRSQRMLBtqj",
        "colab_type": "text"
      },
      "source": [
        "### 4.2 F1-score\n",
        "Another useful metric for the anomaly detection problem is [F1-score](https://en.wikipedia.org/wiki/F1_score) which is calculated below for a threshold of 0.5. <br />\n",
        "In order for the model to be considered reliable, the F1-score should be well above 0.5 (0.75~0.8 can be considered quite good)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS3Q1mIPBtqk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "8ebe9a33-9fdc-49e5-e48b-ba8375b8b026"
      },
      "source": [
        "outputs=[]\n",
        "targets=[]\n",
        "for data,target in test_loader:\n",
        "    if on_cuda:\n",
        "        data,target=data.cuda(),target.cuda()\n",
        "    output=model(data.float())\n",
        "    output=torch.sigmoid(output.cpu())\n",
        "    output=list(np.where(output<0.5,0,1)[:])\n",
        "    target=list(target.cpu().numpy())\n",
        "    outputs+=output\n",
        "    targets+=target\n",
        "\n",
        "outputs_of_model=np.stack(outputs,axis=0).squeeze().astype(dtype=int)\n",
        "targets_of_model=np.array(targets).astype(dtype=int)\n",
        "\n",
        "\n",
        "TN=np.array(targets_of_model[outputs_of_model==0]==0).sum()\n",
        "FP=np.array(targets_of_model[outputs_of_model==1]==0).sum()\n",
        "FN=np.array(targets_of_model[outputs_of_model==0]==1).sum()\n",
        "TP=np.array(targets_of_model[outputs_of_model==1]==1).sum()\n",
        "\n",
        "\n",
        "print(\"True Positives: {}\".format(TP))\n",
        "print(\"True Negatives: {}\".format(TN))\n",
        "print(\"False Positives: {}\".format(FP))\n",
        "print(\"False Negatives: {}\".format(FN))\n",
        "\n",
        "\n",
        "precision=TP/(TP+FP)\n",
        "recall=TP/(TP+FN)\n",
        "print(\"Model Precision: {}\".format(precision))\n",
        "print(\"Model Recall: {}\".format(recall))\n",
        "\n",
        "F1=2*precision*recall/(precision+recall)\n",
        "\n",
        "print(\"F1-score: {}\".format(F1))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True Positives: 46\n",
            "True Negatives: 28421\n",
            "False Positives: 8\n",
            "False Negatives: 6\n",
            "Model Precision: 0.8518518518518519\n",
            "Model Recall: 0.8846153846153846\n",
            "F1-score: 0.8679245283018868\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ekh_bCgtFmc0",
        "colab_type": "text"
      },
      "source": [
        "F1-score of 0.87 is a great result!\n",
        "This is only one possible solution for the present problem. Try changing the number of layers, the learning rate and all the parameters to achieve a better F1-score. <br />\n",
        "In addition, some other algorithms (like random forest) may achieve even higher scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIydQGVWGKY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}